[
  {
    "doc_id": "c0b7d76e",
    "source_path": "data/uploads/Assignment 12 dl .pdf",
    "text": "Paper Review: Long-term Recurrent Convolutional Networks for\nVisual Recognition and Description (CVPR 2015)\nNithish Karanam\n1 Paper Summary\nThis paper describes the Long-term Recurrent Convolutional Networks (LRCN), a unified framework that links convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) — more broadly, with Long Short-Term Memory (LSTM) units — for visual\ndescription and visual recognition. Visual recognition traditionally deals with individual images or videos as singleton instances,\nwhereas LRCN deals with sequences of frames, representing spatial and temporal dynamics. The features of the individual video or\nthe single image are represented by the CNN component and are fed into an LSTM network representing temporal relations over the\nsequence. The end-to-end approach makes the model an attractive candidate for carrying out tasks such as video classification and\nimage captioning in a unified end-to-end trainable framework. The paper describes large-scale experiments on benchmarking sets,\nwhich demonstrate that LRCN achieves state-of-the-art competitive on video recognition (e.g., on UCF101) and image captioning\n(e.g., on Flickr30K) and proposes a new paradigm for the gap between visual content and natural language. The work in particular\nis a major advance by showing that joint modeling of spatial and temporal features could be leading towards higher visual data\nunderstanding and description and opens the stage for further breakthroughs in multi-modal learning.\n2 Experimental Results\nEmpirical evaluation of the model of LRCN is extensive and comprises several applications such as video classification and image\ncaptioning. The model of LRCN is compared with conventional techniques and with simpler baselines in an effort to demonstrate its\nstrength for modeling long temporal relationships and employing spatial features effectively.\nPerformance Comparison\nFor video classification, the paper explains that LRCN outperforms baseline models that either pay no attention to temporal dynamics\nor are in a two-stage framework. For example, while a model with only a CNN may have decent classification accuracy, concatenation\nof temporal features with an LSTM results in a substantial boost in performance. Comparison has been illustrated in the following\nrepresentative table (numeric values are for representation):\nModel Accuracy (%) Sequence Length\nCNN + SVM 65.0 –\nCNN + RNN 68.5 50 frames\nLRCN (Proposed) 72.0 100 frames\nTable 1: Illustrative performance comparison for video classification.\nArchitecture and Visualization\nMoreover, the paper includes a diagram of the architecture of LRCN (see Figure 1). It illustrates how the CNN extracts features from\neach frame of a video and passes these features sequentially through the LSTM, which gives class probabilities for a recognition task\nor word sequences for a captioning task. The diagram serves an important function in enabling one to easily understand how the\nspatial and temporal processing are merged into one model.\n1\nFigure 1: A schematic illustration of the LRCN architecture, showing the integration of the CNN and LSTM components.\n3 Contribution\n3.1 3.1 Unified Spatio-Temporal Modeling Framework\nLRCN represents a major breakthrough in fusing convolutional and recurrent neural network models. Unlike earlier when image\nand temporal modeling were treated as two separable tasks, LRCN combines the two into one end-to-end trainable model. The\nunified model bestows the model with an ability to encode temporal dynamics and spatial features effectively, hence improving\nsequence-understanding task performance such as video classification and image captioning.\n3.2 3.2 Detailed Analysis of Temporal Dynamics\nA thorough comparison of temporal modeling for visual description and recognition by the authors is presented. The authors show\nthrough comparisons with models with and without the LSTM module that temporal dynamics play an immense role in leading\ntowards much impr"
  },
  {
    "doc_id": "c0b7d76e",
    "source_path": "data/uploads/Assignment 12 dl .pdf",
    "text": " tasks, LRCN combines the two into one end-to-end trainable model. The\nunified model bestows the model with an ability to encode temporal dynamics and spatial features effectively, hence improving\nsequence-understanding task performance such as video classification and image captioning.\n3.2 3.2 Detailed Analysis of Temporal Dynamics\nA thorough comparison of temporal modeling for visual description and recognition by the authors is presented. The authors show\nthrough comparisons with models with and without the LSTM module that temporal dynamics play an immense role in leading\ntowards much improved recognition. The comparison exhibits the benefits of long-term modeling with clear evidence that the\nsequential context model of the LSTM provides more sturdy and explainable visual representations.\n3.3 3.3 Comprehensive Evaluation and Code Release\nBesides methodology breakthroughs, the paper includes full empirical justifications on benchmarking sets with demonstrations of\npractical viability of LRCN. Presenting both qualitative findings such as sample classifications of the video and the resulting captions,\nand the quantitative findings such as classification accuracy and BLEU scores for captioning provides a comprehensive evaluation\nof the model. Furthermore, the code was made available, not only enabling reproducibility but also enabling further research and\ninnovation into multi-modal learning.\n4 Criticism\n4.1 4.1 High Computational Requirements\nExcessive computational demands A major disadvantage of LRCN is that it is extremely computationally expensive. End-to-end\ncomposition of CNNs and LSTMs for modelling needs a lot of computational resources, especially during training over immense\nvideo datasets. The computational demand may limit the application of the model for practising professionals and researchers with\nlesser access to higher-end hardware.\n4.2 4.2 Limited Exploration of Multi-Modal Integration\nWhile both video classification and image captioning perform very well in LRCN, the work of the paper focuses primarily on the\nRGB video. Little work with other modalities such as depth and audio is presented, which could contain even more contextual\ncontent and produce even higher performance. Exploring the model with more than one modality could be an interesting research\ndirection.\n2\n4.3 4.3 Simplistic Temporal Modeling Approach\nNaive Temporal Modeling Strategy While the application of LSTMs enables temporal relationships to be modeled, the paper doesn’t\nfully cover the limitations of conventional LSTM architectures. For instance, the conventional LSTM may be hard for extremely\nlengthy sequences or extremely complex temporal relationships, and other temporal modeling mechanisms like attention mechanisms\nor transformers could be applied for further performance improvement. The approach of the paper, while efficient, will be improved\nfurther if such state-of-the-art techniques are used for handling long-distance dependencies more robustly.\nReference\n• Donahue, J., Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., & Darrell, T. (2015). Long-term\nRecurrent Convolutional Networks for Visual Recognition and Description. In CVPR.\n3"
  },
  {
    "doc_id": "c0b7d76e",
    "source_path": "data/uploads/Assignment 12 dl .pdf",
    "text": "Paper Review: Long-term Recurrent Convolutional Networks for\nVisual Recognition and Description (CVPR 2015)\nNithish Karanam\n1 Paper Summary\nThis paper describes the Long-term Recurrent Convolutional Networks (LRCN), a unified framework that links convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) — more broadly, with Long Short-Term Memory (LSTM) units — for visual\ndescription and visual recognition. Visual recognition traditionally deals with individual images or videos as singleton instances,\nwhereas LRCN deals with sequences of frames, representing spatial and temporal dynamics. The features of the individual video or\nthe single image are represented by the CNN component and are fed into an LSTM network representing temporal relations over the\nsequence. The end-to-end approach makes the model an attractive candidate for carrying out tasks such as video classification and\nimage captioning in a unified end-to-end trainable framework. The paper describes large-scale experiments on benchmarking sets,\nwhich demonstrate that LRCN achieves state-of-the-art competitive on video recognition (e.g., on UCF101) and image captioning\n(e.g., on Flickr30K) and proposes a new paradigm for the gap between visual content and natural language. The work in particular\nis a major advance by showing that joint modeling of spatial and temporal features could be leading towards higher visual data\nunderstanding and description and opens the stage for further breakthroughs in multi-modal learning.\n2 Experimental Results\nEmpirical evaluation of the model of LRCN is extensive and comprises several applications such as video classification and image\ncaptioning. The model of LRCN is compared with conventional techniques and with simpler baselines in an effort to demonstrate its\nstrength for modeling long temporal relationships and employing spatial features effectively.\nPerformance Comparison\nFor video classification, the paper explains that LRCN outperforms baseline models that either pay no attention to temporal dynamics\nor are in a two-stage framework. For example, while a model with only a CNN may have decent classification accuracy, concatenation\nof temporal features with an LSTM results in a substantial boost in performance. Comparison has been illustrated in the following\nrepresentative table (numeric values are for representation):\nModel Accuracy (%) Sequence Length\nCNN + SVM 65.0 –\nCNN + RNN 68.5 50 frames\nLRCN (Proposed) 72.0 100 frames\nTable 1: Illustrative performance comparison for video classification.\nArchitecture and Visualization\nMoreover, the paper includes a diagram of the architecture of LRCN (see Figure 1). It illustrates how the CNN extracts features from\neach frame of a video and passes these features sequentially through the LSTM, which gives class probabilities for a recognition task\nor word sequences for a captioning task. The diagram serves an important function in enabling one to easily understand how the\nspatial and temporal processing are merged into one model.\n1\nFigure 1: A schematic illustration of the LRCN architecture, showing the integration of the CNN and LSTM components.\n3 Contribution\n3.1 3.1 Unified Spatio-Temporal Modeling Framework\nLRCN represents a major breakthrough in fusing convolutional and recurrent neural network models. Unlike earlier when image\nand temporal modeling were treated as two separable tasks, LRCN combines the two into one end-to-end trainable model. The\nunified model bestows the model with an ability to encode temporal dynamics and spatial features effectively, hence improving\nsequence-understanding task performance such as video classification and image captioning.\n3.2 3.2 Detailed Analysis of Temporal Dynamics\nA thorough comparison of temporal modeling for visual description and recognition by the authors is presented. The authors show\nthrough comparisons with models with and without the LSTM module that temporal dynamics play an immense role in leading\ntowards much impr"
  },
  {
    "doc_id": "c0b7d76e",
    "source_path": "data/uploads/Assignment 12 dl .pdf",
    "text": " tasks, LRCN combines the two into one end-to-end trainable model. The\nunified model bestows the model with an ability to encode temporal dynamics and spatial features effectively, hence improving\nsequence-understanding task performance such as video classification and image captioning.\n3.2 3.2 Detailed Analysis of Temporal Dynamics\nA thorough comparison of temporal modeling for visual description and recognition by the authors is presented. The authors show\nthrough comparisons with models with and without the LSTM module that temporal dynamics play an immense role in leading\ntowards much improved recognition. The comparison exhibits the benefits of long-term modeling with clear evidence that the\nsequential context model of the LSTM provides more sturdy and explainable visual representations.\n3.3 3.3 Comprehensive Evaluation and Code Release\nBesides methodology breakthroughs, the paper includes full empirical justifications on benchmarking sets with demonstrations of\npractical viability of LRCN. Presenting both qualitative findings such as sample classifications of the video and the resulting captions,\nand the quantitative findings such as classification accuracy and BLEU scores for captioning provides a comprehensive evaluation\nof the model. Furthermore, the code was made available, not only enabling reproducibility but also enabling further research and\ninnovation into multi-modal learning.\n4 Criticism\n4.1 4.1 High Computational Requirements\nExcessive computational demands A major disadvantage of LRCN is that it is extremely computationally expensive. End-to-end\ncomposition of CNNs and LSTMs for modelling needs a lot of computational resources, especially during training over immense\nvideo datasets. The computational demand may limit the application of the model for practising professionals and researchers with\nlesser access to higher-end hardware.\n4.2 4.2 Limited Exploration of Multi-Modal Integration\nWhile both video classification and image captioning perform very well in LRCN, the work of the paper focuses primarily on the\nRGB video. Little work with other modalities such as depth and audio is presented, which could contain even more contextual\ncontent and produce even higher performance. Exploring the model with more than one modality could be an interesting research\ndirection.\n2\n4.3 4.3 Simplistic Temporal Modeling Approach\nNaive Temporal Modeling Strategy While the application of LSTMs enables temporal relationships to be modeled, the paper doesn’t\nfully cover the limitations of conventional LSTM architectures. For instance, the conventional LSTM may be hard for extremely\nlengthy sequences or extremely complex temporal relationships, and other temporal modeling mechanisms like attention mechanisms\nor transformers could be applied for further performance improvement. The approach of the paper, while efficient, will be improved\nfurther if such state-of-the-art techniques are used for handling long-distance dependencies more robustly.\nReference\n• Donahue, J., Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., & Darrell, T. (2015). Long-term\nRecurrent Convolutional Networks for Visual Recognition and Description. In CVPR.\n3"
  },
  {
    "doc_id": "86284412",
    "source_path": "data/uploads/Assignment 12 dl .pdf",
    "text": "Paper Review: Long-term Recurrent Convolutional Networks for\nVisual Recognition and Description (CVPR 2015)\nNithish Karanam\n1 Paper Summary\nThis paper describes the Long-term Recurrent Convolutional Networks (LRCN), a unified framework that links convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) — more broadly, with Long Short-Term Memory (LSTM) units — for visual\ndescription and visual recognition. Visual recognition traditionally deals with individual images or videos as singleton instances,\nwhereas LRCN deals with sequences of frames, representing spatial and temporal dynamics. The features of the individual video or\nthe single image are represented by the CNN component and are fed into an LSTM network representing temporal relations over the\nsequence. The end-to-end approach makes the model an attractive candidate for carrying out tasks such as video classification and\nimage captioning in a unified end-to-end trainable framework. The paper describes large-scale experiments on benchmarking sets,\nwhich demonstrate that LRCN achieves state-of-the-art competitive on video recognition (e.g., on UCF101) and image captioning\n(e.g., on Flickr30K) and proposes a new paradigm for the gap between visual content and natural language. The work in particular\nis a major advance by showing that joint modeling of spatial and temporal features could be leading towards higher visual data\nunderstanding and description and opens the stage for further breakthroughs in multi-modal learning.\n2 Experimental Results\nEmpirical evaluation of the model of LRCN is extensive and comprises several applications such as video classification and image\ncaptioning. The model of LRCN is compared with conventional techniques and with simpler baselines in an effort to demonstrate its\nstrength for modeling long temporal relationships and employing spatial features effectively.\nPerformance Comparison\nFor video classification, the paper explains that LRCN outperforms baseline models that either pay no attention to temporal dynamics\nor are in a two-stage framework. For example, while a model with only a CNN may have decent classification accuracy, concatenation\nof temporal features with an LSTM results in a substantial boost in performance. Comparison has been illustrated in the following\nrepresentative table (numeric values are for representation):\nModel Accuracy (%) Sequence Length\nCNN + SVM 65.0 –\nCNN + RNN 68.5 50 frames\nLRCN (Proposed) 72.0 100 frames\nTable 1: Illustrative performance comparison for video classification.\nArchitecture and Visualization\nMoreover, the paper includes a diagram of the architecture of LRCN (see Figure 1). It illustrates how the CNN extracts features from\neach frame of a video and passes these features sequentially through the LSTM, which gives class probabilities for a recognition task\nor word sequences for a captioning task. The diagram serves an important function in enabling one to easily understand how the\nspatial and temporal processing are merged into one model.\n1\nFigure 1: A schematic illustration of the LRCN architecture, showing the integration of the CNN and LSTM components.\n3 Contribution\n3.1 3.1 Unified Spatio-Temporal Modeling Framework\nLRCN represents a major breakthrough in fusing convolutional and recurrent neural network models. Unlike earlier when image\nand temporal modeling were treated as two separable tasks, LRCN combines the two into one end-to-end trainable model. The\nunified model bestows the model with an ability to encode temporal dynamics and spatial features effectively, hence improving\nsequence-understanding task performance such as video classification and image captioning.\n3.2 3.2 Detailed Analysis of Temporal Dynamics\nA thorough comparison of temporal modeling for visual description and recognition by the authors is presented. The authors show\nthrough comparisons with models with and without the LSTM module that temporal dynamics play an immense role in leading\ntowards much impr"
  },
  {
    "doc_id": "86284412",
    "source_path": "data/uploads/Assignment 12 dl .pdf",
    "text": " tasks, LRCN combines the two into one end-to-end trainable model. The\nunified model bestows the model with an ability to encode temporal dynamics and spatial features effectively, hence improving\nsequence-understanding task performance such as video classification and image captioning.\n3.2 3.2 Detailed Analysis of Temporal Dynamics\nA thorough comparison of temporal modeling for visual description and recognition by the authors is presented. The authors show\nthrough comparisons with models with and without the LSTM module that temporal dynamics play an immense role in leading\ntowards much improved recognition. The comparison exhibits the benefits of long-term modeling with clear evidence that the\nsequential context model of the LSTM provides more sturdy and explainable visual representations.\n3.3 3.3 Comprehensive Evaluation and Code Release\nBesides methodology breakthroughs, the paper includes full empirical justifications on benchmarking sets with demonstrations of\npractical viability of LRCN. Presenting both qualitative findings such as sample classifications of the video and the resulting captions,\nand the quantitative findings such as classification accuracy and BLEU scores for captioning provides a comprehensive evaluation\nof the model. Furthermore, the code was made available, not only enabling reproducibility but also enabling further research and\ninnovation into multi-modal learning.\n4 Criticism\n4.1 4.1 High Computational Requirements\nExcessive computational demands A major disadvantage of LRCN is that it is extremely computationally expensive. End-to-end\ncomposition of CNNs and LSTMs for modelling needs a lot of computational resources, especially during training over immense\nvideo datasets. The computational demand may limit the application of the model for practising professionals and researchers with\nlesser access to higher-end hardware.\n4.2 4.2 Limited Exploration of Multi-Modal Integration\nWhile both video classification and image captioning perform very well in LRCN, the work of the paper focuses primarily on the\nRGB video. Little work with other modalities such as depth and audio is presented, which could contain even more contextual\ncontent and produce even higher performance. Exploring the model with more than one modality could be an interesting research\ndirection.\n2\n4.3 4.3 Simplistic Temporal Modeling Approach\nNaive Temporal Modeling Strategy While the application of LSTMs enables temporal relationships to be modeled, the paper doesn’t\nfully cover the limitations of conventional LSTM architectures. For instance, the conventional LSTM may be hard for extremely\nlengthy sequences or extremely complex temporal relationships, and other temporal modeling mechanisms like attention mechanisms\nor transformers could be applied for further performance improvement. The approach of the paper, while efficient, will be improved\nfurther if such state-of-the-art techniques are used for handling long-distance dependencies more robustly.\nReference\n• Donahue, J., Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., & Darrell, T. (2015). Long-term\nRecurrent Convolutional Networks for Visual Recognition and Description. In CVPR.\n3"
  },
  {
    "doc_id": "03ee669d",
    "source_path": "data/uploads/Assignment1 Big data .docx",
    "text": "Assignment-1\n\t\t\t\t\t\t\t\t\tNithish\n\t\t\t\t\t\t\t\t\t11823599\nTask1: Hadoop & MapReduce\nTask 1A:\n\n1.Hadoop is a high-performance, open-source ecosystem with the primary function of large data processing and distributed storage. At its core is the Hadoop Distributed File System (HDFS) used to store files by splitting them into blocks and spreading the blocks across various nodes. Not only is this approach scalable, but it also supports fault tolerance based on the replication of every block on other DataNodes. A master NameNode maintains the directory hierarchy and metadata (such as block locations) of the file system, and DataNodes store and retrieve blocks.\nOn top of HDFS is Yet Another Resource Negotiator (YARN), the resource management layer of Hadoop. YARN consists of a ResourceManager—responsible for the allocation of CPU and memory to different applications—and NodeManagers, which are executed on every node to manage container execution and resource usage. This architecture enables effective utilization of the cluster by enabling multiple jobs to be executed simultaneously without clogging the system.\nOn top of these layers, MapReduce is the central data processing engine of Hadoop. A typical job is divided into map and reduce stages. In map stage, data is filtered and transformed into intermediate key-value pairs. These pairs are shuffled and sorted, and then come the reduce stage, where data is aggregated or summarized into final outputs. By distributing tasks across multiple nodes, Hadoop facilitates high parallelism and handles huge data sets with ease, becoming the backbone of most big data processing and analysis pipelines.\n\nHadoop's architecture is designed to handle vast amounts of data by distributing storage and processing across clusters of commodity hardware. At its core is the Hadoop Distributed File System (HDFS), which divides data into blocks and stores them redundantly across multiple DataNodes, while the NameNode manages metadata and coordinates data access. Complementing HDFS is Yet Another Resource Negotiator (YARN), which allocates resources and schedules tasks across the cluster. In a standard job, an ApplicationMaster negotiates with the ResourceManager to obtain containers on NodeManagers where computation actually occurs, usually based on the MapReduce programming model. MapReduce divides the workload into Map tasks that process input data in parallel and Reduce tasks that combine the intermediate results, providing efficient processing of large-scale data. This architecture is further enriched by various ecosystem components—like Hive for SQL-like querying, Pig for scripting, and HBase for real-time read/write operations—creating a comprehensive framework that is scalable, fault-tolerant, and flexible enough to support a wide range of big data applications.\n\n2. Prime number \nI have created a S3 bucket using name nithish-task1a and in that I uploaded the Jar file in it and in that I created the florder for the sample. Txt file which I kept in the file called inputfile .\n\nHere i uploaded the data set in the bucket in the inputfile\n\nIn the bucket there are 2 folders and one is the jar file and another one is the outputfile \n\nI Created a EMR for this , which I have given name PrimeNumberCheck1, in this I added the steps and changes the location to the  Public 172.31.64.0/20 us-east-1f and changed the security to the default , Now it is the initially in the starting stage and we need to wait till it reaches the Running state.\n\nFinally , after 5 min and 54 sec Elapsed time the status changes to the running status ,now we are good to run the cloud shell in the order to check the prime number \n\nI opened the  cloud shell after running the output file that was generated in the Bucket  where  I have created the bucket . when I pasted the url in the cloud shell it is showing which numbers is prime number or not \n\n1b.Describe the MapReduce Framework. Explain the workings of a MapReduce job with a detailed example"
  },
  {
    "doc_id": "03ee669d",
    "source_path": "data/uploads/Assignment1 Big data .docx",
    "text": "MapReduce is a processing engine and programming model used for processing large volumes of data in a parallel and distributed way. It decomposes a job into two major tasks: the Reduce task and the Map task. In the Map stage, input data—frequently held within a distributed file system such as HDFS—is divided into chunks of independent pieces of data. Each piece of data is processed by a mapper, and each mapper produces an output in terms of a series of key-value pairs. These key-value pairs are shuffled and sorted so that all values which belong to the same key are collated. In the Reduce phase, reducers operate on these collated values to generate a summarized or aggregated output, which is then written back to HDFS.\nConsider a step-by-step example based on the traditional \"Word Count\" problem. Suppose you have a large collection of text documents stored in HDFS and you want to count the frequency of each word across all documents. In the Map phase, each mapper reads a portion of the text, tokenizes the content into individual words, and emits a key-value pair for each word encountered—such as (\"hadoop\", 1) or (\"mapreduce\", 1). Once all mappers finish their processing, the framework performs the shuffle operation, grouping all the pairs by key so that each reducer receives a key and a list of corresponding values (for example, (\"hadoop\", [1, 1, 1, .])). In the Reduce phase, each reducer then sums these values to compute the total count of each word and outputs the final result, for instance, (\"hadoop\", 1000).\nThis process not only illustrates how MapReduce abstracts the complexity of parallel data processing but also shows how it efficiently leverages distributed computing resources. The framework handles fault tolerance by reassigning tasks if any node fails, ensuring robust execution. Additionally, the intermediate shuffle phase is optimized to reduce network congestion by sorting and grouping data close to where it is stored. In general, MapReduce is still a core paradigm for big data processing that enables developers to write fault-tolerant and scalable applications to execute on big clusters of commodity hardware.\nCore Components of MapReduce\nJob Client:\nThis is the application or user submitting the job. The job client splits the input data and sends the job configuration (the code for Map and Reduce functions, input and output paths, etc.) to the JobTracker (in older Hadoop versions) or ResourceManager (in YARN-based systems).\nTaskTracker / NodeManager:\nWorker nodes in the cluster that execute the tasks assigned by the master. Each node runs one or more tasks (Map or Reduce) concurrently. They report progress and status back to the master.\nMap Function:\nInput: A split of the overall input data.\nProcess: Each mapper reads its data split, usually line by line or record by record. It performs user-defined processing to convert each record into a collection of intermediate key-value pairs.\nOutput: Intermediate key-value pairs partitioned and sorted by key.\nShuffle and Sort:\nBetween the Map and Reduce phases, the framework automatically redistributes data based on keys. This shuffle phase groups all occurrences of the same key across the entire dataset together, ensuring that each reducer receives all the data it needs to process a given key.\nReduce Function:\nInput: A key and a list of associated values.\nProcess: The reducer aggregates or transforms these values, performing tasks like summing, averaging, or concatenating them.\nThe final output is stored in the output file, typically back to HDFS.\nOutput Collector:\nEach reducer outputs its results to the distributed file system. The output is partitioned according to the keys handled by each reducer, creating the final output of the job.\n\n1b. Design a program to check whether a number is Perfect number or not using  AWS and provide detailed explanation\n\nI created a bucket name nithishtask1b in amazon S3 in order to upload the files "
  },
  {
    "doc_id": "03ee669d",
    "source_path": "data/uploads/Assignment1 Big data .docx",
    "text": "In that nithishtask1b bucket I Uploaded  the perfectnumber jar file in the S3 bucket and created the folder in which I have uploaded the sample file \n\nI uploaded the sample .txt file to the nithishinputfile which I have created the folder \n\nThis is the perfectNumber.jar file \n\nAfter creating the bucket and uploaded the files in it , now I have created the EMR and the name I have given is the nithishtask1b and now it is currently in the Running state and we need to wait for the status to be running .\n\nNow the status changed to the Running that the Steps are correctly executed without any mistake .\n\nAfter successfully creating the EMR we can see that the nithishoutputfile/ in the S3 bucket in the nithishtask1b folder .\n\nwe can see that the EMR is successfully executed and folder is created \nNow we need to copy the URL address of all the part-r- that were generated in the bucket and copy their Url address of ip nodes and now open the cloudshell and paste the URL in their and run , I have did that for all the three generated folders in the bucket .\n               \t\tTask 2 – Hive \nTask 2A\n\n1A. Hive is an Hadoop-based data warehousing tool that supports querying and managing large data with a SQL-like language, HiveQL. Its architecture conceals Hadoop's MapReduce framework complexity, enabling users to analyze data without the need to write low-level code.\nAt the core of Hive is the Hive Driver, which is the query processor coordinator. Upon invoking a query, the Driver passes it to the Compiler, which inspects and validates the query, after which an execution plan is developed. The query is optimized by the Compiler through the translation of HiveQL into one or more MapReduce jobs or other execution engines like Tez or Spark, depending on the configuration.\nThe Hive MetaStore is a significant module that serves as a central repository of metadata. It stores the data related to the database schema, table definition, partition information, and data location. This metadata enables Hive to understand the structure of the data residing in HDFS and finally optimize query execution without scanning complete data sets.\nOnce the execution plan is prepared, the Driver sends it to the Executor, which runs the plan on the Hadoop cluster. The Executor manages the execution of MapReduce jobs (or tasks on the chosen engine) and governs their life cycle, monitoring progress and recovery from failures.\nTogether, these pieces allow Hive to transform complex data processing into SQL queries that are actionable, thereby providing robust data warehousing capability on top of Hadoop's scalable storage and distributed processing capability.\n\nApache Hive is a data warehousing and SQL-like query language built on top of Hadoop with the goal of making it simple to conduct data analysis of large datasets stored in HDFS. It hides the complexity of the underlying distributed computing infrastructures so that one can think in terms of HiveQL queries instead of MapReduce code.\nAt the center of the core architecture of Hive is the Driver, which acts as the query-processing central coordinator. After a HiveQL query is entered by a user, it is received by the Driver and passed on to the Compiler. The Compiler first parses and validates the query against the schema information available in the Hive MetaStore—a metadata store containing information such as table definitions, schemas, partition information, and data locations. This metadata repository is required since it allows Hive to learn about query optimization and data structure without reading the entire dataset.\nOnce the query is parsed, the Compiler produces an optimized plan for execution, which typically maps the abstract HiveQL into one or more MapReduce, Tez, or Spark jobs depending on cluster settings. The plan is then sent back to the Driver, which manages the life cycle of the query.\n\n2A. Perform the tasks using Hive in AWS and provide detailed explanation"
  },
  {
    "doc_id": "03ee669d",
    "source_path": "data/uploads/Assignment1 Big data .docx",
    "text": "I crated a Bucket named nithishtask2 , in that I uploaded the three files that were given movies, ratings and users \n\nhere are the files present in the bucket , that I have uploaded \n\nAfter completing the creating the bucket ,now I created the EMR using the name nithishtask2\n\nafter creating the cluster it will initially under starting stage for some time we need to wait for some time to go in to the waiting state \n\nwhen the state changed in to the waiting status now we need to make sure that we have the ssh inbound rules are present in the secdurity \n\nafter that open the terminal and enter command cd Downloads to change the directory to the downloads and after I entered command ssh -i nithishtask2b.pem hadoop@ec2-44-200-95-47.compute-1.amazon.com and entered yes , now it changes to Hadoop .\n\nnow we change to hive by giving command Hive and now it changed too hive state and I created the table for every file like movies , rating and users and loaded the data in the tables and obtained the data successfully and select * from table_name to view the data present .\n\nCREATE TABLE movies_nithish ( movieId STRING, title STRING, genres STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\nTo load the data in the table \nLOAD DATA INPATH 's3://nithishtask2a/nithishtask2a/movies.csv' INTO TABLE movies_nithish;\n\n-- Create the movies_nithish table\nCREATE TABLE movies_nithish (\n    movieId STRING,\n    title STRING,\n    genres STRING\n)\nROW FORMAT DELIMITED \nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE;\n-- Load data from S3 into the table\nLOAD DATA INPATH 's3://nithishtask2a/nithishtask2a/movies.csv' INTO TABLE movies_nithish;\n\n-- Create the Ratings table\nCREATE TABLE ratings (\n    userId INT,\n    movieId INT,\n    rating FLOAT\n)\nROW FORMAT DELIMITED \nFIELDS TERMINATED BY ','\nSTORED AS TEXTFILE;\n\n2. For movies table: List all movies with genre of movie is “Fantasy” and “Adventure\nselect moviesId,title ,genres from movies_nithish where genres like ‘Fantasy’ and genres like ‘Adventure’\nI used this command to obtain the list , in the below screen short it is present \n\n3. For Ratings table:List movie ids of all movies with rating equal to 4\nselect movieId from Ratings where rating = 4; and obtained the results and where present in the below screenshorts \n\n4. Find top 10 highly rated \"Comedy\" movies with descending order of rating\nSELECT m.movieId, m.title, AVG(r.rating) AS avg_rating\nFROM movies_nithish m\nJOIN Ratings_nithishtask r\nON m.movieId = r.movieId\nWHERE array_contains(split(m.genres, ','), 'Comedy')\nGROUP BY m.movieId, m.title\nORDER BY avg_rating DESC\nLIMIT 10;\n\nTask 2b:\nDiscuss the significance and differences between ORDER BY, CLUSTER BY,SORT BY and DISTRIBUTE BY clauses in HiveQL with examples. Implement anyone query using the given datasets and display the output."
  },
  {
    "doc_id": "03ee669d",
    "source_path": "data/uploads/Assignment1 Big data .docx",
    "text": "ORDER BY\nThis operation makes one reducer sort all the records in the final result. Since it guarantees that the output is globally sorted, it can be a performance bottleneck on very large datasets, since all the data passes through a single node. For instance, using ORDER BY ensures the entire result set is strictly in order.\nSORT BY\nSORT BY sorts data for every reducer separately. That is, the output of every reducer is sorted but the final output may not be globally ordered for all reducers. SORT BY is scalable over ORDER BY since it employs multiple reducers, which is helpful when handling large datasets if a global sort is not required.\nDISTRIBUTE BY\nThis directive specifies the distribution of rows to reducers. It makes sure that all rows with the same column value go to the same reducer. DISTRIBUTE BY splits data between reducers but doesn't sort the data. This is especially useful if you would like to perform subsequent operations (like aggregations) on data by some key.\nCLUSTER BY\nCLUSTER BY is a shortened syntax that encompasses the operations of DISTRIBUTE BY and SORT BY on the same column. It distributes data so that all records of the same column value go to one reducer, and it sorts data within each reducer. It localizes partitions to be sorted, which is beneficial for most downstream operations. It does not produce a globally sorted output, however.\nExample Query\nLet's say you have a table named movies_nithish with columns movieId, title, and genres. If you must partition the data by genre and sort each partition by movie title, you can use the CLUSTER BY clause. Here's an example query:\nSELECT movieId, title, genres FROM movies_nithish CLUSTER BY genres;\nExplanation of the Query:\nCLUSTER BY genres\nThis instructs Hive to send all the rows with the same genre value to the same reducer. Within that reducer, the data will be sorted based on the genres column. This does not necessarily ensure an entire global order across all data, but it does ensure each partition is sorted by genre.\nWhen you run this query, its output will be divided between multiple reducers. All output from each reducer will be sorted by genre, and all rows for the same genre will be consecutive to one another. This might improve performance where local ordering is acceptable and will avoid the overhead of a full global sort that ORDER BY requires.\n\n2b.4. Perform the tasks using Hive in AWS and provide detailed explanation\na. Print the age and total count of ratings given by the userId 100\nSELECT u.age, COUNT(r.rating) AS total_ratings FROM Users_nithish u JOIN Ratings_nithishtask r ON u.user_id = r.userId WHERE u.user_id = 100 GROUP BY u.age;\n\nThis question joins the Ratings_nithishtask table and the Users_nithish table on matching user IDs to gather data about user 100. It filters the records via the WHERE clause so that the data will take into account only the details of user 100. And then groups the results by the age of the user, finding the sum total of the ratings provided by that specific user, using the COUNT function as an aggregate over ratings. Essentially, the query not only retrieves the age of user 100 from the Users_nithish table but also calculates how many ratings they've issued by tallying up the applicable entries in the Ratings_nithishtask table, showing both pieces of information side by side.\n\nBelow are the Screen shorts present \n\n2. List the count of ratings for each distinct userId\nSELECT userId, COUNT(*) AS rating_count FROM Ratings_nithishtask GROUP BY userId;\nThis request divides all the records in the Ratings_nithishtask table by the userId field, counts the number of records (i.e., ratings) for each distinct userId, and returns a list of user IDs and their corresponding rating counts."
  },
  {
    "doc_id": "03ee669d",
    "source_path": "data/uploads/Assignment1 Big data .docx",
    "text": "3.List top 10 movieId and titles with highest rating in the descending order\nSELECT m.movieId, m.title, AVG(r.rating) AS avg_rating FROM movies_nithish m JOIN Ratings_nithishtask r ON m.movieId = r.movieId GROUP BY m.movieId, m.title ORDER BY avg_rating DESC LIMIT 10;\n\nIn this query, we inner join the ratings table and the movies table on the common movieId, calculate the average rating per film, group by title and movieId, and order the results in descending order by the computed average rating. We then restrict the result to the first 10 records.\n\n4.Find the top 3 popular genre movies w.r.t total numbers of ratings received. Sort the genres in the descending order. Print the genre and the count of ratings in the output.\n\nSELECT genre, COUNT(*) AS rating_count FROM movies_nithish m LATERAL VIEW explode(split(m.genres, ',')) movies AS genre JOIN Ratings_nithishtask r ON m.movieId = r.movieId GROUP BY genre ORDER BY rating_count DESC LIMIT 3;\n\nIn this query, the LATERAL VIEW with explode(split(m.genres, ',')) creates a new row for each genre in the comma-separated list for every movie. The JOIN then links each movie (and its individual genres) with its corresponding ratings. Finally, grouping by genre and counting the records gives us the total number of ratings per genre, which is sorted in descending order and limited to the top 3."
  },
  {
    "doc_id": "84e08551",
    "source_path": "data/uploads/Paper Review 9.pdf",
    "hash": "6ff5ee609c4027b591790df604307a45537d22e85009ec02db7d9e3e4addf0bd",
    "text": "Paper Review: Fully Convolutional Networks for Semantic Segmentation (CVPR 2015) Nithish karanam  1. Paper Summary  Full Convolutional Networks for Semantic Segmentation is a foundational paper that adapts deep classification networks into dense, pixel-wise prediction networks. Traditional CNNs have largely been built to aim towards image-level classification where the prediction is one label per image. The authors introduce a variation, though, where they replace the fully connected layers with convolutional layers so that the network is predicting a spatial map of predictions. This enables the network to be able to process an image of any dimension and generate a segmentation map that classifies every one of the pixels. The novelty in the paper is the application of the skip connections and upsampling (or deconvolution) layers. Since convolutional layers will reduce the spatial size of the feature maps, the dimensions lost are restored with the help of the upsampling layers, and a clear segmentation map is formed. Skip connections from previous layers are also introduced, combining high-level context and fine details. This enables the output to have high-level context and accurate object boundary localization. The other key aspect of the paper is the capability to train end-to-end. With the conversion of standard classification networks to full convolutional networks, it is possible to train the entire network within a single framework. This eliminates the necessity to have distinct, hand-tuned feature extraction stages or post-processing methods, and the model is more efficient and effective. Overall, the paper not only shows that deep learning is applicable to semantic segmentation, but establishes a new state of the art on segmentation on the PASCAL VOC benchmark. The paper spawned subsequent work in the field, with more advanced architectures built from these early ideas. This abstract summarizes how the paper shifts the paradigm from patch-based methods to an end-to-end trainable, global system for dense prediction tasks.  2. Experimental Results  The FCN paper's experimental part is strong, demonstrating the performance benefit of full convolutional networks on hard tasks including PASCAL VOC 2011/2012. The authors test several variants of their model—i.e., FCN-32s, FCN-16s, and FCN-8s—that differ in how they upsample the feature maps and introduce skip connections. FCN-32s uses a straightforwardly simple upsampling from the most bottom layer, resulting in less accurate segmentation maps. FCN-16s and FCN-8s, on the other hand, gradually combine features from preceding layers (via skip connections) to improve segmentation, with FCN-8s being the highest in resolution and performance. The key measure in these experiments is mean Intersection over Union (mIoU), a measure of how much overlap there is between ground truth and segmentation prediction. From the results, we can see that FCN-32s performs decently, but by incorporating skip connections into FCN-16s and FCN-8s, there is vast mIoU improvement. There are qualitative results—visual segmentation maps illustrating how more refined object boundaries and details are recovered when these techniques are used. \nHere is a sample table comparing the performance disparity between FCN versions (the below values are typical):        Model\tVariant         mIoU\t(%)          Comments           FCN-32s           59.0 Coarse\tsegmentation;\tupsampling\tfrom\tlast\tlayer\tonly          FCN-16s         62.5 Improved\tdetail\tvia\tskip\tconnections          FCN-8s         64.0 Best\tperformance;\tKinest\tdetails\tcaptured   As supporting evidence to the quantitative results, the authors provide a figure showing the general architecture of the FCN model. The figure shows how an input image is subjected to several convolutional layers to obtain a feature map, and is subsequently pipelined through upsampling (or deconvolutional) layers coupled with skip connections to provide a pixel-wise segmentation map. Visualizing the setup is useful to communicate how the model recovers spatial information lost by downsampling and eventually produces more accurate semantic segmentation.  \n  3. Contributions  The most important contribution of the present paper is to introduce a full convolutional architecture that converts traditional classification networks to segmentation networks. By \n \neliminating the fully connected layers and adding deconvolution layers, the approach enables end-to-end learning for dense prediction tasks. Another contribution is to use skip connections to combine multiple layers' features, enhancing segmentation performance by balancing coarse semantic and fine spatial context. In addition to simplifying the segmentation pipeline, it makes a dramatic gain in performance on hard benchmarks. All in all, the paper paved the way to all the subsequent advances in semantic segmentation and showed that deep, end-to-end trainable models could be used to achieve state-of-the-art performance without intrusive post-processing.   4. Criticism  I'm Despite all its novelty, FCN is not without constraints. One significant problem is that the operation of upsampling may result in blurred segmentation contours, as deconvolutional layers do not necessarily restore fine lost details in downsampling. Additionally, although skip connections make the model more accurate, they introduce difficulty in optimizing the network to perform optimally. The model will also not work with highly slender or tiny objects where fine contours are important. Finally, end-to-end training of these deep networks is computationally intensive and requires hyperparameters to be carefully optimized, something perhaps not so easy to accomplish for every user. These criticisms have led to subsequent studies working to find better architectures and post-processing techniques to overcome these drawbacks  5. Reference  Long, J., Shelhamer, E., & Darrell, T. (2015). Fully Convolutional Networks for Semantic Segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
  },
  {
    "doc_id": "dacc78c3",
    "source_path": "data/uploads/Data Visualization Project final .docx",
    "hash": "16ebe2a47ce3505e885f59ce54d17b8e56192b062a2c3ee875f3674c759f1e20",
    "text": "PROJECT 1\nCINEMATIC TRENDS: EXPLORING MOVIE METADATA AND VIEWER PREFERENCES\nTeam Members: \nSashidhar Chary Viswanathula (11714360)\nDimpu Nithish Karanam (11823599)\nVidya Gangasani (11702121)\nProblem Statement:\nThe movie industry produces a huge amount of data, from movie genres and release dates, ratings and popularity metrics. But it can be challenging to understand what truly drives a movie’s success or failure. This project focuses on uncovering insights into viewer preferences, genre popularity, trends in movie ratings over time, and Profits earned by a movie.\nBy analyzing the two datasets of movie metadata and ratings, we plan to answer questions like:\nDo certain genres consistently receive higher ratings?\nHow does a movie's release year or runtime influence audience perception?\nAre there patterns in viewer behavior that correlate with popularity or average rating?\nWhat is the average budget of a genre, and average profits earned?\nWe are focusing on communicating these insights through interactive and engaging dashboards that reveal how people engage with films.\nWorkflow Diagram:\nThis is the Workflow diagram for the project 1, We started off by collecting the dataset from Kaggle and loading it to the Google collab, after that preprocessed the dataset using Python and loaded the preprocessed dataset into power BI to generate meaningful insights and visualizations.\nData Abstraction:\nFor this project, we are using datasets from Kaggle, which are publicly available and have rich information about the movies and their ratings. We’ve used a structured dataset, but it has complex columns which have a JSON type in it. We processed the JSON type columns to get the data we needed from those columns.\nmovies_metadata.csv (From Kaggle – TMDB dataset):\nContains information about movies like titles, genres, budget, revenue, runtime, popularity, release dates, production companies, and more. This will help us analyze movie characteristics and metadata.\nratings.csv (From Kaggle – TMDB dataset):\nThis dataset includes over 100,000 user ratings for movies. Each rating includes a user ID, movie ID, rating score, and timestamp which allows us to evaluate user behavior, preferences, and seasonal trends based on the ratings.\nDataset Merging and Final Data Preparation :\nTo enhance the depth of our analysis, we merged two datasets:\nmovies_metadata.csv: Contains detailed movie information such as genre, budget, revenue, popularity, runtime, and release date.\nratings.csv: Contains over 100,000 user ratings, including user IDs, movie IDs, rating scores, and timestamps.\nThe merged dataset, named movies_data.csv, combines both movie characteristics and viewer behavior insights.\nThis enriched dataset enables a more comprehensive exploration of the relationships between movie attributes and audience preferences\nKey Highlights:\nIntegration of movie metadata with user rating patterns.\nAbility to analyze correlations between production features (e.g., genre, budget) and user engagement (e.g., ratings, vote counts).\nFoundation for creating deeper, more meaningful visualizations and insights.\nBelow   is a sample view of the final merged dataset:\nData Transformation:\nHere we can see that the dataset is loaded using pandas framework and df.head() is called to expose the first five rows. The dataset is not clean and transformed. There are many unwanted columns, null values and nested data.\nDropped unnecessary columns and retrieved the genre form the nested list.\nNow we can see that the dataset is cleaned and transformed.\nHere we have changed the datatypes of the columns to their appropriate datatypes like int and float. The dataset also have lots of null values, which are handled in the next step.\nHere the dataset has no null values because they are dropped. Therefore, our dataset is clean and transformed ready for visualizations.\nTarget: \nWe are planning on analyzing:\nRevenue VS Budget Analysis\nTop genres by average rating\nTrending popularity over years\nRuntime distribution\nMovies by language\nTop production companies by revenue\nVote count vs vote average\nActions:\nFiltered the data that doesn’t make sense or bring any value to the project. Like null values, unwanted columns etc...\nAggregated the columns to get more granularity on the project and much more summarized analysis.\nFound relation between budget, revenue, release date, popularity.\nUtilized suitable visualizations to better understand the data. (E.g.: bar chart, scatterplot etc...)\nTool used for visualization:\nFor this project, we have utilized Power BI to generate visualizations because it is easy to use interface and data loading is easier and flexible in this platform.\nData Loading:\nVisualization 1: REVENUE VS BUDGET OVER TIME\nThis visualization compares total revenue vs total budget per year which is in the bar chart and trends of both overtime in the line chart. This page also has highlights summary KPIs for budget utilization percentage, total revenue, total budget, profit.\nKPI’s: By seeing the KPIs, we can say that the budget utilization is at 0.4%, which is extremely low, indicating very low budget utilization. The Profit is 255 billion, total budget is 170 billion, total revenue is 425 billion. Despite a low budget utilization percentage, the industry has generated a substantial profit of 255 billion, showing a strong return on investment over the observed period.\nLine Chart:\nThe above chart shows the sum of budget and revenue by year from 1995 to 2020. As we can see, budget (Orange line) remains relatively studied between six billion and 10 billion from 2000 onward and revenue (Pink line) shows an upward trend, peaking sharply at 30 billion in 2018 then dropping to zero in 2020 (Possibly due to Covid 19 shutdowns).\nStacked Column Chart:\nThe above stacked column chart shows revenue vs budget over the years yellow bars represent budget orange bars represent revenue. There is a visible gap between the two which is the two bars. Revenue bars Are always higher, signaling profit every year. The trend becomes more significant post 2005, showing increased revenue even though budgets grow only slightly.\nInsights:\nBy observing this PBI report we can say that the movie industry has maintained a healthy financial trend over the years, with profits increasing significantly relative to budget. The 30 billion revenues in 2018 was the highest performing year the impact in 2020 is worth highlighting.\nVisualization 2: VOTE AVERAGE OF GENRES\nThis visualization represents average voting votes of genres. This is a horizontal bar chart displaying the average vote rating for each movie genre we have used a dax measure called average vote rating = AVERAGE (modified_movies_metadata[vote_average]). The slicers (Year, language, production company) Remind the same, enabling interactive filtering for deep exploration.\nInsights:\nOn the X axis we have average vote rating and on the Y axis we have genre. This chart displays the average audience rating for each movie genre based on voting data from the data set it allows us to compare how positively or natively weas have rated different genres on average.\nBy seeing the visualization, we can say that animation has a rating of 6.3, music 6.2, history or war or crime 6.0 are the highest rated genres. TV movie is 5.3 and thriller 5.3 are underperforming, possibly due to lower budgets, production quality or niche appeal.\nVisualization 3: TRENDING POPULARITY OVER YEARS\nThe above line chart shows the sum of vote count by year from 1995 – 2020. Here I’ve utilized two KPIs which are average board rating and total vote count and two filters which are genre and language.\nBy observing the chart, from 1995 to 2005 a steady slow growth in both counts from 0.04M to 0 .12M which tells us that the digital movie platforms or metadata sources were limited in audience interaction. From 2006 to 2014 a significant spike from 0.16M to a peak of 0.35M votes in 2014 says that there is a sharp price in increased digital engagement, due to the rise of online movie databases, streaming platforms or mobile access to content from 2015 to 2018 vote counts remain relatively high indicating sustained viewer activity from 2019 to 2020 a dynamic drop to zero votes this is most likely due to disruptions like the COVID 19 pandemic which stalled movie releases and audience interaction.\nInsights:\nWe can say that there is a clear correlation between year and digital audience engagement watch rise steadily with Internet and streaming growth. The sudden drop in 2019 to 2020 is because of the global impact of COVID 19 the inclusion of genre and language filters alert us for detailed investigation into how specific categories or language based films trended in popularity overtime.\nVisualization 4: MOVIES BY LANGUAGE\nIn this Visualization we have used two charts, one is doughnut chart, and one is bar chart. Represents a distribution of movie count by the original language in which they were produced. There are also two KPIs here, one is average vote rating, other is total vote count.\nDonut Chart: Percentage of language:\nBy observing the donor chart, we can say that English is the dominant language accounting for 69 .96% of all movies. Other languages like Hindi are 5.2%, French 2.9%, Japanese 2.7%, Spanish 2.4%, Korean 1.6% and rest other languages are less than 1%. \nThis shows a heavy lean towards English language films, which is common in global movie datasets however non-English films do show up in notable volumes, especially from Bollywood.\nBar Chart: Revenue by language:\nEnglish not only dominates in volume but also in total revenue reaching over 400 billion non-English languages such as Hindi Japanese Spanish contributes smaller but still visible revenue figures. Each genre is color coded to show the difference.\nEnglish language films dominate both quantity and profitability, but diverse journals and substantial revenue exists in global markets too, especially in highly specialized genres.\nInsights:\nThis Visualization provides a clear global distribution view for content acquisition teams, showing where the majority of content originates from. Marketing or streaming platforms, helping tailor regional catalogs. production studios, Identifying untapped language - genre revenue pairs.\nVisualization 5: \nVOTE COUNT VS AVERAGE RATING – REVENUE-WEIGHTED SCATTER PLOT\nThe above chart is a scatter plot on X axis we have sum of word count on Y axis we have sum of vote average bubble size is proportional to revenue. Disk slatter plot allows us to analyze three performance metrics simultaneously.\nHow many people voted for a movie? \nHow highly did those people rate the movie? \nHow much revenue does the movie make?\nVisually we can observe that a majority of bubbles cluster around the mid lower Y axis indicating a concentration of average rated movies rating between five and 10. The horizontal spread of bubbles shows that some movies have very high vote counts exceeding 15,000 larger bubbles are generally on the right side which tells us the movies with more votes tend to generate modest revenue.\nInsights:\nBy observing this scatterplot, we can say that high revenue is not equal to high ratings because many large bubbles are around the average rating 5 to 7. Success of a film lies in the mid rating but high vote count zone mainstream blockbusters that receive wide attention but average ratings high rating, low vote low revenue films suggest hidden gems or niche productions loved by a small but passionate audience.\nVisualization 6: TOP PRODUCTION COMPANIES BY REVENUE\nThis visualization is a combination of bar and line chart. X axis we have production company on the Y axis we have total revenue, and the line indicates profit this chart compares revenue vs profit for top production companies. The table visualization has revenue earned in millions and total movies produced\nThe chart tells us Walt Disney Pictures has a revenue of 37 billion with a profit of 25 billion. By this we can say that Disney is the undisputed leader with strong margins and a large catalog of 201 movies. Paramount pictures and Universal pictures are two production companies whose revenue is close to Disney and profit is slight lower than Disney around 22 billion to 24 billion. which accounts for 260 plus movies from each production company. As a chart moves to the right side revenue and profit both decline but profit decreases more steeply than revenue for example Summit Entertainment and Warner Bros make 7 billion and 8 billion, with profits of one to 2 billion, suggesting higher costs or low ROI.\nMatrix visualization adds important context by showing volume of production such as universal pictures leads in total movies of 39 dream works animation with fewer titles of 29 studios like Marvel Studios and legendary pictures appear mid table with fewer movies but relatively high revenues, showing quality over quantity success.\nInsights:\nBy observing the whole visualization, we can say that Walt Disney’s dominance in both volume and profitability confirms its position as a market leader although there is a noticeable correlation between scale and success but not always fuse studios with small catalogs deliver strong returns companies with higher output but lower profits might need operational or content strategy adjustments.\nVisualization 7: GENRE-LANGUAGE HEATMAP\nThe above visualization is a heat map which shows average votes by genre. In category axis we have genre and original language, and the metric displayed here is voting average. This heat map is a multilayered analysis which shows how each genre performs in terms of audience rating and which original language versions of the genre received higher or lower ratings. Each small tile represents a unique combination of genre plus language and is proportional in size based on the number of titles and displays the average via rating for that combination.\nInsights:\nBy observing the heat map, we can say that drama has the widest representation, indicating high volume and variety across languages like animation, foreign and documentary also show healthy ratings across multiple languages. For example, in the tool tip romance movies in Russian average a 5.5 rating which shows moderate performance foreign genre entries often hover above 6.5, showing strong audience appreciation for international cinema.\nSome genres like TV movie horror tend to have lower ratings or fewer language variations. Crime thriller and mystery genres also display many language tiles with ratings close to the global average of 5.66.\nVisualization 8: RATING DISTRIBUTION\nThis visualization shows the rating distribution on the X axis we have ranges like 02468 and 10 on the Y axis we have the number of movies that fall into each rating bin. For this visualization I’ve binned the vote average field with the size of two, ranging from 0 to 10. The rating buckets are 0-2,2-4,4 -6, 6 - 8, 8 - 10 and exactly 10.\nInsights:\nBy observing the chart, most movies are rated between 4 to 8, which captures 23,000 of the total movies out of 27,000. Specifically, 6 to 8 range is the peak, with 13,000 movies rated here. Is also strong with 1000 entries. Only 1000 movies scored between 8 to 10 and even fewer 100 hit a percent score of 10. Similarly, only 1500 scored between zero to two indicating few movies are universally disliked. Ratings show a bell-shaped distribution towards the center.\nStorytelling Approach:\nStage 1: understanding and Exploring the Dataset:\nWhat does raw data look like?\nThe dataset, as seen in the Fields pane, is from a table called modified_movies_metadata. It contains attributes such as:\nIdentifiers: id, original_title, imdb_id\nDescriptive: genre, original_language, production_company\nPerformance metrics: budget, revenue, vote_average, vote_count, runtime, ROI, Profit, Budget Utilization %\nCalculated fields: Average ROI by Genre, Rating Bins, etc.\nWhat key variables and trends stand out?\nRevenue & Profit Trends: Movies have consistently generated strong profits, with a total revenue of 425bn and profit of 255bn. Walt Disney, Paramount, and Universal are the top earners.\nViewer Sentiment: Average rating is 5.66 across 27,038 movies. Majority of movies fall in the 6-8 rating bin, indicating a large volume of moderately liked content. Animation, Music, and History genres received the highest ratings.\nLanguage & Genre: English (69.9%) dominates, but foreign-language films also show strong niche performance. Performance of genres varies by language. e.g., Romance in Russian vs. Comedy in Korean.\nViewer Engagement: Vote count peaked around 2014-2018 and dropped off in 2020 (likely due to pandemic). High vote count doesn’t always equal high ratings, popularity and quality are not strictly correlated.\nStage 2: Data Processing and Correlation Analysis\nIn this stage, we move from descriptive analysis to uncovering underlying patterns and relationships between variables. Key operations include:\nCleaning missing or inconsistent values\nMerging rating and metadata tables\nCreating new fields (e.g., Rating Bins, ROI, Profit)\nGrouping and binning numeric fields for trend analysis\nCorrelation Analysis:\nVote Count vs Rating vs Revenue (Bubble Chart): High revenue movies often lie in the mid-rating zone (5–7), not necessarily the highest-rated. Some well-rated movies have low vote counts, revealing undiscovered gems.\nGenre vs Average Rating: Top genres in ratings: Animation, Music, History. Bottom genres: TV Movie, Horror. Supports genre-based content curation or investment strategies.\nRuntime vs Rating: Longer movies may have better storytelling and thus higher ratings. To be validated in later phases with scatter or trend lines.\nProduction Companies: Revenue vs Profit: Walt Disney leads with high revenue and profit efficiency. Marvel Studios shows high ROI with fewer productions. Smaller companies tend to have lower revenue-per-movie.\nPhase 2:\nCinematic Trends – Building an Interactive Dashboard for Movie Analytics\nCSS Styling (responsive.css and style.css):\nWe designed the website layout and appearance using two main CSS files:\nstyle.css handles the global styles, color themes, typography, and layout structure.\nresponsive.css ensures the dashboard is fully responsive across devices by adjusting the layout for different screen sizes.\n Dataset Preparation (movies_data.csv):\nThis file represents the final merged dataset combining movies metadata and user ratings.\nIt was created after cleaning, preprocessing, and joining the original Kaggle datasets to enable comprehensive analysis.\nInteractive Visualizations (JavaScript Files):\nCustom interactive visualizations were created using:\ninteractive-visualizations.js for overall event handling and chart rendering.\nSpecific visualization scripts like language-popularity.js, genre-ratings.js, etc., to generate different interactive dashboards.\nLibraries used include D3.js, Plotly.js, and Vega-Lite for dynamic chart generation.\nWeb Structure (index.html and Other HTML Files)\nThe index.html file organizes the website structure, including:\nNavigation menu\nLinking CSS/JS files\nEmbedding visualization pages Each analysis section like \"Revenue vs Budget,\" \"Trending Popularity,\" etc., is structured into individual HTML files under the pages/ directory.\nDocumentation (README.md)\nThe README.md file provides a project overview, listing the technologies used and describing the main features, visualizations, and goals of the project.\nIt serves as a guide for users and collaborators visiting the GitHub repository.\nWebsite Overview:\nWe developed an interactive website named Cinematic Trends to explore movie metadata and viewer preferences through dynamic, embedded visualizations. The site is structured with sections such as Home, Visualizations, Methodology, Conclusion, and Team\nThe website provides multiple interactive visualizations focused on analyzing key cinematic metrics such as Revenue vs Budget, Genre Ratings, Trending Popularity, and more. Each visualization offers dynamic filtering and deeper exploration options.\nWebsite Link: https://github.com/sashi789/SDV_project\nDashboard View: Revenue vs Budget Analysis:\nIn this dashboard, we explore the financial relationship between movie production budgets and their corresponding box office revenues.\nUsers can interactively filter the data by year and genre, enabling deeper exploration of specific trends over time.\nThe visualization provides a dynamic scatter plot that reveals clusters of financially successful films and helps identify outliers with high budgets but low returns.\nBy embedding this analysis into the Cinematic Trends website, we allow users to intuitively understand how budget allocations have historically influenced financial performance in the movie industry.\nDashboard View: Genre Ratings Analysis\nThis interactive dashboard analyzes the average viewer ratings across different movie genres.\nUsers can dynamically filter by year, language, and production company to observe how audience preferences change over time or across regions.\nThe visualization helps identify which genres consistently receive high ratings and which ones underperform, offering valuable insights into audience sentiment and genre popularity.\nEmbedding this analysis within the Cinematic Trends website allows users to explore genre-specific performance patterns in a user-friendly, visual format.\nDashboard View: Trending Popularity Analysis\nThis dashboard visualizes the changing popularity of movies over time based on viewer engagement metrics, such as vote counts.\nUsers can interactively filter the data by genre and language to explore how popularity trends differ across categories and regions.\nThe visualization highlights key periods of increased viewer activity, such as the rise of streaming platforms, and also reflects major disruptions like the COVID-19 pandemic.\nBy embedding this visualization within the Cinematic Trends website, users can intuitively analyze historical audience behavior and discover how external factors have influenced movie popularity over the decades.\nDashboard View: Movies by Language\nThis dashboard explores the distribution of movies based on their original production languages.\nUsers can view the number of movies produced in each language, as well as key performance metrics such as average rating and total vote count for each language group.\nThe visualization reveals the dominance of English-language films while also highlighting the significant contributions of other languages like Hindi, French, Japanese, and Spanish to the global cinema landscape.\nEmbedding this analysis within the Cinematic Trends website provides users with an intuitive understanding of global film production diversity and regional audience reach.\nDashboard View: Vote Count vs Rating Analysis\nThis interactive dashboard analyzes the relationship between the number of audience votes a movie receives and its average rating.\nIt helps uncover trends such as whether highly rated movies attract more votes, or whether popular movies often have average ratings.\nUsers can explore how vote volume and critical reception align across different genres and languages, providing insight into audience engagement versus perceived quality.\nEmbedding this visualization on the Cinematic Trends website enables a deeper understanding of how audience size correlates with movie success metrics.\nDashboard View: Top Production Companies\nThis dashboard ranks the top production companies based on their total movie revenue and profits.\nUsers can compare studios like Disney, Universal, and Paramount to understand differences in financial performance and production volume.\nThe visualization reveals which companies consistently deliver high revenue films, and highlights strategic differences between studios focusing on quantity versus quality.\nEmbedded within the Cinematic Trends website, it offers users an industry-wide perspective on financial success and production strategies\nDashboard View: Genre-Language Heatmap\nThis heatmap explores how different movie genres perform across various languages in terms of average ratings.\nEach cell represents a genre-language combination, allowing users to discover cultural preferences and rating patterns across global cinema.\nIt highlights genres that perform strongly across multiple languages (such as Animation and Documentary) and those that vary significantly based on cultural factors.\nEmbedding this multilayered visualization enhances user exploration of cross-cultural genre success on the Cinematic Trends website.\nDashboard View: Rating Distribution\nThis dashboard visualizes the distribution of movie ratings across defined rating bins (e.g., 0–2, 2–4, 4–6, etc.).\nIt shows how most movies cluster around average ratings, while very high-rated or very low-rated movies are rare.\nThis helps users understand overall audience scoring behavior and identify trends like the bell-shaped (normal) distribution of ratings.\nHosted on the Cinematic Trends website, this dashboard makes exploring rating patterns intuitive and interactive.\nDashboard View: Movie Data Analysis\nThis visualization presents an interactive D3.js-based view of movie rating distributions, focusing on the spread and density of viewer ratings.\nIt provides a more granular, visually engaging exploration of how ratings are distributed across thousands of films, offering a deeper layer of user engagement compared to traditional charts.\nThis integration of custom visualization tools enhances the analytical capabilities of the Cinematic Trends platform\nMethodology:\nData Collection\nGathered two primary datasets from Kaggle’s TMDB Collection: movies_metadata.csv and ratings.csv.\nSelected datasets for comprehensiveness, reliability, and relevance to cinematic trends and viewer behavior.\nFocused on movie details (budget, revenue, genres) and user ratings to build a robust foundation for analysis.\nData Preprocessing\nRemoved duplicates, handled missing values, corrected inconsistencies for clean data.\nTransformed complex fields (e.g., JSON genre parsing) and normalized monetary values.\nMerged metadata and ratings datasets into a final enriched dataset (movies_data.csv).\nAnalysis Approach\nPerformed Exploratory Data Analysis (EDA) to understand distributions and initial patterns.\nConducted Temporal and Comparative Analyses across genres, languages, and time periods.\nIdentified significant patterns and correlations while ensuring statistical validity and minimizing bias.\nVisualization Techniques\nCreated scatter plots, bar charts, line charts, heatmaps, and choropleth maps for insights.\nBuilt interactive dashboards and custom visualizations using Power BI, D3.js, and Plotly.js.\nFocused on clarity, simplicity, and user-driven exploration with responsive design principles.\nValidation and Quality Assurance\nCross-validated results using multiple analytical methods to ensure reliability.\nConducted sensitivity analysis to understand impacts of assumptions and data limitations.\nPerformed peer review and user testing to refine visualizations and improve usability.\nConclusion:\nOur project, Cinematic Trends, successfully explored key insights into the movie industry by combining metadata and user ratings from large datasets.\nKey Findings:\nBudget vs Revenue Relationship:\nHigher budgets often correlate with higher revenues, but mid-budget films frequently show better ROI. Genre type significantly impacts financial success.\nGenre Preferences:\nGenres such as Animation, Documentary, and Adventure consistently receive higher viewer ratings, while Horror and Thriller genres show more polarized audience reactions.\nLanguage and Global Appeal:\nWhile English-language films dominate, several non-English productions, particularly in Korean, Spanish, and French, have achieved significant international success.\nTemporal Trends:\nMovie popularity and viewer preferences have evolved over time, with the rise of sci-fi and superhero genres and an overall increase in average ratings post-2000.\nProject Outcome:\nDeveloped an interactive web platform for dynamic exploration of cinematic data trends.\nProvided users with the ability to filter, explore, and interpret movie insights in an engaging, accessible way.\nSuccessfully transitioned from static dashboards to a fully deployed web-based interactive system.\nWork Management:\nAs mentioned in the proposal below, tasks have been completed for the project 1:\nExplored metadata features like genres, average ratings, popularity\nDetected basic correlations such as genre vs. average rating, release year vs. popularity\nIdentified outliers and missing data\nGenerated initial visual dashboards\nData Cleaning & Transformation:\nHandle missing or inconsistent values in the movies_metadata.csv file (e.g., missing runtimes, differently formatted release dates).\nConvert data types (e.g., release dates to datetime, budget/revenue to numeric).\nExploratory Data Analysis (EDA):\nAnalyze the distribution of movie genres, release years, and popularity scores.\nVisualize the average user rating across different genres and decades.\nCreate basic visualizations using Power BI \nInitial Correlation Analysis:\nIdentify relationships between variables such as:\nMovie runtime vs. average rating\nBudget/revenue vs. popularity\nNumber of ratings vs. movie rating score"
  },
  {
    "doc_id": "ef94e603",
    "source_path": "data/uploads/Deep learning paper 1 .pdf",
    "hash": "7c8fba7878698530cceb090e76eeb23df6fc2718f162c2e0437fcc7b45d67470",
    "text": "Review of \"ImageNet Classification with Deep Convolutional Neural Networks\" Nithish Karanam,I’d-11823599 Department of Artificial Intelligence , University of North texas   1. Paper Summary  The paper Summarizing the deep convolutional neural infrastructure known as AlexNet was first revealed in the 2012 CE function in ImageNet categorization, which accelerated deep learning within the mass eye. In addition, in addition to a million photographs in the ImageNet dataset, Thymine, which represents a thousand different types of objects, the author used to train his network. GPU-accelerated deep learning has been one of the most significant improvements in GPU-accelerated deep learning since its introduction, with the simplified use of GPUs to perform the demanding calculations required to rain multiple layers. In addition to highlighting the advantages of using the Rectified Linear Unit (ReLU ) as an activation function, which accelerates convergence and reduces the subject of disappearing gradient. In order to avoid overfitting, they introduced a dropout regularization technique in a completely connected layer and statistical enrichment methods including random crop, horizontal somersault, and small color variation to avoid overfitting. Therefore AlexNet surpasses the previous methods by a huge margin, achieving a peak- 5 error measure of approximately 15.3 % in the large extent of ImageNet Visual appreciation obstacle. Besides its precise architecture, the study shows how careful design decisions of nervous associates and hardware development second might significantly increase performance. As a result, it determines the pace of subsequent studies, resulting in increasingly complex and more detailed models driving computer vision developments.\t\t2. Experimental Results Krizhevsky, Sutskever, and Hinton experimented with their land-interrupt paper on the experimental findings of the year 2 thousand and twelve ImageNet vast Gradation Visual Appreciation Issue (ILSVRC ), which together with the era possessed approximately 1.2 million training pictures covering 1000 object types. When compared to traditional methods based on manually generated features, AlexNet significantly reduces classification errors. In particular, they supplement the training information with horizontal reflections, random likeness translation, and multi-crop strategies, which contribute to generalization and to preventing overfitting. The use of Rectified Linear Units (ReLUs ) made the training of infrastructure more efficient, and the optimization was driven by stochastic gradient descent alongside momentum. AlexNet has been a resounding winner of the ILSVRC 2012 competition, achieving an impressive oinnacle - 5 slip-up gauge of approximately 15.3 % after the end of the training, which was significantly lower than the previous record of roundabout 26 %. This meaningful growth demonstrated the utility of GPU acceleration for large-scale training objectives and immediately attracted interest in deep networ k. Fig. 1 Correlation with other errors Algorithm meter Featur vitamin E oinnacle - 5 error (ILSVRC ) Speed (Federal Protective Service) Propose 5 hundred AlexNet (both statistics Aug ) Raw pixel 28.0 % 5 0 AlexNet (full ) Raw pixel + data Augmentatio n 15.3 % 4 5 alternative algorithms ZFNet [ 1 ] ameliorated architecture with deconvolution layer s 11.7 % 3 0 GoogLeNet [ 2 ] Inception faculty for multi - extent attribute fusio n 6.7 % 2 0 VGG [ 3 ] Deeper stack of 33 whirl layer s 7.3 % 1 5 ResNet [ 4 ] Residual connection to counter disappear grad s 3.5 % 1 0 Tab. 1 Experimental effects of inflation and additional correlations.\n     Fig. 1: Comparison with other errors  Algorithm  Feature Top-5\tError\t(ILSVRC)  Speed\t(fps) Proposed\tAlexNet\t(No\tData\tAug) Raw\tpixels 28.0% 50 \nAlexNet\t(Full) Raw\tpixels\t+\tData\tAugmentation 15.3% 45 Other algorithms ZFNet\t[1] Improved architecture with deconvolution layers 11.7% 30 GoogLeNet [2] Inception modules for multi-scale feature fusion 6.7% 20 VGG\t[3] Deeper stack of 3×3 convolution layers 7.3% 15 ResNet\t[4] Residual connections to counter vanishing grads 3.5% 10 Tab. 1: Ablation experimental results and further comparison."
  },
  {
    "doc_id": "ef94e603",
    "source_path": "data/uploads/Deep learning paper 1 .pdf",
    "hash": "88c49450a3581d0ecaceef79967404b8e09a5a1df0565255adb4d0805c24d19a",
    "text": "chitecture with deconvolution layers 11.7% 30 GoogLeNet [2] Inception modules for multi-scale feature fusion 6.7% 20 VGG\t[3] Deeper stack of 3×3 convolution layers 7.3% 15 ResNet\t[4] Residual connections to counter vanishing grads 3.5% 10 Tab. 1: Ablation experimental results and further comparison.\n\n3. Contribution 3.1 Implementation of ReLU Activations ReLUs (rectify Linear Units) accelerate convergence substantially and remove gradient difficulties in deep architecture alongside sigmoid activation. 3.2 Effective Regularization Productive Regularization data augmentation (random crop, somersault, color jitter) and dropout in a completely connected layer are equally important for reducing overfitting. The above-mentioned method has become standard techniques in the next deep learning facility. 3.3 GPU-Driven Training GPU - driven train The sheet shows how the essential GPU funding is for a train with long, deep links. This penetration has affected the approach to improving the code for parallel calculations. 3.4 Inspiration for Future Architectures inspiration for future architectures AlexNet provides a blueprint for other alliances such as VGG, GoogLeNet, and ResNet. The value of a more sophisticated model illustrates its own triumph.  4. Criticism 4.1 Resource Demands Supply Demands Training AlexNet required high-end GPU hardware, which was more common in 2012 CE. Researchers who miss big GPUs discover the need to retrofit or extend the consequences. 4.2 Over-parameterization Excessive parameterization of the large amount of parameter risk overfitting of the network. Although these approaches acknowledge dropout, a further compact model with a smaller parameter set may obtain the same results together with a smaller parameter set. 4.3 Limited Architectural Variants Restrictive Architectural Variants The document does not deeply explore the unique layer configuration (e.g. , over depth, and varying kernel size ). After that, they polish and expand on top of the basic AlexNet theory. 4.4 Transfer Learning Not Fully Addressed Transfer education nay is not used in academic writing entirely handled Although the writer briefly mentioned a few generalization elements, they make nay examine how effectively the AlexNet feature might be transferred to another ocular undertaking otherwise. Reference A. Krizhevsky, I. Sutskever, G. Hinton, \" ImageNet Classification alongside Deep Convolutional Neural Webs,\" Neural Intelligence Processing Algorithms (NeurIPS), 2012 CE."
  },
  {
    "doc_id": "7c7d0000",
    "source_path": "data/uploads/deep learning paper 3.pdf",
    "hash": "344a771ae41c6b9577c4dfe1ad09e4b417d7990ea46cee3ccb7c82d6c1b0f84e",
    "text": "Review of “Very Deep Convolutional Networks for Large-  Scale Image Recognition” Nithish karanam  1. Paper Summary  In this paper, Simonyan and Zisserman present what has been commonly known as the VGG network—a deep convolutional network that set new records in image classification. The key insight here is to use multiple stacked tiny 3×3 convolutional filters, rather than single-shot larger filters. This not only enables deeper network (up to 19 layers in some models) but also keeps the number of parameters within reasonable bounds. By training such deep models on the large ImageNet dataset, the authors demonstrate systematic improvement in recognition performance with network depth. I appreciate that the paper is explicit about the network architecture and training process, including design choices like ReLU activations, max-pooling layers, and fixed filter sizes. I believe what makes this work excellent is the simplicity—the homogeneous architecture is straightforward to implement, understand, and build upon. Overall, the paper shows that deeper networks learn more abstract and robust features, with the final outcome of state-of-the-art performance on image classification. In this foundational paper, Simonyan and Zisserman present a new deep convolutional network architecture, now known simply as VGG, that achieves a significant boost in large-scale image recognition. The authors argue that by stacking multiple small 3×3 convolutional filters, one can build very deep networks (up to 19 layers) that learn highly complex and discriminative features, yet remain relatively simple and uniform in design. This approach is contrary to previous architectures that had a tendency to apply larger filters, and it indicates that depth is an important factor for achieving greater accuracy.  2. Experimental Results Experimental work in the paper is thorough and persuasious. The authors compare several variations of the VGG network (e.g., VGG-11, VGG-16, and VGG-19) in the ImageNet competition. Their experiments show that the increase in the number of layers reduces the top-5 error rate gradually. They also discuss accuracy vs. computational cost trade-offs. For instance, deeper networks are more memory hungry and slower to infer, but the gain in the recognition accuracy is worth the increasedcomplexity. I was also surprised to find that even though VGG networks are deep, using small filters keeps the parameters in check, and the models can be trained on current hardware. To further highlight the performance differences, I've constructed a demonstration table comparing hypothetical figures of different VGG models: Model Variant Number of Layers Top-5 Error Rate (ILSVRC) Parameters (Millions) Inference Time (ms) VGG-11 11 25.8% 133 25 VGG-16 16 21.3% 138 30 VGG-19 19 20.5% 144 35 This Photo by Unknown Author is licensed under CC BY 3.  \n  To further clarify the performance differences, I’ve prepared a sample table comparing hypothetical metrics of different VGG variants: Model\tVariant Number\tof\tLayers Top-5\tError\tRate\t(ILSVRC) Parameters\t(Millions) Inference\tTime\t(ms) \nVGG-11 11 25.8% 133 25 \nVGG-16 16 21.3% 138 30 \nVGG-19 19 20.5% 144 35 \n  \nThis Photo by Unknown Author is licensed under CC BY \n3. Contribution  One of the key contributions of this paper is the introduction of a very straightforward yet surprisingly effective method for deep convolutional networks: stacking multiple 3×3 convolutional filters in each layer block. This option reduces the overall architecture and, meanwhile, enables the network to learn more abstract and complex features with growing depth. Moreover, the authors systematically tried varying depths—between 11 and 19 layers—and demonstrated that deeper models work consistently better on large-scale datasets like ImageNet. By providing explicit network configurations and training protocols, they also made it easier for other researchers to reproduce and extend their results. In addition to surpassing state-of-the-art performance previously, the paper's stated design principles—i.e., uniform filter sizes, consistent ReLU activations, and carefully controlled pooling layers—had a lasting influence on subsequent work in the deep learning community, influencing a wide range of future architectures. The paper also dictates practical training heuristics—such as specific weight initialization heuristics and learning rate schedules—that have become community standard practice. Overall, the work gives a clear, modular blueprint for constructing deep convolutional networks and has formed the foundation of many subsequent innovations, with subsequent architectures like ResNet and Inception building upon the concepts of deep, hierarchical feature extraction. 4. Criticism  Even with its impact, the VGG method does have some significant limitations. First, the massive depth of such networks requires high levels of computational resources, particularly at training, which can prove prohibitive to individuals or institutions without access to high-end GPUs. The additional number of layers also results in greater memory requirements and slower inference times, potentially capping real-time applications. Moreover, while the paper highlights the benefits of employing small filter stacking, it does not compare other filter sizes or other architectural variations that would achieve similar accuracy with fewer parameters. Finally, deeper models necessarily come with greater risk of overfitting, requiring extensive data augmentation and other regularization techniques to generalize well. They highlight that while VGG design broke new ground, there is further room for improving and optimizing the design. The application of a series of fully connected layers at the end of the network also adds to the number of parameters, and thus, the model becomes overfit even with the use of data augmentation and regularization techniques. Also, although the paper succeeds in making the case for the use of small 3×3 filters, it fails to consider other arrangements that will yield similar performance improvements without the added computation requirements, such as the application of skip connections or the use of hybrid approaches. As a result, although the VGG model was a fresh benchmark when initially released, its architecture has since been mostly overshadowed by superior architectures that achieve or exceed its performance with improved speed and resource efficiency.  Reference K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” ICLR, 2015."
  },
  {
    "doc_id": "a520b841",
    "source_path": "data/uploads/deeplearning paper 4.pdf",
    "hash": "d5e48e83c0367060f689a4c5200ad87d34bc7a39986dc2b238cfb3695fcb4832",
    "text": "Review of “Deep Residual Learning for Image Recognition”  Nithish karanam  1. Paper Summary  He, Zhang, Ren, and Sun introduce a novel architecture concept called residual learning to address the challenge of training very deep neural networks. The new idea is that the network should learn the mapping of residuals instead of trying to map directly to some desired underlying mapping. That is, instead of forcing every layer to learn the entire transformation, the network learns to predict changes in terms of input. This is done by avoiding connections that allow input to skip one or more layers and contribute to a deeper layer's output. This approach addresses the degradation problem—where networks deteriorate as they become deeper—by allowing gradients to pass more easily when backpropagating. Through this, the authors were able to successfully train 50, 101, and even 152-layer networks, establishing the state-of-the-art on large-scale image recognition tasks like ImageNet.  Massive experiments on the ImageNet dataset indicate that residual networks (ResNets) dominate the previous architectures by a wide margin, with reduced top-1 and top-5 error rates. The paper also contains ablation studies and an in-depth analysis that indicate the importance of the residual connections, confirming that these shortcuts are critical to state-of-the-art performance. Overall, this work not only sets new state-of-the-art in image recognition but also transforms the way deep networks are constructed by researchers, with residual learning becoming a fundamental component in most modern designs.  2.\tExperimental Results  The authors performed deep experiments primarily on the ImageNet dataset (ILSVRC 2012) to demonstrate the validity of residual learning. They tested several residual network variants—starting from fairly shallow networks like ResNet-18 and ResNet-34 to very deep ones like ResNet-50, ResNet-101, and ResNet-152. The experimental results show that the deeper the network, the better the performance, and deeper models have lower top-1 and top-5 error rates. One of the key findings is that when the residual (shortcut) connections are taken out of these deep networks, the performance gets much worse. This ablation study confirms that the residual connections are required for the effective training of very deep networks. The paper provides extensive training curves which demonstrate faster convergence with residual connections. These graphs show the more gradual decrease of training error over time, which supports the hypothesis that residual connections facilitate easier gradient flow. In addition, the experiments provide comparisons to the then state-of-the-art models such as VGG and Inception networks. The residual networks not only obtain higher accuracy than these architectures but also maintain computational efficiency and memory demands competitive. For instance, with many more layers, ResNet-152 achieves a lower error rate while keeping the model size within practical bounds.  \n Besides classification, the authors also verified the robustness and transferability of the residual learning approach on tasks like object detection and segmentation on the COCO dataset. These further experiments illustrate the benefit of residual learning beyond image classification, indicating its potential for a variety of computer vision tasks. Overall, the experimental results provide comprehensive evidence that residual networks are deep and robust, and they readily solve the degradation problem that was present in those very deep, earlier networks. This work set a new benchmark for network performance and introduced the possibility of exploring even deeper and more complex architectures.  The next Table 1 presents hypothetical performance metrics of some of the ResNet models on ImageNet, illustrating the gain in accuracy and the computational expense incurred: Framework Layers Top-1\tAccuracy Top-5\tAccuracy Params\t(Millions) ResNet-18 \t\t\t\t\t\t\t\t\t\t18 \t\t\t\t\t\t\t69.0%    89.0%      11.7 ResNet-34 \t\t\t\t\t\t\t\t\t\t34         73.3%    91.2%      21.8 \nResNet-50           50        75.3%     92.2%      25.6 ResNet-101         100       76.4%      92.9%      44.5 ResNet-152        152       77.0%      93.3%       60.2 \n \n                            Fig 1 : ResNet\tperformance\tcomparison\tchart\t\nThis Photo by Unknown Author is licensed under CC BY \n3. Contribution  One of the biggest contributions of the paper is the proposal of residual learning, a new concept that fundamentally alters the mechanism of training deep networks. Rather than having each layer learn an unreferenced transformation, the network learns a residual function—basically the difference between the input and the desired output. This straightforward concept allows the network to learn an identity mapping if that is optimal, making it much easier for the vanishing gradient problem. Consequently, very deep networks can be trained without experiencing the degradation in performance that typically happens when layers are simply piled one on top of the other. Furthermore, the authors show that these residual networks (ResNets) are not only theoretically appealing but also practically effective. Through large-scale experiments on datasets like ImageNet, they demonstrate that deeper ResNets achieve significantly lower error rates compared to their non-residual counterparts. This breakthrough has made it possible to train networks with over 150 layers successfully, which has established new benchmarks in image recognition performance. The simplicity and elegance of the residual block structure have made it easy for other researchers to adopt and extend the idea, influencing a wide range of subsequent architectures in computer vision and beyond.Besides the technical contribution, the paper also presents practical contributions in network architecture and training schemes. The authors detail the use of shortcut connections, explain how batch normalization enables training, and include comprehensive ablation studies to confirm the usefulness of each element. These contributions not only improve our knowledge of deep network training but also provide a modular architecture that is easy to implement and build upon. Overall, the study has contributed greatly to the field and has made residual learning a cornerstone technique in modern deep learning research.  4.Criticism  Although novel in nature, the paper is not without flaws. One of the main issues is the added architectural complexity; though residual connections facilitate training, they also add more design decisions that can be challenging to fine-tune to a particular task. The paper discusses almost entirely image classification, and thus the application of the residual learning methodology to other fields (e.g., speech or natural language processing) is less fully considered. Also, while very deep ResNets have high accuracy, they are computationally and memory intensive, making them impossible to use in low-resource settings or real-time processing. Finally, while the accuracy gains are observed, the benefits over conventional architectures are sometimes incremental when transferred to tasks other than the ImageNet benchmark, suggesting that further work is necessary to fully grasp and generalize the benefits of residual learning Reference  K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” CVPR, 2016."
  },
  {
    "doc_id": "afd8f5f8",
    "source_path": "data/uploads/Activity 14 dl.pdf",
    "hash": "85f5cc7b957649732185f2f1359e553c7bf60f48b053bd59e901c45e851b5ca7",
    "text": "Paper Review: An Image is Worth 16x16 Words: Transformers for\nImage Recognition at Scale (ICLR 2021)\nNithish Karanam\n1 Paper Summary\nThis work introduces Vision Transformer (ViT), a new framework that utilizes transformer architectures, originally conceived for\nnatural language processing, for image classification. ViT divides an input image into a sequence of fixed-size patches, typically of\nsize 16x16 pixels, and considers each patch as a ”word” in a sequence. The patches are later flattened and are linearly embedded,\nwith position encodings added so that spatial relationships are preserved. The sequence thus obtained passes through a standard\ntransformer encoder that uses multi-head self-attention, thus obtaining contextualized patch representations. By representing only\nimages as sequences, the model avoids the inductive biases native to convolutional neural networks (CNNs) and relies solely on\nattention mechanisms in order to identify local and global dependencies.\nA key strength of this approach is its scalability; placed under pre-training over vast datasets like JFT-300M, the Vision Trans-\nformer (ViT) achieves performance metrics that not only match but often surpass those linked with established benchmarks such as\nImageNet using training methodologies that demand proportionately less initial data. The work includes extensive experiments that\ninvestigate factors such as patch size, model depth, and pre-training methods. Overall, the work represents a major breakthrough\nin computer vision by demonstrating the effective use of attention-driven models, specifically the transformer, for large-scale image\nclassification, thus challenging the dominant reign of traditional convolutional neural nets (CNNs) and opening the doors for further\nincorporation of attention-based models.\n2 Experimental Results\nEmpirical evaluation performed in the current research is thorough and reflects the effectiveness of the Vision Transformer. The\nresearchers perform a comparison with top architectures like ResNet and EfficientNet using metrics like top-1 accuracy and training\nperformance. One of the interesting findings of this research is that ViT achieves significantly higher accuracy upon pre-training\nover large datasets, thus reflecting its strength with more amounts of data. Aside from the improvement noted in the metrics, the\nresearch also includes ablation experiments that test the influence of patch sizes and the usefulness of position encodings on model\nperformance.\nPerformance Comparison\nTable 1 compares the Vision Transformer with popularly known CNN architectures. Although the values in the table are representa-\ntive in nature, they reflect a trend where the Vision Transformer outperforms traditional architectures with extensive pre-training.\nModel Top-1 Accuracy (%) Training Data\nResNet-50 76.0 1M images\nEfficientNet-B3 77.0 1M images\nVision Transformer (ViT-Large) 82.0 300M images (pre-trained)\nTable 1: Illustrative performance comparison of ViT with popular CNN architectures.\nArchitecture Diagram\nFigure 1 shown below, displays a schematic of the Vision Transformer model. The figure essentially depicts the initial segmentation\nof the image into equally sized pieces, which are then embedded and augmented with position embeddings before their encoding by\na transformer encoder. That particular design layout plays an important part in the Vision Transformer’s ability to learn local and\nglobal information contained in images.\n1\nFigure 1: Diagram of the Vision Transformer (ViT) architecture, illustrating patch embedding, positional encoding, and the trans-\nformer encoder.\n3 Contribution\n3.1 3.1 Novel Application of Transformers in Vision\nThe Vision Transformer is groundbreaking because it applies transformer architectures, which have revolutionized the natural lan-\nguage processing field, to the domain of visual recognition tasks. This marks a significant departure from the convolutional frame-\nworks that have dominated computer vision for a long time.\n3.2 3.2 Simplified yet Powerful Image Representation\nBy treating images as sequences of patches, ViT eliminates the need for convolutional operations while relying solely on self-\nattention to model interactions between patches. This approach simplifies the model architecture and allows it to capture long-range\ndependencies that may be challenging for CNNs.\n3.3 3.3 Scalability with Large-Scale Pre-training\nThe paper demonstrates that the Vision Transformer has good scalability with respect to larger training data. When pre-trained on\nlarge datasets, ViT achieves competitive or better performance on standard benchmarks, thus validating that transformers are a strong\nalternative to convolutional neural networks for image recognition tasks. The parallelizable nature of the model also significantly\nreduces training time.\n4 Criticism\n4.1 4.1 Quadratic Complexity of Self-Attention\nVision Transformers utilize a self-attention mechanism that computes the relationships between all patch pairs with quadratic com-\nplexity with respect to the number of patches. This component becomes a limitation when we handle very-high-resolution images or\nlong sequences because they increase computational and memory loads.\n4.2 4.2 Dependence on Massive Datasets\nWhile ViT shows impressive performance when pre-trained on enormous datasets, its reliance on such vast amounts of data can be\nproblematic for domains where labeled data is limited. This data dependency raises concerns about the model’s generalizability in\n2\nlow-data scenarios.\n4.3 4.3 Lack of Convolutional Inductive Bias\nNetworks are preferred because they have strong inductive biases that hierarchically represent spatial information while being trans-\nlation invariant. The Vision Transformer dispenses with convolutional layers and thus loses these beneficial properties, which may\nnegatively impact its performance on tasks where these biases are helpful, especially in the case of limited or poorly diverse training\nsets.\n4.4 4.4 Sensitivity to Hyperparameters\nThe performance of ViT can be highly sensitive to choices of hyperparameters—such as patch size, learning rate, and the number\nof transformer layers—requiring extensive tuning. This sensitivity may pose challenges for practitioners attempting to deploy the\nmodel in different settings without exhaustive experimentation.\nReference\n• Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold,\nG., Gelly, S., Uszkoreit, J., & Houlsby, N. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at\nScale. In ICLR.\n3"
  },
  {
    "doc_id": "4c990861",
    "source_path": "data/uploads/Microsoft SQL Server _241106_145359 3.pdf",
    "hash": "b9c1c68e9b107bbf768c65b99dc71139a118581fec064a97d8599b1e86f74244",
    "text": "Microsoft SQL Server\nNotes for Professionals\nMicrosoft\n®\nSQL Server\n®\nNotes for Professionals\nGoalKicker.com\nFree Programming Books\nDisclaimer\nThis is an unocial free book created for educational purposes and is\nnot aliated with ocial Microsoft® SQL Server® group(s) or company(s).\nAll trademarks and registered trademarks are\nthe property of their respective owners\n200+ pages\nof professional hints and tricks\nContents\nAbout 1 ................................................................................................................................................................................... \nChapter 1: Getting started with Microsoft SQL Server 2 .............................................................................. \nSection 1.1: INSERT / SELECT / UPDATE / DELETE: the basics of Data Manipulation Language 2 ......................... \nSection 1.2: SELECT all rows and columns from a table 6 ............................................................................................ \nSection 1.3: UPDATE Speciﬁc Row 6 ................................................................................................................................ \nSection 1.4: DELETE All Rows 7 ........................................................................................................................................ \nSection 1.5: Comments in code 7 .................................................................................................................................... \nSection 1.6: PRINT 8 .......................................................................................................................................................... \nSection 1.7: Select rows that match a condition 8 ......................................................................................................... \nSection 1.8: UPDATE All Rows 8 ....................................................................................................................................... \nSection 1.9: TRUNCATE TABLE 9 ..................................................................................................................................... \nSection 1.10: Retrieve Basic Server Information 9 ......................................................................................................... \nSection 1.11: Create new table and insert records from old table 9 ............................................................................. \nSection 1.12: Using Transactions to change data safely 10 ......................................................................................... \nSection 1.13: Getting Table Row Count 11 ...................................................................................................................... \nChapter 2: Data Types 12 ............................................................................................................................................. \nSection 2.1: Exact Numerics 12 ........................................................................................................................................ \nSection 2.2: Approximate Numerics 13 .......................................................................................................................... \nSection 2.3: Date and Time 13 ........................................................................................................................................ \nSection 2.4: Character Strings 14 .................................................................................................................................... \nSection 2.5: Unicode Character Strings 14 .................................................................................................................... \nSection 2.6: Binary Strings 14 .......................................................................................................................................... \nSection 2.7: Other Data Types 14 ................................................................................................................................... \nChapter 3: Converting data types 15 ..................................................................................................................... \nSection 3.1: TRY PARSE 15 ............................................................................................................................................... \nSection 3.2: TRY CONVERT 15 ......................................................................................................................................... \nSection 3.3: TRY CAST 16 ................................................................................................................................................. \nSection 3.4: Cast 16 .......................................................................................................................................................... \nSection 3.5: Convert 16 .................................................................................................................................................... \nChapter 4: User Deﬁned Table Types 18 ............................................................................................................. \nSection 4.1: creating a UDT with a single int column that is also a primary key 18 .................................................. \nSection 4.2: Creating a UDT with multiple columns 18 ................................................................................................. \nSection 4.3: Creating a UDT with a unique constraint: 18 ............................................................................................ \nSection 4.4: Creating a UDT with a primary key and a column with a default value: 18 ......................................... \nChapter 5: SELECT statement 19 .............................................................................................................................. \nSection 5.1: Basic SELECT from table 19 ........................................................................................................................ \nSection 5.2: Filter rows using WHERE clause 19 ........................................................................................................... \nSection 5.3: Sort results using ORDER BY 19 ................................................................................................................. \nSection 5.4: Group result using GROUP BY 19 ............................................................................................................... \nSection 5.5: Filter groups using HAVING clause 20 ....................................................................................................... \nSection 5.6: Returning only ﬁrst N rows 20 .................................................................................................................... \nSection 5.7: Pagination using OFFSET FETCH 20 .......................................................................................................... \nSection 5.8: SELECT without FROM (no data souce) 20 ............................................................................................... \nChapter 6: Alias Names in SQL Server 21 ............................................................................................................. \nSection 6.1: Giving alias after Derived table name 21 .................................................................................................. \nSection 6.2: Using AS 21 ................................................................................................................................................... \nSection 6.3: Using = 21 ..................................................................................................................................................... \nSection 6.4: Without using AS 21 ..................................................................................................................................... \nChapter 7: NULLs 22 ........................................................................................................................................................ \nSection 7.1: COALESCE () 22 ............................................................................................................................................ \nSection 7.2: ANSI NULLS 22 ............................................................................................................................................. \nSection 7.3: ISNULL() 23 ................................................................................................................................................... \nSection 7.4: Is null / Is not null 23 .................................................................................................................................... \nSection 7.5: NULL comparison 23 ................................................................................................................................... \nSection 7.6: NULL with NOT IN SubQuery 24 ................................................................................................................. \nChapter 8: Variables 26 ................................................................................................................................................. \nSection 8.1: Declare a Table Variable 26 ........................................................................................................................ \nSection 8.2: Updating variables using SELECT 26 ......................................................................................................... \nSection 8.3: Declare multiple variables at once, with initial values 27 ........................................................................ \nSection 8.4: Updating a variable using SET 27 .............................................................................................................. \nSection 8.5: Updating variables by selecting from a table 28 ..................................................................................... \nSection 8.6: Compound assignment operators 28 ........................................................................................................ \nChapter 9: Dates 29 ......................................................................................................................................................... \nSection 9.1: Date & Time Formatting using CONVERT 29 ............................................................................................ \nSection 9.2: Date & Time Formatting using FORMAT 30 ............................................................................................. \nSection 9.3: DATEADD for adding and subtracting time periods 31 ........................................................................... \nSection 9.4: Create function to calculate a person's age on a speciﬁc date 32 ........................................................ \nSection 9.5: Get the current DateTime 32 ...................................................................................................................... \nSection 9.6: Getting the last day of a month 33 ............................................................................................................ \nSection 9.7: CROSS PLATFORM DATE OBJECT 33 ....................................................................................................... \nSection 9.8: Return just Date from a DateTime 33 ....................................................................................................... \nSection 9.9: DATEDIFF for calculating time period dierences 34 ............................................................................. \nSection 9.10: DATEPART & DATENAME 34 ..................................................................................................................... \nSection 9.11: Date parts reference 35 ............................................................................................................................. \nSection 9.12: Date Format Extended 35 ......................................................................................................................... \nChapter 10: Generating a range of dates 39 ...................................................................................................... \nSection 10.1: Generating Date Range With Recursive CTE 39 ...................................................................................... \nSection 10.2: Generating a Date Range With a Tally Table 39 .................................................................................... \nChapter 11: Database Snapshots 40 ........................................................................................................................ \nSection 11.1: Create a database snapshot 40 ................................................................................................................. \nSection 11.2: Restore a database snapshot 40 .............................................................................................................. \nSection 11.3: DELETE Snapshot 40 ................................................................................................................................... \nChapter 12: COALESCE 41 .............................................................................................................................................. \nSection 12.1: Using COALESCE to Build Comma-Delimited String 41 .......................................................................... \nSection 12.2: Getting the ﬁrst not null from a list of column values 41 ....................................................................... \nSection 12.3: Coalesce basic Example 41 ....................................................................................................................... \nChapter 13: IF...ELSE 43 ................................................................................................................................................... \nSection 13.1: Single IF statement 43 ................................................................................................................................ \nSection 13.2: Multiple IF Statements 43 .......................................................................................................................... \nSection 13.3: Single IF..ELSE statement 43 ...................................................................................................................... \nSection 13.4: Multiple IF... ELSE with ﬁnal ELSE Statements 44 ..................................................................................... \nSection 13.5: Multiple IF...ELSE Statements 44 ................................................................................................................ \nChapter 14: CASE Statement 45 ................................................................................................................................ \nSection 14.1: Simple CASE statement 45 ......................................................................................................................... \nSection 14.2: Searched CASE statement 45 ................................................................................................................... \nChapter 15: INSERT INTO 46 ........................................................................................................................................ \nSection 15.1: INSERT multiple rows of data 46 ............................................................................................................... \nSection 15.2: Use OUTPUT to get the new Id 46 ............................................................................................................ \nSection 15.3: INSERT from SELECT Query Results 47 ................................................................................................... \nSection 15.4: INSERT a single row of data 47 ................................................................................................................ \nSection 15.5: INSERT on speciﬁc columns 47 ................................................................................................................. \nSection 15.6: INSERT Hello World INTO table 47 ........................................................................................................... \nChapter 16: MERGE 48 ..................................................................................................................................................... \nSection 16.1: MERGE to Insert / Update / Delete 48 ...................................................................................................... \nSection 16.2: Merge Using CTE Source 49 ...................................................................................................................... \nSection 16.3: Merge Example - Synchronize Source And Target Table 49 ................................................................. \nSection 16.4: MERGE using Derived Source Table 50 .................................................................................................... \nSection 16.5: Merge using EXCEPT 50 ............................................................................................................................. \nChapter 17: CREATE VIEW 52 ....................................................................................................................................... \nSection 17.1: CREATE Indexed VIEW 52 ........................................................................................................................... \nSection 17.2: CREATE VIEW 52 ......................................................................................................................................... \nSection 17.3: CREATE VIEW With Encryption 53 ............................................................................................................ \nSection 17.4: CREATE VIEW With INNER JOIN 53 .......................................................................................................... \nSection 17.5: Grouped VIEWs 53 ...................................................................................................................................... \nSection 17.6: UNION-ed VIEWs 54 ................................................................................................................................... \nChapter 18: Views 55 ........................................................................................................................................................ \nSection 18.1: Create a view with schema binding 55 ..................................................................................................... \nSection 18.2: Create a view 55 ......................................................................................................................................... \nSection 18.3: Create or replace view 55 .......................................................................................................................... \nChapter 19: UNION 56 ...................................................................................................................................................... \nSection 19.1: Union and union all 56 ................................................................................................................................ \nChapter 20: TRY/CATCH 59 ......................................................................................................................................... \nSection 20.1: Transaction in a TRY/CATCH 59 .............................................................................................................. \nSection 20.2: Raising errors in try-catch block 59 ........................................................................................................ \nSection 20.3: Raising info messages in try catch block 60 .......................................................................................... \nSection 20.4: Re-throwing exception generated by RAISERROR 60 ........................................................................... \nSection 20.5: Throwing exception in TRY/CATCH blocks 60 ....................................................................................... \nChapter 21: WHILE loop 62 ............................................................................................................................................ \nSection 21.1: Using While loop 62 .................................................................................................................................... \nSection 21.2: While loop with min aggregate function usage 62 ................................................................................. \nChapter 22: OVER Clause 63 ........................................................................................................................................ \nSection 22.1: Cumulative Sum 63 .................................................................................................................................... \nSection 22.2: Using Aggregation functions with OVER 63 ........................................................................................... \nSection 22.3: Dividing Data into equally-partitioned buckets using NTILE 64 ........................................................... \nSection 22.4: Using Aggregation funtions to ﬁnd the most recent records 64 .......................................................... \nChapter 23: GROUP BY 66 ............................................................................................................................................. \nSection 23.1: Simple Grouping 66 .................................................................................................................................... \nSection 23.2: GROUP BY multiple columns 66 ............................................................................................................... \nSection 23.3: GROUP BY with ROLLUP and CUBE 67 .................................................................................................... \nSection 23.4: Group by with multiple tables, multiple columns 68 .............................................................................. \nSection 23.5: HAVING 69 .................................................................................................................................................. \nChapter 24: ORDER BY 71 ............................................................................................................................................ \nSection 24.1: Simple ORDER BY clause 71 ...................................................................................................................... \nSection 24.2: ORDER BY multiple ﬁelds 71 .................................................................................................................... \nSection 24.3: Custom Ordering 71 .................................................................................................................................. \nSection 24.4: ORDER BY with complex logic 72 ............................................................................................................ \nChapter 25: The STUFF Function 73 ........................................................................................................................ \nSection 25.1: Using FOR XML to Concatenate Values from Multiple Rows 73 ........................................................... \nSection 25.2: Basic Character Replacement with STUFF() 73 ..................................................................................... \nSection 25.3: Basic Example of STUFF() function 74 .................................................................................................... \nSection 25.4: stu for comma separated in sql server 74 ........................................................................................... \nSection 25.5: Obtain column names separated with comma (not a list) 74 .............................................................. \nChapter 26: JSON in SQL Server 76 ......................................................................................................................... \nSection 26.1: Index on JSON properties by using computed columns 76 .................................................................. \nSection 26.2: Join parent and child JSON entities using CROSS APPLY OPENJSON 77 ........................................... \nSection 26.3: Format Query Results as JSON with FOR JSON 78 .............................................................................. \nSection 26.4: Parse JSON text 78 .................................................................................................................................... \nSection 26.5: Format one table row as a single JSON object using FOR JSON 78 .................................................. \nSection 26.6: Parse JSON text using OPENJSON function 79 ..................................................................................... \nChapter 27: OPENJSON 80 ........................................................................................................................................... \nSection 27.1: Transform JSON array into set of rows 80 ............................................................................................. \nSection 27.2: Get key:value pairs from JSON text 80 ................................................................................................... \nSection 27.3: Transform nested JSON ﬁelds into set of rows 80 ................................................................................ \nSection 27.4: Extracting inner JSON sub-objects 81 ..................................................................................................... \nSection 27.5: Working with nested JSON sub-arrays 81 .............................................................................................. \nChapter 28: FOR JSON 83 ............................................................................................................................................. \nSection 28.1: FOR JSON PATH 83 ................................................................................................................................... \nSection 28.2: FOR JSON PATH with column aliases 83 ................................................................................................ \nSection 28.3: FOR JSON clause without array wrapper (single object in output) 83 ............................................... \nSection 28.4: INCLUDE_NULL_VALUES 84 .................................................................................................................... \nSection 28.5: Wrapping results with ROOT object 84 ................................................................................................... \nSection 28.6: FOR JSON AUTO 84 .................................................................................................................................. \nSection 28.7: Creating custom nested JSON structure 85 ........................................................................................... \nChapter 29: Queries with JSON data 86 ................................................................................................................ \nSection 29.1: Using values from JSON in query 86 ....................................................................................................... \nSection 29.2: Using JSON values in reports 86 ............................................................................................................. \nSection 29.3: Filter-out bad JSON text from query results 86 ..................................................................................... \nSection 29.4: Update value in JSON column 86 ............................................................................................................ \nSection 29.5: Append new value into JSON array 87 ................................................................................................... \nSection 29.6: JOIN table with inner JSON collection 87 ............................................................................................... \nSection 29.7: Finding rows that contain value in the JSON array 87 .......................................................................... \nChapter 30: Storing JSON in SQL tables 88 ......................................................................................................... \nSection 30.1: JSON stored as text column 88 ................................................................................................................ \nSection 30.2: Ensure that JSON is properly formatted using ISJSON 88 ................................................................... \nSection 30.3: Expose values from JSON text as computed columns 88 .................................................................... \nSection 30.4: Adding index on JSON path 88 ................................................................................................................ \nSection 30.5: JSON stored in in-memory tables 89 ...................................................................................................... \nChapter 31: Modify JSON text 90 ............................................................................................................................... \nSection 31.1: Modify value in JSON text on the speciﬁed path 90 ................................................................................ \nSection 31.2: Append a scalar value into a JSON array 90 .......................................................................................... \nSection 31.3: Insert new JSON Object in JSON text 90 ................................................................................................. \nSection 31.4: Insert new JSON array generated with FOR JSON query 91 ................................................................ \nSection 31.5: Insert single JSON object generated with FOR JSON clause 91 ........................................................... \nChapter 32: FOR XML PATH 93 ................................................................................................................................... \nSection 32.1: Using FOR XML PATH to concatenate values 93 .................................................................................... \nSection 32.2: Specifying namespaces 93 ....................................................................................................................... \nSection 32.3: Specifying structure using XPath expressions 94 ................................................................................... \nSection 32.4: Hello World XML 95 ................................................................................................................................... \nChapter 33: Join 96 ........................................................................................................................................................... \nSection 33.1: Inner Join 96 ................................................................................................................................................ \nSection 33.2: Outer Join 97 .............................................................................................................................................. \nSection 33.3: Using Join in an Update 99 ....................................................................................................................... \nSection 33.4: Join on a Subquery 99 .............................................................................................................................. \nSection 33.5: Cross Join 100 ............................................................................................................................................ \nSection 33.6: Self Join 101 ............................................................................................................................................... \nSection 33.7: Accidentally turning an outer join into an inner join 101 ....................................................................... \nSection 33.8: Delete using Join 102 ................................................................................................................................ \nChapter 34: cross apply 104 ........................................................................................................................................ \nSection 34.1: Join table rows with dynamically generated rows from a cell 104 ...................................................... \nSection 34.2: Join table rows with JSON array stored in cell 104 ............................................................................... \nSection 34.3: Filter rows by array values 104 ................................................................................................................ \nChapter 35: Computed Columns 106 ....................................................................................................................... \nSection 35.1: A column is computed from an expression 106 ...................................................................................... \nSection 35.2: Simple example we normally use in log tables 106 ............................................................................... \nChapter 36: Common Table Expressions 107 ...................................................................................................... \nSection 36.1: Generate a table of dates using CTE 107 ................................................................................................ \nSection 36.2: Employee Hierarchy 107 ........................................................................................................................... \nSection 36.3: Recursive CTE 108 ..................................................................................................................................... \nSection 36.4: Delete duplicate rows using CTE 109 ...................................................................................................... \nSection 36.5: CTE with multiple AS statements 110 ...................................................................................................... \nSection 36.6: Find nth highest salary using CTE 110 .................................................................................................... \nChapter 37: Move and copy data around tables 111 ..................................................................................... \nSection 37.1: Copy data from one table to another 111 ............................................................................................... \nSection 37.2: Copy data into a table, creating that table on the ﬂy 111 .................................................................... \nSection 37.3: Move data into a table (assuming unique keys method) 111 .............................................................. \nChapter 38: Limit Result Set 113 ............................................................................................................................... \nSection 38.1: Limiting With PERCENT 113 ....................................................................................................................... \nSection 38.2: Limiting with FETCH 113 ........................................................................................................................... \nSection 38.3: Limiting With TOP 113 ............................................................................................................................... \nChapter 39: Retrieve Information about your Instance 114 ....................................................................... \nSection 39.1: General Information about Databases, Tables, Stored procedures and how to search them\n114 ............................................................................................................................................................................. \nSection 39.2: Get information on current sessions and query executions 115 .......................................................... \nSection 39.3: Information about SQL Server version 116 ............................................................................................. \nSection 39.4: Retrieve Edition and Version of Instance 116 ......................................................................................... \nSection 39.5: Retrieve Instance Uptime in Days 116 .................................................................................................... \nSection 39.6: Retrieve Local and Remote Servers 116 ................................................................................................. \nChapter 40: With Ties Option 117 ............................................................................................................................ \nSection 40.1: Test Data 117 ............................................................................................................................................. \nChapter 41: String Functions 119 .............................................................................................................................. \nSection 41.1: Quotename 119 ........................................................................................................................................... \nSection 41.2: Replace 119 ................................................................................................................................................ \nSection 41.3: Substring 120 .............................................................................................................................................. \nSection 41.4: String_Split 120 ........................................................................................................................................... \nSection 41.5: Left 121 ........................................................................................................................................................ \nSection 41.6: Right 121 ..................................................................................................................................................... \nSection 41.7: Soundex 122 ................................................................................................................................................ \nSection 41.8: Format 122 .................................................................................................................................................. \nSection 41.9: String_escape 124 ..................................................................................................................................... \nSection 41.10: ASCII 124 .................................................................................................................................................... \nSection 41.11: Char 125 ...................................................................................................................................................... \nSection 41.12: Concat 125 ................................................................................................................................................. \nSection 41.13: LTrim 125 ................................................................................................................................................... \nSection 41.14: RTrim 126 ................................................................................................................................................... \nSection 41.15: PatIndex 126 .............................................................................................................................................. \nSection 41.16: Space 126 ................................................................................................................................................... \nSection 41.17: Dierence 127 ........................................................................................................................................... \nSection 41.18: Len 127 ....................................................................................................................................................... \nSection 41.19: Lower 128 ................................................................................................................................................... \nSection 41.20: Upper 128 ................................................................................................................................................. \nSection 41.21: Unicode 128 ............................................................................................................................................... \nSection 41.22: NChar 129 ................................................................................................................................................. \nSection 41.23: Str 129 ........................................................................................................................................................ \nSection 41.24: Reverse 129 .............................................................................................................................................. \nSection 41.25: Replicate 129 ............................................................................................................................................ \nSection 41.26: CharIndex 130 ........................................................................................................................................... \nChapter 42: Logical Functions 131 ........................................................................................................................... \nSection 42.1: CHOOSE 131 ............................................................................................................................................... \nSection 42.2: IIF 131 .......................................................................................................................................................... \nChapter 43: Aggregate Functions 132 ................................................................................................................... \nSection 43.1: SUM() 132 .................................................................................................................................................... \nSection 43.2: AVG() 132 ................................................................................................................................................... \nSection 43.3: MAX() 133 ................................................................................................................................................... \nSection 43.4: MIN() 133 .................................................................................................................................................... \nSection 43.5: COUNT() 133 .............................................................................................................................................. \nSection 43.6: COUNT(Column_Name) with GROUP BY Column_Name 134 ............................................................. \nChapter 44: String Aggregate functions in SQL Server 135 ...................................................................... \nSection 44.1: Using STUFF for string aggregation 135 ................................................................................................. \nSection 44.2: String_Agg for String Aggregation 135 .................................................................................................. \nChapter 45: Ranking Functions 136 ......................................................................................................................... \nSection 45.1: DENSE_RANK () 136 .................................................................................................................................. \nSection 45.2: RANK() 136 ................................................................................................................................................. \nChapter 46: Window functions 137 .......................................................................................................................... \nSection 46.1: Centered Moving Average 137 ................................................................................................................. \nSection 46.2: Find the single most recent item in a list of timestamped events 137 ................................................ \nSection 46.3: Moving Average of last 30 Items 137 ...................................................................................................... \nChapter 47: PIVOT / UNPIVOT 138 .......................................................................................................................... \nSection 47.1: Dynamic PIVOT 138 ................................................................................................................................... \nSection 47.2: Simple PIVOT & UNPIVOT (T-SQL) 139 ................................................................................................... \nSection 47.3: Simple Pivot - Static Columns 141 ............................................................................................................ \nChapter 48: Dynamic SQL Pivot 142 ....................................................................................................................... \nSection 48.1: Basic Dynamic SQL Pivot 142 ................................................................................................................... \nChapter 49: Partitioning 143 ....................................................................................................................................... \nSection 49.1: Retrieve Partition Boundary Values 143 .................................................................................................. \nSection 49.2: Switching Partitions 143 ............................................................................................................................ \nSection 49.3: Retrieve partition table,column, scheme, function, total and min-max boundry values using\nsingle query 143 ....................................................................................................................................................... \nChapter 50: Stored Procedures 145 ........................................................................................................................ \nSection 50.1: Creating and executing a basic stored procedure 145 .......................................................................... \nSection 50.2: Stored Procedure with If...Else and Insert Into operation 146 ............................................................... \nSection 50.3: Dynamic SQL in stored procedure 147 ................................................................................................... \nSection 50.4: STORED PROCEDURE with OUT parameters 148 ................................................................................. \nSection 50.5: Simple Looping 149 ................................................................................................................................... \nSection 50.6: Simple Looping 150 ................................................................................................................................... \nChapter 51: Retrieve information about the database 151 ........................................................................ \nSection 51.1: Retrieve a List of all Stored Procedures 151 ............................................................................................ \nSection 51.2: Get the list of all databases on a server 151 ........................................................................................... \nSection 51.3: Count the Number of Tables in a Database 152 .................................................................................... \nSection 51.4: Database Files 152 ..................................................................................................................................... \nSection 51.5: See if Enterprise-speciﬁc features are being used 153 .......................................................................... \nSection 51.6: Determine a Windows Login's Permission Path 153 ............................................................................... \nSection 51.7: Search and Return All Tables and Columns Containing a Speciﬁed Column Value 153 .................... \nSection 51.8: Get all schemas, tables, columns and indexes 154 ................................................................................. \nSection 51.9: Return a list of SQL Agent jobs, with schedule information 155 ........................................................... \nSection 51.10: Retrieve Tables Containing Known Column 157 ................................................................................... \nSection 51.11: Show Size of All Tables in Current Database 158 ................................................................................... \nSection 51.12: Retrieve Database Options 158 ............................................................................................................... \nSection 51.13: Find every mention of a ﬁeld in the database 158 ................................................................................ \nSection 51.14: Retrieve information on backup and restore operations 158 .............................................................. \nChapter 52: Split String function in SQL Server 160 ....................................................................................... \nSection 52.1: Split string in Sql Server 2008/2012/2014 using XML 160 ...................................................................... \nSection 52.2: Split a String in Sql Server 2016 160 ......................................................................................................... \nSection 52.3: T-SQL Table variable and XML 161 ......................................................................................................... \nChapter 53: Insert 162 ..................................................................................................................................................... \nSection 53.1: Add a row to a table named Invoices 162 ............................................................................................... \nChapter 54: Primary Keys 163 ................................................................................................................................... \nSection 54.1: Create table w/ identity column as primary key 163 ............................................................................. \nSection 54.2: Create table w/ GUID primary key 163 .................................................................................................. \nSection 54.3: Create table w/ natural key 163 .............................................................................................................. \nSection 54.4: Create table w/ composite key 163 ........................................................................................................ \nSection 54.5: Add primary key to existing table 163 .................................................................................................... \nSection 54.6: Delete primary key 164 ............................................................................................................................. \nChapter 55: Foreign Keys 165 ..................................................................................................................................... \nSection 55.1: Foreign key relationship/constraint 165 .................................................................................................. \nSection 55.2: Maintaining relationship between parent/child rows 165 ..................................................................... \nSection 55.3: Adding foreign key relationship on existing table 166 .......................................................................... \nSection 55.4: Add foreign key on existing table 166 ..................................................................................................... \nSection 55.5: Getting information about foreign key constraints 166 ........................................................................ \nChapter 56: Last Inserted Identity 167 ................................................................................................................... \nSection 56.1: @@IDENTITY and MAX(ID) 167 ................................................................................................................ \nSection 56.2: SCOPE_IDENTITY() 167 ............................................................................................................................ \nSection 56.3: @@IDENTITY 167 ...................................................................................................................................... \nSection 56.4: IDENT_CURRENT('tablename') 168 ........................................................................................................ \nChapter 57: SCOPE_IDENTITY() 169 ........................................................................................................................ \nSection 57.1: Introduction with Simple Example 169 ..................................................................................................... \nChapter 58: Sequences 170 .......................................................................................................................................... \nSection 58.1: Create Sequence 170 ................................................................................................................................. \nSection 58.2: Use Sequence in Table 170 ...................................................................................................................... \nSection 58.3: Insert Into Table with Sequence 170 ........................................................................................................ \nSection 58.4: Delete From & Insert New 170 ................................................................................................................. \nChapter 59: Index 171 ...................................................................................................................................................... \nSection 59.1: Create Clustered index 171 ....................................................................................................................... \nSection 59.2: Drop index 171 ........................................................................................................................................... \nSection 59.3: Create Non-Clustered index 171 .............................................................................................................. \nSection 59.4: Show index info 171 ................................................................................................................................... \nSection 59.5: Returns size and fragmentation indexes 171 ......................................................................................... \nSection 59.6: Reorganize and rebuild index 172 ........................................................................................................... \nSection 59.7: Rebuild or reorganize all indexes on a table 172 ................................................................................... \nSection 59.8: Rebuild all index database 172 ................................................................................................................ \nSection 59.9: Index on view 172 ...................................................................................................................................... \nSection 59.10: Index investigations 173 .......................................................................................................................... \nChapter 60: Full-Text Indexing 174 ........................................................................................................................... \nSection 60.1: A. Creating a unique index, a full-text catalog, and a full-text index 174 ............................................. \nSection 60.2: Creating a full-text index on several table columns 174 ....................................................................... \nSection 60.3: Creating a full-text index with a search property list without populating it 174 ................................. \nSection 60.4: Full-Text Search 175 .................................................................................................................................. \nChapter 61: Trigger 176 .................................................................................................................................................. \nSection 61.1: DML Triggers 176 ........................................................................................................................................ \nSection 61.2: Types and classiﬁcations of Trigger 177 ................................................................................................. \nChapter 62: Cursors 178 ................................................................................................................................................. \nSection 62.1: Basic Forward Only Cursor 178 ................................................................................................................ \nSection 62.2: Rudimentary cursor syntax 178 ............................................................................................................... \nChapter 63: Transaction isolation levels 180 ...................................................................................................... \nSection 63.1: Read Committed 180 ................................................................................................................................. \nSection 63.2: What are \"dirty reads\"? 180 ..................................................................................................................... \nSection 63.3: Read Uncommitted 181 ............................................................................................................................ \nSection 63.4: Repeatable Read 181 ................................................................................................................................ \nSection 63.5: Snapshot 181 .............................................................................................................................................. \nSection 63.6: Serializable 181 .......................................................................................................................................... \nChapter 64: Advanced options 183 .......................................................................................................................... \nSection 64.1: Enable and show advanced options 183 ................................................................................................. \nSection 64.2: Enable backup compression default 183 ................................................................................................ \nSection 64.3: Enable cmd permission 183 ...................................................................................................................... \nSection 64.4: Set default ﬁll factor percent 183 ............................................................................................................ \nSection 64.5: Set system recovery interval 183 ............................................................................................................ \nSection 64.6: Set max server memory size 183 ............................................................................................................. \nSection 64.7: Set number of checkpoint tasks 183 ....................................................................................................... \nChapter 65: Migration 184 ............................................................................................................................................ \nSection 65.1: How to generate migration scripts 184 ................................................................................................... \nChapter 66: Table Valued Parameters 186 .......................................................................................................... \nSection 66.1: Using a table valued parameter to insert multiple rows to a table 186 ............................................... \nChapter 67: DBMAIL 187 ................................................................................................................................................. \nSection 67.1: Send simple email 187 ............................................................................................................................... \nSection 67.2: Send results of a query 187 ...................................................................................................................... \nSection 67.3: Send HTML email 187 ................................................................................................................................ \nChapter 68: In-Memory OLTP (Hekaton) 188 ...................................................................................................... \nSection 68.1: Declare Memory-Optimized Table Variables 188 ................................................................................... \nSection 68.2: Create Memory Optimized Table 188 ..................................................................................................... \nSection 68.3: Show created .dll ﬁles and tables for Memory Optimized Tables 189 ................................................. \nSection 68.4: Create Memory Optimized System-Versioned Temporal Table 190 ................................................... \nSection 68.5: Memory-Optimized Table Types and Temp tables 190 ........................................................................ \nChapter 69: Temporal Tables 192 ............................................................................................................................ \nSection 69.1: CREATE Temporal Tables 192 .................................................................................................................. \nSection 69.2: FOR SYSTEM_TIME ALL 192 ..................................................................................................................... \nSection 69.3: Creating a Memory-Optimized System-Versioned Temporal Table and cleaning up the SQL\nServer history table 192 ........................................................................................................................................... \nSection 69.4: FOR SYSTEM_TIME BETWEEN <start_date_time> AND <end_date_time> 194 ............................... \nSection 69.5: FOR SYSTEM_TIME FROM <start_date_time> TO <end_date_time> 194 ......................................... \nSection 69.6: FOR SYSTEM_TIME CONTAINED IN (<start_date_time> , <end_date_time>) 194 ........................... \nSection 69.7: How do I query temporal data? 194 ........................................................................................................ \nSection 69.8: Return actual value speciﬁed point in time(FOR SYSTEM_TIME AS OF <date_time>) 195 .............. \nChapter 70: Use of TEMP Table 196 ........................................................................................................................ \nSection 70.1: Dropping temp tables 196 ......................................................................................................................... \nSection 70.2: Local Temp Table 196 .............................................................................................................................. \nSection 70.3: Global Temp Table 196 ............................................................................................................................. \nChapter 71: Scheduled Task or Job 198 ................................................................................................................. \nSection 71.1: Create a scheduled Job 198 ...................................................................................................................... \nChapter 72: Isolation levels and locking 200 ....................................................................................................... \nSection 72.1: Examples of setting the isolation level 200 .............................................................................................. \nChapter 73: Sorting/ordering rows 201 ................................................................................................................. \nSection 73.1: Basics 201 .................................................................................................................................................... \nSection 73.2: Order by Case 203 ..................................................................................................................................... \nChapter 74: Privileges or Permissions 205 ........................................................................................................... \nSection 74.1: Simple rules 205 .......................................................................................................................................... \nChapter 75: SQLCMD 206 ............................................................................................................................................... \nSection 75.1: SQLCMD.exe called from a batch ﬁle or command line 206 ................................................................. \nChapter 76: Resource Governor 207 ....................................................................................................................... \nSection 76.1: Reading the Statistics 207 ......................................................................................................................... \nChapter 77: File Group 208 ........................................................................................................................................... \nSection 77.1: Create ﬁlegroup in database 208 ............................................................................................................. \nChapter 78: Basic DDL Operations in MS SQL Server 210 ............................................................................ \nSection 78.1: Getting started 210 ..................................................................................................................................... \nChapter 79: Subqueries 212 ......................................................................................................................................... \nSection 79.1: Subqueries 212 ............................................................................................................................................ \nChapter 80: Pagination 214 ......................................................................................................................................... \nSection 80.1: Pagination with OFFSET FETCH 214 ........................................................................................................ \nSection 80.2: Paginaton with inner query 214 ............................................................................................................... \nSection 80.3: Paging in Various Versions of SQL Server 214 ....................................................................................... \nSection 80.4: SQL Server 2012/2014 using ORDER BY OFFSET and FETCH NEXT 215 ............................................. \nSection 80.5: Pagination using ROW_NUMBER with a Common Table Expression 215 .......................................... \nChapter 81: CLUSTERED COLUMNSTORE 217 ...................................................................................................... \nSection 81.1: Adding clustered columnstore index on existing table 217 .................................................................... \nSection 81.2: Rebuild CLUSTERED COLUMNSTORE index 217 .................................................................................... \nSection 81.3: Table with CLUSTERED COLUMNSTORE index 217 ................................................................................ \nChapter 82: Parsename 218 ......................................................................................................................................... \nSection 82.1: PARSENAME 218 ......................................................................................................................................... \nChapter 83: Installing SQL Server on Windows 219 ......................................................................................... \nSection 83.1: Introduction 219 .......................................................................................................................................... \nChapter 84: Analyzing a Query 220 ......................................................................................................................... \nSection 84.1: Scan vs Seek 220 ........................................................................................................................................ \nChapter 85: Query Hints 221 ....................................................................................................................................... \nSection 85.1: JOIN Hints 221 ............................................................................................................................................ \nSection 85.2: GROUP BY Hints 221 ................................................................................................................................. \nSection 85.3: FAST rows hint 222 .................................................................................................................................... \nSection 85.4: UNION hints 222 ........................................................................................................................................ \nSection 85.5: MAXDOP Option 222 ................................................................................................................................. \nSection 85.6: INDEX Hints 222 ......................................................................................................................................... \nChapter 86: Query Store 224 ....................................................................................................................................... \nSection 86.1: Enable query store on database 224 ....................................................................................................... \nSection 86.2: Get execution statistics for SQL queries/plans 224 ............................................................................... \nSection 86.3: Remove data from query store 224 ........................................................................................................ \nSection 86.4: Forcing plan for query 224 ....................................................................................................................... \nChapter 87: Querying results by page 226 .......................................................................................................... \nSection 87.1: Row_Number() 226 .................................................................................................................................... \nChapter 88: Schemas 227 ............................................................................................................................................. \nSection 88.1: Purpose 227 ................................................................................................................................................ \nSection 88.2: Creating a Schema 227 ............................................................................................................................ \nSection 88.3: Alter Schema 227 ....................................................................................................................................... \nSection 88.4: Dropping Schemas 227 ............................................................................................................................. \nChapter 89: Backup and Restore Database 228 ............................................................................................... \nSection 89.1: Basic Backup to disk with no options 228 ............................................................................................... \nSection 89.2: Basic Restore from disk with no options 228 ......................................................................................... \nSection 89.3: RESTORE Database with REPLACE 228 .................................................................................................. \nChapter 90: Transaction handling 229 ................................................................................................................... \nSection 90.1: basic transaction skeleton with error handling 229 ............................................................................... \nChapter 91: Natively compiled modules (Hekaton) 230 ................................................................................ \nSection 91.1: Natively compiled stored procedure 230 ................................................................................................. \nSection 91.2: Natively compiled scalar function 230 ..................................................................................................... \nSection 91.3: Native inline table value function 231 ...................................................................................................... \nChapter 92: Spatial Data 233 ...................................................................................................................................... \nSection 92.1: POINT 233 ................................................................................................................................................... \nChapter 93: Dynamic SQL 234 ..................................................................................................................................... \nSection 93.1: Execute SQL statement provided as string 234 ...................................................................................... \nSection 93.2: Dynamic SQL executed as dierent user 234 ........................................................................................ \nSection 93.3: SQL Injection with dynamic SQL 234 ....................................................................................................... \nSection 93.4: Dynamic SQL with parameters 235 ......................................................................................................... \nChapter 94: Dynamic data masking 236 ............................................................................................................... \nSection 94.1: Adding default mask on the column 236 ................................................................................................. \nSection 94.2: Mask email address using Dynamic data masking 236 ........................................................................ \nSection 94.3: Add partial mask on column 236 ............................................................................................................. \nSection 94.4: Showing random value from the range using random() mask 236 .................................................... \nSection 94.5: Controlling who can see unmasked data 237 ........................................................................................ \nChapter 95: Export data in txt ﬁle by using SQLCMD 238 ............................................................................ \nSection 95.1: By using SQLCMD on Command Prompt 238 ......................................................................................... \nChapter 96: Common Language Runtime Integration 239 .......................................................................... \nSection 96.1: Enable CLR on database 239 .................................................................................................................... \nSection 96.2: Adding .dll that contains Sql CLR modules 239 ...................................................................................... \nSection 96.3: Create CLR Function in SQL Server 239 .................................................................................................. \nSection 96.4: Create CLR User-deﬁned type in SQL Server 240 .................................................................................. \nSection 96.5: Create CLR procedure in SQL Server 240 ............................................................................................... \nChapter 97: Delimiting special characters and reserved words 241 ...................................................... \nSection 97.1: Basic Method 241 ....................................................................................................................................... \nChapter 98: DBCC 242 ..................................................................................................................................................... \nSection 98.1: DBCC statement 242 .................................................................................................................................. \nSection 98.2: DBCC maintenance commands 242 ....................................................................................................... \nSection 98.3: DBCC validation statements 243 ............................................................................................................. \nSection 98.4: DBCC informational statements 243 ....................................................................................................... \nSection 98.5: DBCC Trace commands 243 .................................................................................................................... \nChapter 99: BULK Import 245 ...................................................................................................................................... \nSection 99.1: BULK INSERT 245 ....................................................................................................................................... \nSection 99.2: BULK INSERT with options 245 ................................................................................................................ \nSection 99.3: Reading entire content of ﬁle using OPENROWSET(BULK) 245 .......................................................... \nSection 99.4: Read ﬁle using OPENROWSET(BULK) and format ﬁle 245 .................................................................. \nSection 99.5: Read json ﬁle using OPENROWSET(BULK) 246 ..................................................................................... \nChapter 100: Service broker 247 ............................................................................................................................... \nSection 100.1: Basics 247 .................................................................................................................................................. \nSection 100.2: Enable service broker on database 247 ................................................................................................ \nSection 100.3: Create basic service broker construction on database (single database communication)\n247 ............................................................................................................................................................................. \nSection 100.4: How to send basic communication through service broker 248 ........................................................ \nSection 100.5: How to receive conversation from TargetQueue automatically 248 ................................................. \nChapter 101: Permissions and Security 250 .......................................................................................................... \nSection 101.1: Assign Object Permissions to a user 250 ................................................................................................ \nChapter 102: Database permissions 251 ............................................................................................................... \nSection 102.1: Changing permissions 251 ....................................................................................................................... \nSection 102.2: CREATE USER 251 .................................................................................................................................... \nSection 102.3: CREATE ROLE 251 .................................................................................................................................... \nSection 102.4: Changing role membership 251 ............................................................................................................. \nChapter 103: Row-level security 252 ........................................................................................................................ \nSection 103.1: RLS ﬁlter predicate 252 ............................................................................................................................ \nSection 103.2: Altering RLS security policy 252 ............................................................................................................. \nSection 103.3: Preventing updated using RLS block predicate 253 ............................................................................. \nChapter 104: Encryption 254 ....................................................................................................................................... \nSection 104.1: Encryption by certiﬁcate 254 ................................................................................................................... \nSection 104.2: Encryption of database 254 .................................................................................................................... \nSection 104.3: Encryption by symmetric key 254 .......................................................................................................... \nSection 104.4: Encryption by passphrase 255 ............................................................................................................... \nChapter 105: PHANTOM read 256 .............................................................................................................................. \nSection 105.1: Isolation level READ UNCOMMITTED 256 .............................................................................................. \nChapter 106: Filestream 257 ........................................................................................................................................ \nSection 106.1: Example 257 .............................................................................................................................................. \nChapter 107: bcp (bulk copy program) Utility 258 ........................................................................................... \nSection 107.1: Example to Import Data without a Format File(using Native Format ) 258 ....................................... \nChapter 108: SQL Server Evolution through dierent versions (2000 - 2016) 259 .......................... \nSection 108.1: SQL Server Version 2000 - 2016 259 ....................................................................................................... \nChapter 109: SQL Server Management Studio (SSMS) 262 .......................................................................... \nSection 109.1: Refreshing the IntelliSense cache 262 .................................................................................................... \nChapter 110: Managing Azure SQL Database 263 ............................................................................................. \nSection 110.1: Find service tier information for Azure SQL Database 263 ................................................................... \nSection 110.2: Change service tier of Azure SQL Database 263 .................................................................................. \nSection 110.3: Replication of Azure SQL Database 263 ................................................................................................ \nSection 110.4: Create Azure SQL Database in Elastic pool 264 .................................................................................... \nChapter 111: System database - TempDb 265 .................................................................................................... \nSection 111.1: Identify TempDb usage 265 ...................................................................................................................... \nSection 111.2: TempDB database details 265 ................................................................................................................. \nAppendix A: Microsoft SQL Server Management Studio Shortcut Keys 266 ...................................... \nSection A.1: Shortcut Examples 266 ................................................................................................................................ \nSection A.2: Menu Activation Keyboard Shortcuts 266 ................................................................................................ \nSection A.3: Custom keyboard shortcuts 266 ............................................................................................................... \nCredits 269 ............................................................................................................................................................................ \nYou may also like 273 ...................................................................................................................................................... \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 1\nAbout\nPlease feel free to share this PDF with anyone for free,\nlatest version of this book can be downloaded from:\nhttps://goalkicker.com/MicrosoftSQLServerBook\nThis Microsoft® SQL Server® Notes for Professionals book is compiled from Stack\nOverﬂow Documentation, the content is written by the beautiful people at Stack\nOverﬂow. Text content is released under Creative Commons BY-SA, see credits at\nthe end of this book whom contributed to the various chapters. Images may be\ncopyright of their respective owners unless otherwise speciﬁed\nThis is an unoﬃcial free book created for educational purposes and is not\naﬃliated with oﬃcial Microsoft® SQL Server® group(s) or company(s) nor Stack\nOverﬂow. All trademarks and registered trademarks are the property of their\nrespective company owners\nThe information presented in this book is not guaranteed to be correct nor\naccurate, use at your own risk\nPlease send feedback and corrections to web@petercv.com\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 2\nChapter 1: Getting started with Microsoft\nSQL Server\nVersion Release Date\nSQL Server 2017 2017-10-01\nSQL Server 2016 2016-06-01\nSQL Server 2014 2014-03-18\nSQL Server 2012 2011-10-11\nSQL Server 2008 R2 2010-04-01\nSQL Server 2008 2008-08-06\nSQL Server 2005 2005-11-01\nSQL Server 2000 2000-11-01\nSection 1.1: INSERT / SELECT / UPDATE / DELETE: the basics\nof Data Manipulation Language\nData Manipulation Language (DML for short) includes operations such as INSERT, UPDATE and DELETE:\n-- Create a table HelloWorld\nCREATE TABLE HelloWorld (\n    Id INT IDENTITY,\n    Description VARCHAR(1000)\n)\n-- DML Operation INSERT, inserting a row into the table\nINSERT INTO HelloWorld (Description) VALUES ('Hello World')\n-- DML Operation SELECT, displaying the table\nSELECT * FROM HelloWorld  \n-- Select a specific column from table\nSELECT Description FROM HelloWorld\n-- Display number of records in the table\nSELECT Count(*) FROM HelloWorld\n-- DML Operation UPDATE, updating a specific row in the table\nUPDATE HelloWorld SET Description = 'Hello, World!' WHERE Id = 1\n-- Selecting rows from the table (see how the Description has changed after the update?)\nSELECT * FROM HelloWorld\n-- DML Operation - DELETE, deleting a row from the table\nDELETE FROM HelloWorld WHERE Id = 1\n-- Selecting the table. See table content after DELETE operation\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 3\nSELECT * FROM HelloWorld\nIn this script we're creating a table to demonstrate some basic queries.\nThe following examples are showing how to query tables:\nUSE Northwind;\nGO\nSELECT TOP 10 * FROM Customers\nORDER BY CompanyName\nwill select the ﬁrst 10 records of the Customer table, ordered by the column CompanyName from the database\nNorthwind (which is one of Microsoft's sample databases, it can be downloaded from here):\nNote that Use Northwind; changes the default database for all subsequent queries. You can still reference the\ndatabase by using the fully qualiﬁed syntax in the form of [Database].[Schema].[Table]:\nSELECT TOP 10 * FROM Northwind.dbo.Customers\nORDER BY CompanyName\nSELECT TOP 10 * FROM Pubs.dbo.Authors\nORDER BY City\nThis is useful if you're querying data from diﬀerent databases. Note that dbo, speciﬁed \"in between\" is called a\nschema and needs to be speciﬁed while using the fully qualiﬁed syntax. You can think of it as a folder within your\ndatabase. dbo is the default schema. The default schema may be omitted. All other user deﬁned schemas need to\nbe speciﬁed.\nIf the database table contains columns which are named like reserved words, e.g. Date, you need to enclose the\ncolumn name in brackets, like this:\n-- descending order\nSELECT TOP 10 [Date] FROM dbo.MyLogTable\nORDER BY [Date] DESC\nThe same applies if the column name contains spaces in its name (which is not recommended). An alternative\nsyntax is to use double quotes instead of square brackets, e.g.:\n-- descending order\nSELECT top 10 \"Date\" from dbo.MyLogTable\norder by \"Date\" desc\nis equivalent but not so commonly used. Notice the diﬀerence between double quotes and single quotes: Single\nquotes are used for strings, i.e.\n-- descending order\nSELECT top 10 \"Date\" from dbo.MyLogTable\nwhere UserId='johndoe'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 4\norder by \"Date\" desc\nis a valid syntax. Notice that T-SQL has a N preﬁx for NChar and NVarchar data types, e.g.\nSELECT TOP 10 * FROM Northwind.dbo.Customers\nWHERE CompanyName LIKE N'AL%'\nORDER BY CompanyName\nreturns all companies having a company name starting with AL (% is a wild card, use it as you would use the asterisk\nin a DOS command line, e.g. DIR AL*). For LIKE, there are a couple of wildcards available, look here to ﬁnd out\nmore details.\nJoins\nJoins are useful if you want to query ﬁelds which don't exist in one single table, but in multiple tables. For example:\nYou want to query all columns from the Region table in the Northwind database. But you notice that you require\nalso the RegionDescription, which is stored in a diﬀerent table, Region. However, there is a common key, RgionID\nwhich you can use to combine this information in a single query as follows (Top 5 just returns the ﬁrst 5 rows, omit\nit to get all rows):\nSELECT TOP 5 Territories.*,\n    Regions.RegionDescription\nFROM Territories\nINNER JOIN Region\n    ON Territories.RegionID=Region.RegionID\nORDER BY TerritoryDescription\nwill show all columns from Territories plus the RegionDescription column from Region. The result is ordered by\nTerritoryDescription.\nTable Aliases\nWhen your query requires a reference to two or more tables, you may ﬁnd it useful to use a Table Alias. Table\naliases are shorthand references to tables that can be used in place of a full table name, and can reduce typing and\nediting. The syntax for using an alias is:\n<TableName> [as] <alias>\nWhere as is an optional keyword. For example, the previous query can be rewritten as:\nSELECT TOP 5 t.*,\n    r.RegionDescription\nFROM Territories t\nINNER JOIN Region r\n    ON t.RegionID = r.RegionID\nORDER BY TerritoryDescription\nAliases must be unique for all tables in a query, even if you use the same table twice. For example, if your Employee\ntable included a SupervisorId ﬁeld, you can use this query to return an employee and his supervisor's name:\nSELECT e.*,\n    s.Name as SupervisorName -- Rename the field for output\nFROM Employee e\nINNER JOIN Employee s\n    ON e.SupervisorId = s.EmployeeId\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 5\nWHERE e.EmployeeId = 111\nUnions\nAs we have seen before, a Join adds columns from diﬀerent table sources. But what if you want to combine rows\nfrom diﬀerent sources? In this case you can use a UNION. Suppose you're planning a party and want to invite not\nonly employees but also the customers. Then you could run this query to do it:\nSELECT FirstName+' '+LastName as ContactName, Address, City FROM Employees\nUNION\nSELECT ContactName, Address, City FROM Customers\nIt will return names, addresses and cities from the employees and customers in one single table. Note that\nduplicate rows (if there should be any) are automatically eliminated (if you don't want this, use a UNION ALL\ninstead). The column number, column names, order and data type must match across all the select statements that\nare part of the union - this is why the ﬁrst SELECT combines FirstName and LastName from Employee into\nContactName.\nTable Variables\nIt can be useful, if you need to deal with temporary data (especially in a stored procedure), to use table variables:\nThe diﬀerence between a \"real\" table and a table variable is that it just exists in memory for temporary processing.\nExample:\nDECLARE @Region TABLE\n(\n  RegionID int,\n  RegionDescription NChar(50)\n)\ncreates a table in memory. In this case the @ preﬁx is mandatory because it is a variable. You can perform all DML\noperations mentioned above to insert, delete and select rows, e.g.\nINSERT INTO @Region values(3,'Northern')\nINSERT INTO @Region values(4,'Southern')\nBut normally, you would populate it based on a real table like\nINSERT INTO @Region\nSELECT * FROM dbo.Region WHERE RegionID>2;\nwhich would read the ﬁltered values from the real table dbo.Region and insert it into the memory table @Region -\nwhere it can be used for further processing. For example, you could use it in a join like\nSELECT * FROM Territories t\nJOIN @Region r on t.RegionID=r.RegionID\nwhich would in this case return all Northern and Southern territories. More detailed information can be found here.\nTemporary tables are discussed here, if you are interested to read more about that topic.\nNOTE: Microsoft only recommends the use of table variables if the number of rows of data in the table variable are\nless than 100. If you will be working with larger amounts of data, use a temporary table, or temp table, instead.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 6\nSection 1.2: SELECT all rows and columns from a table\nSyntax:\nSELECT *\nFROM table_name\nUsing the asterisk operator * serves as a shortcut for selecting all the columns in the table. All rows will also be\nselected because this SELECT statement does not have a WHERE clause, to specify any ﬁltering criteria.\nThis would also work the same way if you added an alias to the table, for instance e in this case:\nSELECT *\nFROM Employees AS e\nOr if you wanted to select all from a speciﬁc table you can use the alias + \" .* \":\nSELECT e.*, d.DepartmentName\nFROM Employees AS e\n    INNER JOIN Department AS d\n        ON e.DepartmentID = d.DepartmentID\nDatabase objects may also be accessed using fully qualiﬁed names:\nSELECT * FROM [server_name].[database_name].[schema_name].[table_name]\nThis is not necessarily recommended, as changing the server and/or database names would cause the queries\nusing fully-qualiﬁed names to no longer execute due to invalid object names.\nNote that the ﬁelds before table_name can be omitted in many cases if the queries are executed on a single server,\ndatabase and schema, respectively. However, it is common for a database to have multiple schema, and in these\ncases the schema name should not be omitted when possible.\nWarning: Using SELECT * in production code or stored procedures can lead to problems later on (as new columns\nare added to the table, or if columns are rearranged in the table), especially if your code makes simple assumptions\nabout the order of columns, or number of columns returned. So it's safer to always explicitly specify column names\nin SELECT statements for production code.\nSELECT col1, col2, col3\nFROM table_name\nSection 1.3: UPDATE Speciﬁc Row\nUPDATE HelloWorlds\nSET HelloWorld = 'HELLO WORLD!!!'\nWHERE Id = 5\nThe above code updates the value of the ﬁeld \"HelloWorld\" with \"HELLO WORLD!!!\" for the record where \"Id = 5\" in\nHelloWorlds table.\nNote: In an update statement, It is advised to use a \"where\" clause to avoid updating the whole table unless and\nuntil your requirement is diﬀerent.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 7\nSection 1.4: DELETE All Rows\nDELETE\nFROM Helloworlds\nThis will delete all the data from the table. The table will contain no rows after you run this code. Unlike DROP TABLE,\nthis preserves the table itself and its structure and you can continue to insert new rows into that table.\nAnother way to delete all rows in table is truncate it, as follow:\nTRUNCATE TABLE HelloWords\nDiﬀerence with DELETE operation are several:\nTruncate operation doesn't store in transaction log ﬁle1.\nIf exists IDENTITY ﬁeld, this will be reset2.\nTRUNCATE can be applied on whole table and no on part of it (instead with DELETE command you can3.\nassociate a WHERE clause)\nRestrictions Of TRUNCATE\nCannot TRUNCATE a table if there is a FOREIGN KEY reference1.\nIf the table is participated in an INDEXED VIEW2.\nIf the table is published by using TRANSACTIONAL REPLICATION or MERGE REPLICATION3.\nIt will not ﬁre any TRIGGER deﬁned in the table4.\n[sic]\nSection 1.5: Comments in code\nTransact-SQL supports two forms of comment writing. Comments are ignored by the database engine, and are\nmeant for people to read.\nComments are preceded by -- and are ignored until a new line is encountered:\n-- This is a comment\nSELECT *\nFROM MyTable -- This is another comment\nWHERE Id = 1;\nSlash star comments begin with /* and end with */. All text between those delimiters is considered as a comment\nblock.\n/* This is\na multi-line\ncomment block. */\nSELECT Id = 1, [Message] = 'First row'\nUNION ALL\nSELECT 2, 'Second row'\n/* This is a one liner */\nSELECT 'More';\nSlash star comments have the advantage of keeping the comment usable if the SQL Statement loses new line\ncharacters. This can happen when SQL is captured during troubleshooting.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 8\nSlash star comments can be nested and a starting /* inside a slash star comment needs to be ended with a */ to be\nvalid. The following code will result in an error\n/*\nSELECT *\nFROM CommentTable\nWHERE Comment = '/*'\n*/\nThe slash star even though inside the quote is considered as the start of a comment. Hence it needs to be ended\nwith another closing star slash. The correct way would be\n/*\nSELECT *\nFROM CommentTable\nWHERE Comment = '/*'\n*/  */\nSection 1.6: PRINT\nDisplay a message to the output console. Using SQL Server Management Studio, this will be displayed in the\nmessages tab, rather than the results tab:\nPRINT 'Hello World!';\nSection 1.7: Select rows that match a condition\nGenerally, the syntax is:\nSELECT <column names>\nFROM <table name>\nWHERE <condition>\nFor example:\nSELECT FirstName, Age\nFROM Users\nWHERE LastName = 'Smith'\nConditions can be complex:\nSELECT FirstName, Age\nFROM Users\nWHERE LastName = 'Smith' AND (City = 'New York' OR City = 'Los Angeles')\nSection 1.8: UPDATE All Rows\nA simple form of updating is incrementing all the values in a given ﬁeld of the table. In order to do so, we need to\ndeﬁne the ﬁeld and the increment value\nThe following is an example that increments the Score ﬁeld by 1 (in all rows):\nUPDATE Scores\nSET score = score + 1  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 9\nThis can be dangerous since you can corrupt your data if you accidentally make an UPDATE for a speciﬁc Row with\nan UPDATE for All rows in the table.\nSection 1.9: TRUNCATE TABLE\nTRUNCATE TABLE Helloworlds\nThis code will delete all the data from the table Helloworlds. Truncate table is almost similar to Delete from Table\ncode. The diﬀerence is that you can not use where clauses with Truncate. Truncate table is considered better than\ndelete because it uses less transaction log spaces.\nNote that if an identity column exists, it is reset to the initial seed value (for example, auto-incremented ID will\nrestart from 1). This can lead to inconsistency if the identity columns is used as a foreign key in another table.\nSection 1.10: Retrieve Basic Server Information\nSELECT @@VERSION\nReturns the version of MS SQL Server running on the instance.\nSELECT @@SERVERNAME\nReturns the name of the MS SQL Server instance.\nSELECT @@SERVICENAME\nReturns the name of the Windows service MS SQL Server is running as.\nSELECT serverproperty('ComputerNamePhysicalNetBIOS');\nReturns the physical name of the machine where SQL Server is running. Useful to identify the node in a failover\ncluster.\nSELECT * FROM fn_virtualservernodes();\nIn a failover cluster returns every node where SQL Server can run on. It returns nothing if not a cluster.\nSection 1.11: Create new table and insert records from old\ntable\nSELECT * INTO NewTable FROM OldTable\nCreates a new table with structure of old table and inserts all rows into the new table.\nSome Restrictions\nYou cannot specify a table variable or table-valued parameter as the new table.1.\nYou cannot use SELECT…INTO to create a partitioned table, even when the source table is2.\npartitioned. SELECT...INTO does not use the partition scheme of the source table; instead, the new\ntable is created in the default ﬁlegroup. To insert rows into a partitioned table, you must ﬁrst\ncreate the partitioned table and then use the INSERT INTO...SELECT FROM statement.\nIndexes, constraints, and triggers deﬁned in the source table are not transferred to the new table,3.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 10\nnor can they be speciﬁed in the SELECT...INTO statement. If these objects are required, you can\ncreate them after executing the SELECT...INTO statement.\nSpecifying an ORDER BY clause does not guarantee the rows are inserted in the speciﬁed order.4.\nWhen a sparse column is included in the select list, the sparse column property does not transfer\nto the column in the new table. If this property is required in the new table, alter the column\ndeﬁnition after executing the SELECT...INTO statement to include this property.\nWhen a computed column is included in the select list, the corresponding column in the new table5.\nis not a computed column. The values in the new column are the values that were computed at the\ntime SELECT...INTO was executed.\n[sic]\nSection 1.12: Using Transactions to change data safely\nWhenever you change data, in a Data Manipulation Language(DML) command, you can wrap your changes in a\ntransaction. DML includes UPDATE, TRUNCATE, INSERT and DELETE. One of the ways that you can make sure that\nyou're changing the right data would be to use a transaction.\nDML changes will take a lock on the rows aﬀected. When you begin a transaction, you must end the transaction or\nall objects being changed in the DML will remain locked by whoever began the transaction. You can end your\ntransaction with either ROLLBACK or COMMIT. ROLLBACK returns everything within the transaction to its original state.\nCOMMIT places the data into a ﬁnal state where you cannot undo your changes without another DML statement.\nExample:\n--Create a test table\nUSE [your database]\nGO\nCREATE TABLE test_transaction (column_1 varchar(10))\nGO\nINSERT INTO\n dbo.test_transaction\n        ( column_1 )\nVALUES\n        ( 'a' )\nBEGIN TRANSACTION --This is the beginning of your transaction\nUPDATE dbo.test_transaction\nSET column_1 = 'B'\nOUTPUT INSERTED.*\nWHERE column_1 = 'A'\n \nROLLBACK TRANSACTION  --Rollback will undo your changes\n           --Alternatively, use COMMIT to save your results\nSELECT * FROM dbo.test_transaction   --View the table after your changes have been run\nDROP TABLE dbo.test_transaction\nNotes:\nThis is a simpliﬁed example which does not include error handling. But any database operation can fail and\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 11\nhence throw an exception. Here is an example how such a required error handling might look like. You\nshould never use transactions without an error handler, otherwise you might leave the transaction in an\nunknown state.\nDepending on the isolation level, transactions are putting locks on the data being queried or changed. You\nneed to ensure that transactions are not running for a long time, because they will lock records in a database\nand can lead to deadlocks with other parallel running transactions. Keep the operations encapsulated in\ntransactions as short as possible and minimize the impact with the amount of data you're locking.\nSection 1.13: Getting Table Row Count\nThe following example can be used to ﬁnd the total row count for a speciﬁc table in a database if table_name is\nreplaced by the the table you wish to query:\nSELECT COUNT(*) AS [TotalRowCount] FROM table_name;\nIt is also possible to get the row count for all tables by joining back to the table's partition based oﬀ the tables' HEAP\n(index_id = 0) or cluster clustered index (index_id = 1) using the following script:\nSELECT  [Tables].name                AS [TableName],\n        SUM( [Partitions].[rows] )    AS [TotalRowCount]\nFROM    sys.tables AS [Tables]\nJOIN    sys.partitions AS [Partitions]\n    ON  [Tables].[object_id]    =    [Partitions].[object_id]\n    AND [Partitions].index_id IN ( 0, 1 )\n--WHERE    [Tables].name = N'table name' /* uncomment to look for a specific table */\nGROUP BY    [Tables].name;\nThis is possible as every table is essentially a single partition table, unless extra partitions are added to it. This script\nalso has the beneﬁt of not interfering with read/write operations to the tables rows'.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 12\nChapter 2: Data Types\nThis section discusses the data types that SQL Server can use, including their data range, length, and limitations (if\nany.)\nSection 2.1: Exact Numerics\nThere are two basic classes of exact numeric data types - Integer, and Fixed Precision and Scale.\nInteger Data Types\nbit\ntinyint\nsmallint\nint\nbigint\nIntegers are numeric values that never contain a fractional portion, and always use a ﬁxed amount of storage. The\nrange and storage sizes of the integer data types are shown in this table:\nData type Range Storage\nbit 0 or 1 1 bit **\ntinyint 0 to 255 1 byte\nsmallint -2^15 (-32,768) to 2^15-1 (32,767) 2 bytes\nint -2^31 (-2,147,483,648) to 2^31-1 (2,147,483,647) 4 bytes\nbigint -2^63 (-9,223,372,036,854,775,808) to 2^63-1 (9,223,372,036,854,775,807) 8 bytes\nFixed Precision and Scale Data Types\nnumeric\ndecimal\nsmallmoney\nmoney\nThese data types are useful for representing numbers exactly. As long as the values can ﬁt within the range of the\nvalues storable in the data type, the value will not have rounding issues. This is useful for any ﬁnancial calculations,\nwhere rounding errors will result in clinical insanity for accountants.\nNote that decimal and numeric are synonyms for the same data type.\nData type Range Storage\nDecimal [(p [, s])] or Numeric [(p [, s])] -10^38 + 1 to 10^38 - 1 See Precision table\nWhen deﬁning a decimal or numeric data type, you may need to specify the Precision [p] and Scale [s].\nPrecision is the number of digits that can be stored. For example, if you needed to store values between 1 and 999,\nyou would need a Precision of 3 (to hold the three digits in 100). If you do not specify a precision, the default\nprecision is 18.\nScale is the number of digits after the decimal point. If you needed to store a number between 0.00 and 999.99, you\nwould need to specify a Precision of 5 (ﬁve digits) and a Scale of 2 (two digits after the decimal point). You must\nspecify a precision to specify a scale. The default scale is zero.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 13\nThe Precision of a decimal or numeric data type deﬁnes the number of bytes required to store the value, as shown\nbelow:\nPrecision Table\nPrecision Storage bytes\n1 - 9 5\n10-19 9\n20-28 13\n29-38 17\nMonetary Fixed Data Types\nThese data types are intended speciﬁcally for accounting and other monetary data. These type have a ﬁxed Scale of\n4 - you will always see four digits after the decimal place. For most systems working with most currencies, using a\nnumeric value with a Scale of 2 will be suﬃcient. Note that no information about the type of currency represented is\nstored with the value.\nData type Range Storage\nmoney -922,337,203,685,477.5808 to 922,337,203,685,477.5807 8 bytes\nsmallmoney -214,748.3648 to 214,748.3647 4 bytes\nSection 2.2: Approximate Numerics\nﬂoat [(n)]\nreal\nThese data types are used to store ﬂoating point numbers. Since these types are intended to hold approximate\nnumeric values only, these should not be used in cases where any rounding error is unacceptable. However, if you\nneed to handle very large numbers, or numbers with an indeterminate number of digits after the decimal place,\nthese may be your best option.\nData type Range Size\nﬂoat -1.79E+308 to -2.23E-308, 0 and 2.23E-308 to 1.79E+308 depends on n in table below\nreal -3.40E + 38 to -1.18E - 38, 0 and 1.18E - 38 to 3.40E + 38 4 Bytes\nn value table for ﬂoat numbers. If no value is speciﬁed in the declaration of the ﬂoat, the default value of 53 will be\nused. Note that ﬂoat(24) is the equivalent of a real value.\nn value Precision Size\n1-24 7 digits 4 bytes\n25-53 15 digits 8 bytes\nSection 2.3: Date and Time\nThese types are in all versions of SQL Server\ndatetime\nsmalldatetime\nThese types are in all versions of SQL Server after SQL Server 2012\ndate\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 14\ndatetimeoﬀset\ndatetime2\ntime\nSection 2.4: Character Strings\nchar\nvarchar\ntext\nSection 2.5: Unicode Character Strings\nnchar\nnvarchar\nntext\nSection 2.6: Binary Strings\nbinary\nvarbinary\nimage\nSection 2.7: Other Data Types\ncursor\ntimestamp\nhierarchyid\nuniqueidentiﬁer\nsql_variant\nxml\ntable\nSpatial Types\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 15\nChapter 3: Converting data types\nSection 3.1: TRY PARSE\nVersion ≥ SQL Server 2012\nIt converts string data type to target data type(Date or Numeric).\nFor example, source data is string type and we need to covert to date type. If conversion attempt fails it returns\nNULL value.\nSyntax: TRY_PARSE (string_value AS data_type [ USING culture ])\nString_value – This is argument is source value which is NVARCHAR(4000) type.\nData_type – This argument is target data type either date or numeric.\nCulture – It is an optional argument which helps to convert the value to in Culture format. Suppose you want to\ndisplay the date in French, then you need to pass culture type as ‘Fr-FR’. If you will not pass any valid culture name,\nthen PARSE will raise an error.\nDECLARE @fakeDate AS varchar(10);  \nDECLARE @realDate AS VARCHAR(10);  \nSET @fakeDate = 'iamnotadate';\nSET @realDate = '13/09/2015';  \nSELECT TRY_PARSE(@fakeDate AS DATE); --NULL  as the parsing fails\nSELECT TRY_PARSE(@realDate AS DATE); -- NULL due to type mismatch\nSELECT TRY_PARSE(@realDate AS DATE USING 'Fr-FR'); -- 2015-09-13\nSection 3.2: TRY CONVERT\nVersion ≥ SQL Server 2012\nIt converts value to speciﬁed data type and if conversion fails it returns NULL. For example, source value in string\nformat and we need date/integer format. Then this will help us to achieve the same.\nSyntax: TRY_CONVERT ( data_type [ ( length ) ], expression [, style ] )\nTRY_CONVERT() returns a value cast to the speciﬁed data type if the cast succeeds; otherwise, returns null.\nData_type - The datatype into which to convert. Here length is an optional parameter which helps to get result in\nspeciﬁed length.\nExpression - The value to be convert\nStyle - It is an optional parameter which determines formatting. Suppose you want date format like “May, 18 2013”\nthen you need pass style as 111.\nDECLARE @sampletext AS VARCHAR(10);  \nSET @sampletext = '123456';  \nDECLARE @ realDate AS VARCHAR(10);  \nSET @realDate = '13/09/2015’;  \nSELECT TRY_CONVERT(INT, @sampletext); -- 123456  \nSELECT TRY_CONVERT(DATETIME, @sampletext); -- NULL  \nSELECT TRY_CONVERT(DATETIME, @realDate, 111); -- Sep, 13 2015  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 16\nSection 3.3: TRY CAST\nVersion ≥ SQL Server 2012\nIt converts value to speciﬁed data type and if conversion fails it returns NULL. For example, source value in string\nformat and we need it in double/integer format. Then this will help us in achieving it.\nSyntax: TRY_CAST ( expression AS data_type [ ( length ) ] )\nTRY_CAST() returns a value cast to the speciﬁed data type if the cast succeeds; otherwise, returns null.\nExpression - The source value which will go to cast.\nData_type - The target data type the source value will cast.\nLength - It is an optional parameter that speciﬁes the length of target data type.\nDECLARE @sampletext AS VARCHAR(10);  \nSET @sampletext = '123456';  \n \nSELECT TRY_CAST(@sampletext AS INT); -- 123456  \nSELECT TRY_CAST(@sampletext AS DATE); -- NULL  \nSection 3.4: Cast\nThe Cast() function is used to convert a data type variable or data from one data type to another data type.\nSyntax\nCAST ( [Expression] AS Datatype)\nThe data type to which you are casting an expression is the target type. The data type of the expression from which\nyou are casting is the source type.\nDECLARE @A varchar(2)    \nDECLARE @B varchar(2)\nset @A='25a'    \nset @B='15'\nSelect CAST(@A as int) + CAST(@B as int)  as Result\n--'25a' is casted to 25 (string to int)\n--'15' is casted to 15 (string to int)\n--Result\n --40\nDECLARE @C varchar(2)  = 'a'    \nselect CAST(@C as int) as Result    \n--Result\n --Conversion failed when converting the varchar value 'a' to data type int.\nThrows error if failed\nSection 3.5: Convert\nWhen you convert expressions from one type to another, in many cases there will be a need within a stored\nprocedure or other routine to convert data from a datetime type to a varchar type. The Convert function is used for\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 17\nsuch things. The CONVERT() function can be used to display date/time data in various formats. Syntax\nCONVERT(data_type(length), expression, style)\nStyle - style values for datetime or smalldatetime conversion to character data. Add 100 to a style value to get a\nfour-place year that includes the century (yyyy).\nselect convert(varchar(20),GETDATE(),108)\n13:27:16\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 18\nChapter 4: User Deﬁned Table Types\nUser deﬁned table types (UDT for short) are data types that allows the user to deﬁne a table structure. User deﬁned\ntable types supports primary keys, unique constraints and default values.\nSection 4.1: creating a UDT with a single int column that is\nalso a primary key\nCREATE TYPE dbo.Ids as TABLE\n(\n    Id int PRIMARY KEY\n)\nSection 4.2: Creating a UDT with multiple columns\nCREATE TYPE MyComplexType as TABLE\n(\n    Id int,\n    Name varchar(10)\n)\nSection 4.3: Creating a UDT with a unique constraint:\nCREATE TYPE MyUniqueNamesType as TABLE\n(\n    FirstName varchar(10),\n    LastName varchar(10),\n    UNIQUE (FirstName,LastName)\n)\nNote: constraints in user deﬁned table types can not be named.\nSection 4.4: Creating a UDT with a primary key and a column\nwith a default value:\nCREATE TYPE MyUniqueNamesType as TABLE\n(\n    FirstName varchar(10),\n    LastName varchar(10),\n    CreateDate datetime default GETDATE()\n    PRIMARY KEY (FirstName,LastName)\n)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 19\nChapter 5: SELECT statement\nIn SQL, SELECT statements return sets of results from data collections like tables or views. SELECT statements can\nbe used with various other clauses like WHERE, GROUP BY, or ORDER BY to further reﬁne the desired results.\nSection 5.1: Basic SELECT from table\nSelect all columns from some table (system table in this case):\nSELECT *\nFROM sys.objects\nOr, select just some speciﬁc columns:\nSELECT object_id, name, type, create_date\nFROM sys.objects\nSection 5.2: Filter rows using WHERE clause\nWHERE clause ﬁlters only those rows that satisfy some condition:\nSELECT *\nFROM sys.objects\nWHERE type = 'IT'\nSection 5.3: Sort results using ORDER BY\nORDER BY clause sorts rows in the returned result set by some column or expression:\nSELECT *\nFROM sys.objects\nORDER BY create_date\nSection 5.4: Group result using GROUP BY\nGROUP BY clause groups rows by some value:\nSELECT type, count(*) as c\nFROM sys.objects\nGROUP BY type\nYou can apply some function on each group (aggregate function) to calculate sum or count of the records in the\ngroup.\ntype c\nSQ 3\nS 72\nIT 16\nPK 1\nU 5\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 20\nSection 5.5: Filter groups using HAVING clause\nHAVING clause removes groups that do not satisfy condition:\nSELECT type, count(*) as c\nFROM sys.objects\nGROUP BY type\nHAVING count(*) < 10\ntype c\nSQ 3\nPK 1\nU 5\nSection 5.6: Returning only ﬁrst N rows\nTOP clause returns only ﬁrst N rows in the result:\nSELECT TOP 10 *\nFROM sys.objects\nSection 5.7: Pagination using OFFSET FETCH\nOFFSET FETCH clause is more advanced version of TOP. It enables you to skip N1 rows and take next N2 rows:\nSELECT *\nFROM sys.objects\nORDER BY object_id\nOFFSET 50 ROWS FETCH NEXT 10 ROWS ONLY\nYou can use OFFSET without fetch to just skip ﬁrst 50 rows:\nSELECT *\nFROM sys.objects\nORDER BY object_id\nOFFSET 50 ROWS\nSection 5.8: SELECT without FROM (no data souce)\nSELECT statement can be executed without FROM clause:\ndeclare @var int = 17;\nSELECT @var as c1, @var + 2 as c2, 'third' as c3\nIn this case, one row with values/results of expressions are returned.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 21\nChapter 6: Alias Names in SQL Server\nHere is some of diﬀerent ways to provide alias names to columns in Sql Server\nSection 6.1: Giving alias after Derived table name\nThis is a weird approach most of the people don't know this even exist.\nCREATE TABLE AliasNameDemo(id INT,firstname VARCHAR(20),lastname VARCHAR(20))\nINSERT INTO AliasNameDemo\nVALUES      (1,'MyFirstName','MyLastName')\nSELECT *\nFROM   (SELECT firstname + ' ' + lastname\n        FROM   AliasNameDemo) a (fullname)\nDemo\nSection 6.2: Using AS\nThis is ANSI SQL method works in all the RDBMS. Widely used approach.\nCREATE TABLE AliasNameDemo (id INT,firstname VARCHAR(20),lastname VARCHAR(20))\nINSERT INTO AliasNameDemo\nVALUES      (1,'MyFirstName','MyLastName')\nSELECT FirstName +' '+ LastName As FullName\nFROM   AliasNameDemo\nSection 6.3: Using =\nThis is my preferred approach. Nothing related to performance just a personal choice. It makes the code to look\nclean. You can see the resulting column names easily instead of scrolling the code if you have a big expression.\nCREATE TABLE AliasNameDemo (id INT,firstname VARCHAR(20),lastname VARCHAR(20))\nINSERT INTO AliasNameDemo\nVALUES      (1,'MyFirstName','MyLastName')\nSELECT FullName = FirstName +' '+ LastName\nFROM   AliasNameDemo\nSection 6.4: Without using AS\nThis syntax will be similar to using AS keyword. Just we don't have to use AS keyword\nCREATE TABLE AliasNameDemo (id INT,firstname VARCHAR(20),lastname VARCHAR(20))\nINSERT INTO AliasNameDemo\nVALUES      (1,'MyFirstName','MyLastName')\nSELECT FirstName +' '+ LastName FullName\nFROM   AliasNameDemo\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 22\nChapter 7: NULLs\nIn SQL Server, NULL represents data that is missing, or unknown. This means that NULL is not really a value; it's\nbetter described as a placeholder for a value. This is also the reason why you can't compare NULL with any value,\nand not even with another NULL.\nSection 7.1: COALESCE ()\nCOALESCE () Evaluates the arguments in order and returns the current value of the ﬁrst expression that initially\ndoes not evaluate to NULL.\nDECLARE @MyInt int -- variable is null until it is set with value.\nDECLARE @MyInt2 int -- variable is null until it is set with value.\nDECLARE @MyInt3 int -- variable is null until it is set with value.\nSET @MyInt3  = 3\nSELECT COALESCE (@MyInt, @MyInt2 ,@MyInt3 ,5) -- Returns 3 : value of @MyInt3.\nAlthough ISNULL() operates similarly to COALESCE(), the ISNULL() function only accepts two parameters - one to\ncheck, and one to use if the ﬁrst parameter is NULL. See also ISNULL, below\nSection 7.2: ANSI NULLS\nFrom MSDN\nIn a future version of SQL Server, ANSI_NULLS will always be ON and any applications that explicitly set\nthe option to OFF will generate an error. Avoid using this feature in new development work, and plan to\nmodify applications that currently use this feature.\nANSI NULLS being set to oﬀ allows for a =/<> comparison of null values.\nGiven the following data:\nid someVal\n----\n0 NULL\n1 1\n2 2\nAnd with ANSI NULLS on, this query:\n SELECT id\n FROM table\n WHERE someVal = NULL\nwould produce no results. However the same query, with ANSI NULLS oﬀ:\n set ansi_nulls off\n SELECT id\n FROM table\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 23\n WHERE someVal = NULL\nWould return id 0.\nSection 7.3: ISNULL()\nThe IsNull() function accepts two parameters, and returns the second parameter if the ﬁrst one is null.\nParameters:\ncheck expression. Any expression of any data type.1.\nreplacement value. This is the value that would be returned if the check expression is null. The replacement2.\nvalue must be of a data type that can be implicitly converted to the data type of the check expression.\nThe IsNull() function returns the same data type as the check expression.\nDECLARE @MyInt int -- All variables are null until they are set with values.\nSELECT ISNULL(@MyInt, 3) -- Returns 3.\nSee also COALESCE, above\nSection 7.4: Is null / Is not null\nSince null is not a value, you can't use comparison operators with nulls.\nTo check if a column or variable holds null, you need to use is null:\nDECLARE @Date date = '2016-08-03'\nThe following statement will select the value 6, since all comparisons with null values evaluates to false or unknown:\nSELECT CASE WHEN @Date = NULL THEN 1\n            WHEN @Date <> NULL THEN 2\n            WHEN @Date > NULL THEN 3\n            WHEN @Date < NULL THEN 4\n            WHEN @Date IS NULL THEN 5\n            WHEN @Date IS NOT NULL THEN 6\nSetting the content of the @Date variable to null and try again, the following statement will return 5:\nSET @Date = NULL -- Note that the '=' here is an assignment operator!\nSELECT CASE WHEN @Date = NULL THEN 1\n            WHEN @Date <> NULL THEN 2\n            WHEN @Date > NULL THEN 3\n            WHEN @Date < NULL THEN 4\n            WHEN @Date IS NULL THEN 5\n            WHEN @Date IS NOT NULL THEN 6\nSection 7.5: NULL comparison\nNULL is a special case when it comes to comparisons.\nAssume the following data.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 24\nid someVal\n----\n0 NULL\n1 1\n2 2\nWith a query:\n SELECT id\n FROM table\n WHERE someVal = 1\nwould return id 1\n SELECT id\n FROM table\n WHERE someVal <> 1\nwould return id 2\n SELECT id\n FROM table\n WHERE someVal IS NULL\nwould return id 0\n SELECT id\n FROM table\n WHERE someVal IS NOT NULL\nwould return both ids 1 and 2.\nIf you wanted NULLs to be \"counted\" as values in a =, <> comparison, it must ﬁrst be converted to a countable data\ntype:\n SELECT id\n FROM table\n WHERE ISNULL(someVal, -1) <> 1\nOR\n SELECT id\n FROM table\n WHERE someVal IS NULL OR someVal <> 1\nreturns 0 and 2\nOr you can change your ANSI Null setting.\nSection 7.6: NULL with NOT IN SubQuery\nWhile handling not in sub-query with null in the sub-query we need to eliminate NULLS to get your expected results\ncreate table #outertable (i int)\ncreate table #innertable (i int)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 25\ninsert into #outertable (i) values (1), (2),(3),(4), (5)\ninsert into #innertable (i) values (2), (3), (null)\nselect * from #outertable where i in (select i from #innertable)\n--2\n--3\n--So far so good\nselect * from #outertable where i not in (select i from #innertable)\n--Expectation here is to get 1,4,5 but it is not. It will get empty results because of the NULL it\nexecutes as {select * from #outertable where i not in (null)}\n--To fix this\nselect * from #outertable where i not in (select i from #innertable where i is not null)\n--you will get expected results\n--1\n--4\n--5\nWhile handling not in sub-query with null be cautious with your expected output\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 26\nChapter 8: Variables\nSection 8.1: Declare a Table Variable\nDECLARE @Employees TABLE\n(\n    EmployeeID INT NOT NULL PRIMARY KEY,\n    FirstName NVARCHAR(50) NOT NULL,\n    LastName NVARCHAR(50) NOT NULL,\n    ManagerID INT NULL\n)\nWhen you create a normal table, you use CREATE TABLE Name (Columns) syntax. When creating a table variable,\nyou use DECLARE @Name TABLE (Columns) syntax.\nTo reference the table variable inside a SELECT statement, SQL Server requires that you give the table variable an\nalias, otherwise you'll get an error:\nMust declare the scalar variable \"@TableVariableName\".\ni.e.\nDECLARE @Table1 TABLE (Example INT)\nDECLARE @Table2 TABLE (Example INT)\n/*\n-- the following two commented out statements would generate an error:\nSELECT *\nFROM @Table1\nINNER JOIN @Table2 ON @Table1.Example = @Table2.Example\nSELECT *\nFROM @Table1\nWHERE @Table1.Example = 1\n*/\n-- but these work fine:\nSELECT *\nFROM @Table1 T1\nINNER JOIN @Table2 T2 ON T1.Example = T2.Example\nSELECT *\nFROM @Table1 Table1\nWHERE Table1.Example = 1\nSection 8.2: Updating variables using SELECT\nUsing SELECT, you can update multiple variables at once.\nDECLARE @Variable1 INT, @Variable2 VARCHAR(10)\nSELECT @Variable1 = 1, @Variable2 = 'Hello'\nPRINT @Variable1\nPRINT @Variable2\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 27\n1\nHello\nWhen using SELECT to update a variable from a table column, if there are multiple values, it will use the last value.\n(Normal order rules apply - if no sort is given, the order is not guaranteed.)\nCREATE TABLE #Test (Example INT)\nINSERT INTO #Test VALUES (1), (2)\nDECLARE @Variable INT\nSELECT @Variable = Example\nFROM #Test\nORDER BY Example ASC\nPRINT @Variable\n2\nSELECT TOP 1 @Variable = Example\nFROM #Test\nORDER BY Example ASC\nPRINT @Variable\n1\nIf there are no rows returned by the query, the variable's value won't change:\nSELECT TOP 0 @Variable = Example\nFROM #Test\nORDER BY Example ASC\nPRINT @Variable\n1\nSection 8.3: Declare multiple variables at once, with initial\nvalues\nDECLARE\n  @Var1 INT = 5,\n  @Var2 NVARCHAR(50) = N'Hello World',\n  @Var3 DATETIME = GETDATE()\nSection 8.4: Updating a variable using SET\nDECLARE @VariableName INT\nSET @VariableName = 1\nPRINT @VariableName\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 28\n1\nUsing SET, you can only update one variable at a time.\nSection 8.5: Updating variables by selecting from a table\nDepending on the structure of your data, you can create variables that update dynamically.\nDECLARE @CurrentID int = (SELECT TOP 1 ID FROM Table ORDER BY CreateDate desc)\nDECLARE @Year int = 2014\nDECLARE @CurrentID int = (SELECT ID FROM Table WHERE Year = @Year)\nIn most cases, you will want to ensure that your query returns only one value when using this method.\nSection 8.6: Compound assignment operators\nVersion ≥ SQL Server 2008 R2\nSupported compound operators:\n+= Add and assign\n-= Subtract and assign\n*= Multiply and assign\n/= Divide and assign\n%= Modulo and assign\n&= Bitwise AND and assign\n^= Bitwise XOR and assign\n|= Bitwise OR and assign\nExample usage:\nDECLARE @test INT = 42;\nSET @test += 1;\nPRINT @test;    --43\nSET @test -= 1;\nPRINT @test;    --42\nSET @test *= 2\nPRINT @test;    --84\nSET @test /= 2;\nPRINT @test;    --42\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 29\nChapter 9: Dates\nSection 9.1: Date & Time Formatting using CONVERT\nYou can use the CONVERT function to cast a datetime datatype to a formatted string.\nSELECT GETDATE() AS [Result] -- 2016-07-21 07:56:10.927\nYou can also use some built-in codes to convert into a speciﬁc format. Here are the options built into SQL Server:\nDECLARE @convert_code INT = 100 -- See Table Below\nSELECT CONVERT(VARCHAR(30), GETDATE(), @convert_code) AS [Result]\n@convert_code Result\n100 \"Jul 21 2016 7:56AM\"\n101 \"07/21/2016\"\n102 \"2016.07.21\"\n103 \"21/07/2016\"\n104 \"21.07.2016\"\n105 \"21-07-2016\"\n106 \"21 Jul 2016\"\n107 \"Jul 21, 2016\"\n108 \"07:57:05\"\n109 \"Jul 21 2016 7:57:45:707AM\"\n110 \"07-21-2016\"\n111 \"2016/07/21\"\n112 \"20160721\"\n113 \"21 Jul 2016 07:57:59:553\"\n114 \"07:57:59:553\"\n120 \"2016-07-21 07:57:59\"\n121 \"2016-07-21 07:57:59.553\"\n126 \"2016-07-21T07:58:34.340\"\n127 \"2016-07-21T07:58:34.340\"\n130 \"16 ???? 1437 7:58:34:340AM\"\n131 \"16/10/1437 7:58:34:340AM\"\nSELECT GETDATE() AS [Result]                                -- 2016-07-21 07:56:10.927\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),100) AS [Result] -- Jul 21 2016  7:56AM\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),101) AS [Result] -- 07/21/2016\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),102) AS [Result] -- 2016.07.21\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),103) AS [Result] -- 21/07/2016\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),104) AS [Result] -- 21.07.2016\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),105) AS [Result] -- 21-07-2016\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),106) AS [Result] -- 21 Jul 2016\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),107) AS [Result] -- Jul 21, 2016\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),108) AS [Result] -- 07:57:05\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),109) AS [Result] -- Jul 21 2016  7:57:45:707AM\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),110) AS [Result] -- 07-21-2016\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),111) AS [Result] -- 2016/07/21\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),112) AS [Result] -- 20160721\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),113) AS [Result] -- 21 Jul 2016 07:57:59:553\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),114) AS [Result] -- 07:57:59:553\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),120) AS [Result] -- 2016-07-21 07:57:59\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),121) AS [Result] -- 2016-07-21 07:57:59.553\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 30\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),126) AS [Result] -- 2016-07-21T07:58:34.340\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),127) AS [Result] -- 2016-07-21T07:58:34.340\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),130) AS [Result] -- 16 ???? 1437  7:58:34:340AM\nUNION SELECT CONVERT(VARCHAR(30),GETDATE(),131) AS [Result] -- 16/10/1437  7:58:34:340AM\nSection 9.2: Date & Time Formatting using FORMAT\nVersion ≥ SQL Server 2012\nYou can utilize the new function: FORMAT().\nUsing this you can transform your DATETIME ﬁelds to your own custom VARCHAR format.\nExample\nDECLARE @Date DATETIME = '2016-09-05 00:01:02.333'\nSELECT FORMAT(@Date, N'dddd, MMMM dd, yyyy hh:mm:ss tt')\nMonday, September 05, 2016 12:01:02 AM\nArguments\nGiven the DATETIME being formatted is 2016-09-05 00:01:02.333, the following chart shows what their output\nwould be for the provided argument.\nArgument Output\nyyyy 2016\nyy 16\nMMMM September\nMM 09\nM 9\ndddd Monday\nddd Mon\ndd 05\nd 5\nHH 00\nH 0\nhh 12\nh 12\nmm 01\nm 1\nss 02\ns 2\ntt AM\nt A\nﬀf 333\nﬀ 33\nf 3\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 31\nYou can also supply a single argument to the FORMAT() function to generate a pre-formatted output:\nDECLARE @Date DATETIME = '2016-09-05 00:01:02.333'\nSELECT FORMAT(@Date, N'U')\nMonday, September 05, 2016 4:01:02 AM\nSingle Argument Output\nD Monday, September 05, 2016\nd 9/5/2016\nF Monday, September 05, 2016 12:01:02 AM\nf Monday, September 05, 2016 12:01 AM\nG 9/5/2016 12:01:02 AM\ng 9/5/2016 12:01 AM\nM September 05\nO 2016-09-05T00:01:02.3330000\nR Mon, 05 Sep 2016 00:01:02 GMT\ns 2016-09-05T00:01:02\nT 12:01:02 AM\nt 12:01 AM\nU Monday, September 05, 2016 4:01:02 AM\nu 2016-09-05 00:01:02Z\nY September, 2016\nNote: The above list is using the en-US culture. A diﬀerent culture can be speciﬁed for the FORMAT() via the third\nparameter:\nDECLARE @Date DATETIME = '2016-09-05 00:01:02.333'\nSELECT FORMAT(@Date, N'U', 'zh-cn')\n2016年9月5日 4:01:02\nSection 9.3: DATEADD for adding and subtracting time\nperiods\nGeneral syntax:\nDATEADD (datepart , number , datetime_expr)  \nTo add a time measure, the number must be positive. To subtract a time measure, the number must be negative.\nExamples\nDECLARE @now DATETIME2 = GETDATE();\nSELECT @now;                        --2016-07-21 14:39:46.4170000\nSELECT DATEADD(YEAR, 1, @now)       --2017-07-21 14:39:46.4170000\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 32\nSELECT DATEADD(QUARTER, 1, @now)    --2016-10-21 14:39:46.4170000\nSELECT DATEADD(WEEK, 1, @now)       --2016-07-28 14:39:46.4170000\nSELECT DATEADD(DAY, 1, @now)        --2016-07-22 14:39:46.4170000\nSELECT DATEADD(HOUR, 1, @now)       --2016-07-21 15:39:46.4170000\nSELECT DATEADD(MINUTE, 1, @now)     --2016-07-21 14:40:46.4170000\nSELECT DATEADD(SECOND, 1, @now)     --2016-07-21 14:39:47.4170000\nSELECT DATEADD(MILLISECOND, 1, @now)--2016-07-21 14:39:46.4180000\nNOTE: DATEADD also accepts abbreviations in the datepart parameter. Use of these abbreviations is generally\ndiscouraged as they can be confusing (m vs mi, ww vs w, etc.).\nSection 9.4: Create function to calculate a person's age on a\nspeciﬁc date\nThis function will take 2 datetime parameters, the DOB, and a date to check the age at\n  CREATE FUNCTION [dbo].[Calc_Age]\n    (\n    @DOB datetime , @calcDate datetime\n    )\n    RETURNS int\n    AS\n   BEGIN\ndeclare @age int\nIF (@calcDate < @DOB  )\nRETURN -1\n-- If a DOB is supplied after the comparison date, then return -1\nSELECT @age = YEAR(@calcDate) - YEAR(@DOB) +\n  CASE WHEN DATEADD(year,YEAR(@calcDate) - YEAR(@DOB)\n  ,@DOB) > @calcDate THEN -1 ELSE 0 END\n   \nRETURN @age\n   \nEND\neg to check the age today of someone born on 1/1/2000\nSELECT  dbo.Calc_Age('2000-01-01',Getdate())\nSection 9.5: Get the current DateTime\nThe built-in functions GETDATE and GETUTCDATE each return the current date and time without a time zone oﬀset.\nThe return value of both functions is based on the operating system of the computer on which the instance of SQL\nServer is running.\nThe return value of GETDATE represents the current time in the same timezone as operating system. The return\nvalue of GETUTCDATE represents the current UTC time.\nEither function can be included in the SELECT clause of a query or as part of boolean expression in the WHERE clause.\nExamples:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 33\n-- example query that selects the current time in both the server time zone and UTC\nSELECT GETDATE() as SystemDateTime, GETUTCDATE() as UTCDateTime\n-- example query records with EventDate in the past.\nSELECT * FROM MyEvents WHERE EventDate < GETDATE()\nThere are a few other built-in functions that return diﬀerent variations of the current date-time:\nSELECT\n    GETDATE(),          --2016-07-21 14:27:37.447\n    GETUTCDATE(),       --2016-07-21 18:27:37.447\n    CURRENT_TIMESTAMP,  --2016-07-21 14:27:37.447\n    SYSDATETIME(),      --2016-07-21 14:27:37.4485768\n    SYSDATETIMEOFFSET(),--2016-07-21 14:27:37.4485768 -04:00\n    SYSUTCDATETIME()    --2016-07-21 18:27:37.4485768\nSection 9.6: Getting the last day of a month\nUsing the DATEADD and DATEDIFF functions, it's possible to return the last date of a month.\nSELECT DATEADD(d, -1, DATEADD(m, DATEDIFF(m, 0, '2016-09-23') + 1, 0))\n-- 2016-09-30 00:00:00.000\nVersion ≥ SQL Server 2012\nThe EOMONTH function provides a more concise way to return the last date of a month, and has an optional\nparameter to oﬀset the month.\nSELECT EOMONTH('2016-07-21')        --2016-07-31\nSELECT EOMONTH('2016-07-21', 4)     --2016-11-30\nSELECT EOMONTH('2016-07-21', -5)    --2016-02-29\nSection 9.7: CROSS PLATFORM DATE OBJECT\nVersion ≥ SQL Server 2012\nIn Transact SQL , you may deﬁne an object as Date (or DateTime) using the [DATEFROMPARTS][1] (or\n[DATETIMEFROMPARTS][1]) function like following:\n DECLARE @myDate DATE=DATEFROMPARTS(1988,11,28)\n DECLARE @someMoment DATETIME=DATEFROMPARTS(1988,11,28,10,30,50,123)\nThe parameters you provide are Year, Month, Day for the DATEFROMPARTS function and, for the DATETIMEFROMPARTS\nfunction you will need to provide year, month, day, hour, minutes, seconds and milliseconds.\nThese methods are useful and worth being used because using the plain string to build a date(or datetime) may fail\ndepending on the host machine region, location or date format settings.\nSection 9.8: Return just Date from a DateTime\nThere are many ways to return a Date from a DateTime object\nSELECT CONVERT(Date, GETDATE())1.\nSELECT DATEADD(dd, 0, DATEDIFF(dd, 0, GETDATE())) returns 2016-07-21 00:00:00.0002.\nSELECT CAST(GETDATE() AS DATE)3.\nSELECT CONVERT(CHAR(10),GETDATE(),111)4.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 34\nSELECT FORMAT(GETDATE(), 'yyyy-MM-dd')5.\nNote that options 4 and 5 returns a string, not a date.\nSection 9.9: DATEDIFF for calculating time period dierences\nGeneral syntax:\nDATEDIFF (datepart, datetime_expr1, datetime_expr2)\nIt will return a positive number if datetime_expr is in the past relative to datetime_expr2, and a negative number\notherwise.\nExamples\nDECLARE @now DATETIME2 = GETDATE();\nDECLARE @oneYearAgo DATETIME2 = DATEADD(YEAR, -1, @now);\nSELECT @now                                    --2016-07-21 14:49:50.9800000\nSELECT @oneYearAgo                             --2015-07-21 14:49:50.9800000\nSELECT DATEDIFF(YEAR, @oneYearAgo, @now)       --1\nSELECT DATEDIFF(QUARTER, @oneYearAgo, @now)    --4\nSELECT DATEDIFF(WEEK, @oneYearAgo, @now)       --52\nSELECT DATEDIFF(DAY, @oneYearAgo, @now)        --366\nSELECT DATEDIFF(HOUR, @oneYearAgo, @now)       --8784\nSELECT DATEDIFF(MINUTE, @oneYearAgo, @now)     --527040\nSELECT DATEDIFF(SECOND, @oneYearAgo, @now)     --31622400\nNOTE: DATEDIFF also accepts abbreviations in the datepart parameter. Use of these abbreviations is generally\ndiscouraged as they can be confusing (m vs mi, ww vs w, etc.).\nDATEDIFF can also be used to determine the oﬀset between UTC and the local time of the SQL Server. The following\nstatement can be used to calculate the oﬀset between UTC and local time (including timezone).\nselect  DATEDIFF(hh, getutcdate(), getdate()) as 'CentralTimeOffset'\nSection 9.10: DATEPART & DATENAME\nDATEPART returns the speciﬁed datepart of the speciﬁed datetime expression as a numeric value.\nDATENAME returns a character string that represents the speciﬁed datepart of the speciﬁed date. In practice\nDATENAME is mostly useful for getting the name of the month or the day of the week.\nThere are also some shorthand functions to get the year, month or day of a datetime expression, which behave like\nDATEPART with their respective datepart units.\nSyntax:\nDATEPART ( datepart , datetime_expr )\nDATENAME ( datepart , datetime_expr )\nDAY ( datetime_expr )\nMONTH ( datetime_expr )\nYEAR ( datetime_expr )\nExamples:\nDECLARE @now DATETIME2 = GETDATE();\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 35\nSELECT @now                       --2016-07-21 15:05:33.8370000\nSELECT DATEPART(YEAR, @now)       --2016\nSELECT DATEPART(QUARTER, @now)    --3\nSELECT DATEPART(WEEK, @now)       --30\nSELECT DATEPART(HOUR, @now)       --15\nSELECT DATEPART(MINUTE, @now)     --5\nSELECT DATEPART(SECOND, @now)     --33\n-- Differences between DATEPART and DATENAME:\nSELECT DATEPART(MONTH, @now)      --7\nSELECT DATENAME(MONTH, @now)      --July\nSELECT DATEPART(WEEKDAY, @now)    --5\nSELECT DATENAME(WEEKDAY, @now)    --Thursday\n--shorthand functions\nSELECT DAY(@now)    --21\nSELECT MONTH(@now)  --7\nSELECT YEAR(@now)   --2016\nNOTE: DATEPART and DATENAME also accept abbreviations in the datepart parameter. Use of these abbreviations is\ngenerally discouraged as they can be confusing (m vs mi, ww vs w, etc.).\nSection 9.11: Date parts reference\nThese are the datepart values available to date & time functions:\ndatepart Abbreviations\nyear yy, yyyy\nquarter qq, q\nmonth mm, m\ndayofyear dy, y\nday dd, d\nweek wk, ww\nweekday dw, w\nhour hh\nminute mi, n\nsecond ss, s\nmillisecond ms\nmicrosecond mcs\nnanosecond ns\nNOTE: Use of abbreviations is generally discouraged as they can be confusing (m vs mi, ww vs w, etc.). The long\nversion of the datepart representation promotes clarity and readability, and should be used whenever possible\n(month, minute, week, weekday, etc.).\nSection 9.12: Date Format Extended\nDate Format SQL Statement Sample\nOutput\nYY-MM-DD SELECT RIGHT(CONVERT(VARCHAR(10), SYSDATETIME(), 20), 8) AS [YY-MM-DD]\nSELECT REPLACE(CONVERT(VARCHAR(8), SYSDATETIME(), 11), '/', '-') AS [YY-MM-DD] 11-06-08\nYYYY-MM-DD\nSELECT CONVERT(VARCHAR(10), SYSDATETIME(), 120) AS [YYYY-MM-DD]\nSELECT REPLACE(CONVERT(VARCHAR(10), SYSDATETIME(), 111), '/', '-') AS [YYYY-\nMM-DD]\n2011-06-08\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 36\nYYYY-M-D\nSELECT CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) + '-' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' + CAST(DAY(SYSDATETIME()) AS\nVARCHAR(2)) AS [YYYY-M-D]\n2011-6-8\nYY-M-D\nSELECT RIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) + '-' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' + CAST(DAY(SYSDATETIME()) AS\nVARCHAR(2)) AS [YY-M-D]\n11-6-8\nM-D-YYYY\nSELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '-' + CAST(YEAR(SYSDATETIME()) AS\nVARCHAR(4)) AS [M-D-YYYY]\n6-8-2011\nM-D-YY\nSELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [M-D-YY]\n6-8-11\nD-M-YYYY\nSELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' + CAST(YEAR(SYSDATETIME())\nAS VARCHAR(4)) AS [D-M-YYYY]\n8-6-2011\nD-M-YY\nSELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [D-M-YY]\n8-6-11\nYY-MM\nSELECT RIGHT(CONVERT(VARCHAR(7), SYSDATETIME(), 20), 5) AS [YY-MM]\nSELECT SUBSTRING(CONVERT(VARCHAR(10), SYSDATETIME(), 120), 3, 5) AS [YY-\nMM]\n11-06\nYYYY-MM SELECT CONVERT(VARCHAR(7), SYSDATETIME(), 120) AS [YYYY-MM] 2011-06\nYY-M SELECT RIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) + '-' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [YY-M] 11-6\nYYYY-M SELECT CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) + '-' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [YYYY-M] 2011-6\nMM-YY SELECT RIGHT(CONVERT(VARCHAR(8), SYSDATETIME(), 5), 5) AS [MM-YY]\nSELECT SUBSTRING(CONVERT(VARCHAR(8), SYSDATETIME(), 5), 4, 5) AS [MM-YY] 06-11\nMM-YYYY SELECT RIGHT(CONVERT(VARCHAR(10), SYSDATETIME(), 105), 7) AS [MM-YYYY] 06-2011\nM-YY SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [M-YY] 6-11\nM-YYYY SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nCAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) AS [M-YYYY] 6-2011\nMM-DD SELECT CONVERT(VARCHAR(5), SYSDATETIME(), 10) AS [MM-DD] 06-08\nDD-MM SELECT CONVERT(VARCHAR(5), SYSDATETIME(), 5) AS [DD-MM] 08-06\nM-D SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) AS [M-D] 6-8\nD-M SELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '-' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [D-M] 8-6\nM/D/YYYY\nSELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '/' + CAST(YEAR(SYSDATETIME()) AS\nVARCHAR(4)) AS [M/D/YYYY]\n6/8/2011\nM/D/YY\nSELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [M/D/YY]\n6/8/11\nD/M/YYYY\nSELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' + CAST(YEAR(SYSDATETIME())\nAS VARCHAR(4)) AS [D/M/YYYY]\n8/6/2011\nD/M/YY\nSELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [D/M/YY]\n8/6/11\nYYYY/M/D\nSELECT CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) + '/' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' + CAST(DAY(SYSDATETIME()) AS\nVARCHAR(2)) AS [YYYY/M/D]\n2011/6/8\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 37\nYY/M/D\nSELECT RIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) + '/' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' + CAST(DAY(SYSDATETIME()) AS\nVARCHAR(2)) AS [YY/M/D]\n11/6/8\nMM/YY SELECT RIGHT(CONVERT(VARCHAR(8), SYSDATETIME(), 3), 5) AS [MM/YY] 06/11\nMM/YYYY SELECT RIGHT(CONVERT(VARCHAR(10), SYSDATETIME(), 103), 7) AS [MM/YYYY] 06/2011\nM/YY SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [M/YY] 6/11\nM/YYYY SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nCAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) AS [M/YYYY] 6/2011\nYY/MM SELECT CONVERT(VARCHAR(5), SYSDATETIME(), 11) AS [YY/MM] 11/06\nYYYY/MM SELECT CONVERT(VARCHAR(7), SYSDATETIME(), 111) AS [YYYY/MM] 2011/06\nYY/M SELECT RIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) + '/' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [YY/M] 11/6\nYYYY/M SELECT CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) + '/' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [YYYY/M] 2011/6\nMM/DD SELECT CONVERT(VARCHAR(5), SYSDATETIME(), 1) AS [MM/DD] 06/08\nDD/MM SELECT CONVERT(VARCHAR(5), SYSDATETIME(), 3) AS [DD/MM] 08/06\nM/D SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) AS [M/D] 6/8\nD/M SELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '/' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [D/M] 8/6\nMM.DD.YYYY SELECT REPLACE(CONVERT(VARCHAR(10), SYSDATETIME(), 101), '/', '.') AS\n[MM.DD.YYYY] 06.08.2011\nMM.DD.YY SELECT REPLACE(CONVERT(VARCHAR(8), SYSDATETIME(), 1), '/', '.') AS [MM.DD.YY] 06.08.11\nM.D.YYYY\nSELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '.' + CAST(YEAR(SYSDATETIME()) AS\nVARCHAR(4)) AS [M.D.YYYY]\n6.8.2011\nM.D.YY\nSELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nCAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [M.D.YY]\n6.8.11\nDD.MM.YYYY SELECT CONVERT(VARCHAR(10), SYSDATETIME(), 104) AS [DD.MM.YYYY] 08.06.2011\nDD.MM.YY SELECT CONVERT(VARCHAR(10), SYSDATETIME(), 4) AS [DD.MM.YY] 08.06.11\nD.M.YYYY\nSELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' + CAST(YEAR(SYSDATETIME())\nAS VARCHAR(4)) AS [D.M.YYYY]\n8.6.2011\nD.M.YY\nSELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [D.M.YY]\n8.6.11\nYYYY.M.D\nSELECT CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) + '.' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' + CAST(DAY(SYSDATETIME()) AS\nVARCHAR(2)) AS [YYYY.M.D]\n2011.6.8\nYY.M.D\nSELECT RIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) + '.' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' + CAST(DAY(SYSDATETIME()) AS\nVARCHAR(2)) AS [YY.M.D]\n11.6.8\nMM.YYYY SELECT RIGHT(CONVERT(VARCHAR(10), SYSDATETIME(), 104), 7) AS [MM.YYYY] 06.2011\nMM.YY SELECT RIGHT(CONVERT(VARCHAR(8), SYSDATETIME(), 4), 5) AS [MM.YY] 06.11\nM.YYYY SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nCAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) AS [M.YYYY] 6.2011\nM.YY SELECT CAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) + '.' +\nRIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS [M.YY] 6.11\nYYYY.MM SELECT CONVERT(VARCHAR(7), SYSDATETIME(), 102) AS [YYYY.MM] 2011.06\nYY.MM SELECT CONVERT(VARCHAR(5), SYSDATETIME(), 2) AS [YY.MM] 11.06\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 38\nYYYY.M SELECT CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)) + '.' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [YYYY.M] 2011.6\nYY.M SELECT RIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) + '.' +\nCAST(MONTH(SYSDATETIME()) AS VARCHAR(2)) AS [YY.M] 11.6\nMM.DD SELECT RIGHT(CONVERT(VARCHAR(8), SYSDATETIME(), 2), 5) AS [MM.DD] 06.08\nDD.MM SELECT CONVERT(VARCHAR(5), SYSDATETIME(), 4) AS [DD.MM] 08.06\nMMDDYYYY SELECT REPLACE(CONVERT(VARCHAR(10), SYSDATETIME(), 101), '/', '') AS\n[MMDDYYYY] 06082011\nMMDDYY SELECT REPLACE(CONVERT(VARCHAR(8), SYSDATETIME(), 1), '/', '') AS [MMDDYY] 060811\nDDMMYYYY SELECT REPLACE(CONVERT(VARCHAR(10), SYSDATETIME(), 103), '/', '') AS\n[DDMMYYYY] 08062011\nDDMMYY SELECT REPLACE(CONVERT(VARCHAR(8), SYSDATETIME(), 3), '/', '') AS [DDMMYY] 080611\nMMYYYY SELECT RIGHT(REPLACE(CONVERT(VARCHAR(10), SYSDATETIME(), 103), '/', ''), 6) AS\n[MMYYYY] 062011\nMMYY SELECT RIGHT(REPLACE(CONVERT(VARCHAR(8), SYSDATETIME(), 3), '/', ''), 4) AS\n[MMYY] 0611\nYYYYMM SELECT CONVERT(VARCHAR(6), SYSDATETIME(), 112) AS [YYYYMM] 201106\nYYMM SELECT CONVERT(VARCHAR(4), SYSDATETIME(), 12) AS [YYMM] 1106\nMonth DD, YYYY SELECT DATENAME(MONTH, SYSDATETIME())+ ' ' + RIGHT('0' + DATENAME(DAY,\nSYSDATETIME()), 2) + ', ' + DATENAME(YEAR, SYSDATETIME()) AS [Month DD, YYYY] June 08, 2011\nMon YYYY SELECT LEFT(DATENAME(MONTH, SYSDATETIME()), 3) + ' ' + DATENAME(YEAR,\nSYSDATETIME()) AS [Mon YYYY] Jun 2011\nMonth YYYY SELECT DATENAME(MONTH, SYSDATETIME()) + ' ' + DATENAME(YEAR,\nSYSDATETIME()) AS [Month YYYY] June 2011\nDD Month SELECT RIGHT('0' + DATENAME(DAY, SYSDATETIME()), 2) + ' ' + DATENAME(MONTH,\nSYSDATETIME()) AS [DD Month] 08 June\nMonth DD SELECT DATENAME(MONTH, SYSDATETIME()) + ' ' + RIGHT('0' + DATENAME(DAY,\nSYSDATETIME()), 2) AS [Month DD] June 08\nDD Month YY\nSELECT CAST(DAY(SYSDATETIME()) AS VARCHAR(2)) + ' ' + DATENAME(MM,\nSYSDATETIME()) + ' ' + RIGHT(CAST(YEAR(SYSDATETIME()) AS VARCHAR(4)), 2) AS\n[DD Month YY]\n08 June 11\nDD Month YYYY SELECT RIGHT('0' + DATENAME(DAY, SYSDATETIME()), 2) + ' ' + DATENAME(MONTH,\nSYSDATETIME())+ ' ' + DATENAME(YEAR, SYSDATETIME()) AS [DD Month YYYY] 08 June 2011\nMon-YY SELECT REPLACE(RIGHT(CONVERT(VARCHAR(9), SYSDATETIME(), 6), 6), ' ', '-') AS\n[Mon-YY] Jun-08\nMon-YYYY SELECT REPLACE(RIGHT(CONVERT(VARCHAR(11), SYSDATETIME(), 106), 8), ' ', '-') AS\n[Mon-YYYY] Jun-2011\nDD-Mon-YY SELECT REPLACE(CONVERT(VARCHAR(9), SYSDATETIME(), 6), ' ', '-') AS [DD-Mon-YY] 08-Jun-11\nDD-Mon-YYYY SELECT REPLACE(CONVERT(VARCHAR(11), SYSDATETIME(), 106), ' ', '-') AS [DD-Mon-\nYYYY] 08-Jun-2011\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 39\nChapter 10: Generating a range of dates\nParameter Details\n@FromDate The inclusive lower boundary of the generated date range.\n@ToDate The inclusive upper boundary of the generated date range.\nSection 10.1: Generating Date Range With Recursive CTE\nUsing a Recursive CTE, you can generate an inclusive range of dates:\nDeclare  @FromDate    Date = '2014-04-21',\n         @ToDate      Date = '2014-05-02'\n;With DateCte (Date) As\n(\n    Select  @FromDate Union All\n    Select  DateAdd(Day, 1, Date)\n    From    DateCte\n    Where   Date < @ToDate\n)\nSelect  Date\nFrom    DateCte\nOption  (MaxRecursion 0)\nThe default MaxRecursion setting is 100. Generating more than 100 dates using this method will require the Option\n(MaxRecursion N) segment of the query, where N is the desired MaxRecursion setting. Setting this to 0 will remove\nthe MaxRecursion limitation altogether.\nSection 10.2: Generating a Date Range With a Tally Table\nAnother way you can generate a range of dates is by utilizing a Tally Table to create the dates between the range:\nDeclare   @FromDate   Date = '2014-04-21',\n          @ToDate     Date = '2014-05-02'\n;With\n   E1(N) As (Select 1 From (Values (1), (1), (1), (1), (1), (1), (1), (1), (1), (1)) DT(N)),\n   E2(N) As (Select 1 From E1 A Cross Join E1 B),\n   E4(N) As (Select 1 From E2 A Cross Join E2 B),\n   E6(N) As (Select 1 From E4 A Cross Join E2 B),\n   Tally(N) As\n   (\n        Select    Row_Number() Over (Order By (Select Null))\n        From    E6\n   )\nSelect   DateAdd(Day, N - 1, @FromDate) Date\nFrom     Tally\nWhere    N <= DateDiff(Day, @FromDate, @ToDate) + 1\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 40\nChapter 11: Database Snapshots\nSection 11.1: Create a database snapshot\nA database snapshot is a read-only, static view of a SQL Server database (the source database). It is similar to\nbackup, but it is available as any other database so client can query snapshot database.\nCREATE DATABASE MyDatabase_morning -- name of the snapshot\nON (\n     NAME=MyDatabase_data, -- logical name of the data file of the source database\n     FILENAME='C:\\SnapShots\\MySnapshot_Data.ss' -- snapshot file;\n)\nAS SNAPSHOT OF MyDatabase; -- name of source database\nYou can also create snapshot of database with multiple ﬁles:\nCREATE DATABASE MyMultiFileDBSnapshot ON\n    (NAME=MyMultiFileDb_ft, FILENAME='C:\\SnapShots\\MyMultiFileDb_ft.ss'),\n    (NAME=MyMultiFileDb_sys, FILENAME='C:\\SnapShots\\MyMultiFileDb_sys.ss'),\n    (NAME=MyMultiFileDb_data, FILENAME='C:\\SnapShots\\MyMultiFileDb_data.ss'),\n    (NAME=MyMultiFileDb_indx, FILENAME='C:\\SnapShots\\MyMultiFileDb_indx.ss')\nAS SNAPSHOT OF MultiFileDb;\nSection 11.2: Restore a database snapshot\nIf data in a source database becomes damaged or some wrong data is written into database, in some cases,\nreverting the database to a database snapshot that predates the damage might be an appropriate alternative to\nrestoring the database from a backup.\nRESTORE DATABASE MYDATABASE FROM DATABASE_SNAPSHOT='MyDatabase_morning';\nWarning: This will delete all changes made to the source database since the snapshot was taken!\nSection 11.3: DELETE Snapshot\nYou can delete existing snapshots of database using DELETE DATABASE statement:\nDROP DATABASE Mydatabase_morning\nIn this statement you should reference name of the database snapshot.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 41\nChapter 12: COALESCE\nSection 12.1: Using COALESCE to Build Comma-Delimited String\nWe can get a comma delimited string from multiple rows using coalesce as shown below.\nSince table variable is used, we need to execute whole query once. So to make easy to understand, I have added\nBEGIN and END block.\nBEGIN\n    --Table variable declaration to store sample records\n    DECLARE @Table TABLE (FirstName varchar(256), LastName varchar(256))\n    --Inserting sample records into table variable @Table\n    INSERT INTO @Table (FirstName, LastName)\n    VALUES\n    ('John','Smith'),\n    ('Jane','Doe')\n    --Creating variable to store result          \n    DECLARE @Names varchar(4000)\n    --Used COLESCE function, so it will concatenate comma separated FirstName into @Names varible\n    SELECT @Names = COALESCE(@Names + ',', '') + FirstName\n    FROM @Table\n    --Now selecting actual result\n    SELECT @Names\n    END\nSection 12.2: Getting the ﬁrst not null from a list of column\nvalues\nSELECT COALESCE(NULL, NULL, 'TechOnTheNet.com', NULL, 'CheckYourMath.com');\nResult: 'TechOnTheNet.com'\nSELECT COALESCE(NULL, 'TechOnTheNet.com', 'CheckYourMath.com');\nResult: 'TechOnTheNet.com'\nSELECT COALESCE(NULL, NULL, 1, 2, 3, NULL, 4);\nResult: 1\nSection 12.3: Coalesce basic Example\nCOALESCE() returns the ﬁrst NON NULL value in a list of arguments. Suppose we had a table containing phone\nnumbers, and cell phone numbers and wanted to return only one for each user. In order to only obtain one, we can\nget the ﬁrst NON NULL value.\nDECLARE @Table TABLE (UserID int, PhoneNumber varchar(12), CellNumber varchar(12))\nINSERT INTO @Table (UserID, PhoneNumber, CellNumber)\nVALUES\n(1,'555-869-1123',NULL),\n(2,'555-123-7415','555-846-7786'),\n(3,NULL,'555-456-8521')\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 42\nSELECT\n    UserID,\n    COALESCE(PhoneNumber, CellNumber)\nFROM\n    @Table\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 43\nChapter 13: IF...ELSE\nSection 13.1: Single IF statement\nLike most of the other programming languages, T-SQL also supports IF..ELSE statements.\nFor example in the example below 1 = 1 is the expression, which evaluates to True and the control enters the\nBEGIN..END block and the Print statement prints the string 'One is equal to One'\nIF ( 1 = 1)  --<-- Some Expression\n BEGIN\n   PRINT 'One is equal to One'\n END\nSection 13.2: Multiple IF Statements\nWe can use multiple IF statement to check multiple expressions totally independent from each other.\nIn the example below, each IF statement's expression is evaluated and if it is true the code inside the BEGIN...END\nblock is executed. In this particular example, the First and Third expressions are true and only those print\nstatements will be executed.\nIF (1 = 1)  --<-- Some Expression      --<-- This is true\nBEGIN\n    PRINT 'First IF is True'           --<-- this will be executed\nEND\nIF (1 = 2)  --<-- Some Expression\nBEGIN\n    PRINT 'Second IF is True'\nEND\nIF (3 = 3)  --<-- Some Expression        --<-- This true\nBEGIN\n    PRINT 'Thrid IF is True'             --<-- this will be executed\nEND\nSection 13.3: Single IF..ELSE statement\nIn a single IF..ELSE statement, if the expression evaluates to True in the IF statement the control enters the ﬁrst\nBEGIN..END block and only the code inside that block gets executed , Else block is simply ignored.\nOn the other hand if the expression evaluates to False the ELSE BEGIN..END block gets executed and the control\nnever enters the ﬁrst BEGIN..END Block.\nIn the Example below the expression will evaluate to false and the Else block will be executed printing the string\n'First expression was not true'\nIF ( 1 <> 1)  --<-- Some Expression\n BEGIN\n     PRINT 'One is equal to One'\n END\nELSE\n BEGIN\n     PRINT 'First expression was not true'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 44\n END\nSection 13.4: Multiple IF... ELSE with ﬁnal ELSE Statements\nIf we have Multiple IF...ELSE IF statements but we also want also want to execute some piece of code if none of\nexpressions are evaluated to True , then we can simple add a ﬁnal ELSE block which only gets executed if none of\nthe IF or ELSE IF expressions are evaluated to true.\nIn the example below none of the IF or ELSE IF expression are True hence only ELSE block is executed and prints\n'No other expression is true'\nIF ( 1 = 1 + 1 )\n    BEGIN\n       PRINT 'First If Condition'\n    END\nELSE IF (1 = 2)\n    BEGIN\n        PRINT 'Second If Else Block'\n    END\nELSE IF (1 = 3)\n    BEGIN\n        PRINT 'Third If Else Block'\n    END\nELSE\n    BEGIN\n        PRINT 'No other expression is true'  --<-- Only this statement will be printed\n    END\nSection 13.5: Multiple IF...ELSE Statements\nMore often than not we need to check multiple expressions and take speciﬁc actions based on those expressions.\nThis situation is handled using multiple IF...ELSE IF statements.\nIn this example all the expressions are evaluated from top to bottom. As soon as an expression evaluates to true,\nthe code inside that block is executed. If no expression is evaluated to true, nothing gets executed.\nIF (1 = 1 + 1)\nBEGIN\n    PRINT 'First If Condition'\nEND\nELSE IF (1 = 2)\nBEGIN\n    PRINT 'Second If Else Block'\nEND\nELSE IF (1 = 3)\nBEGIN\n    PRINT 'Third If Else Block'\nEND\nELSE IF (1 = 1)      --<-- This is True\nBEGIN\n    PRINT 'Last Else Block'  --<-- Only this statement will be printed\nEND\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 45\nChapter 14: CASE Statement\nSection 14.1: Simple CASE statement\nIn a simple case statement, one value or variable is checked against multiple possible answers. The code below is\nan example of a simple case statement:\nSELECT CASE DATEPART(WEEKDAY, GETDATE())\n    WHEN 1 THEN 'Sunday'\n    WHEN 2 THEN 'Monday'\n    WHEN 3 THEN 'Tuesday'\n    WHEN 4 THEN 'Wednesday'\n    WHEN 5 THEN 'Thursday'\n    WHEN 6 THEN 'Friday'\n    WHEN 7 THEN 'Saturday'\nEND\nSection 14.2: Searched CASE statement\nIn a Searched Case statement, each option can test one or more values independently. The code below is an\nexample of a searched case statement:\nDECLARE @FirstName varchar(30) = 'John'\nDECLARE @LastName varchar(30) = 'Smith'\nSELECT CASE\n    WHEN LEFT(@FirstName, 1) IN ('a','e','i','o','u')\n        THEN 'First name starts with a vowel'\n    WHEN LEFT(@LastName, 1) IN ('a','e','i','o','u')\n        THEN 'Last name starts with a vowel'\n    ELSE\n        'Neither name starts with a vowel'\nEND\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 46\nChapter 15: INSERT INTO\nThe INSERT INTO statement is used to insert new records in a table.\nSection 15.1: INSERT multiple rows of data\nTo insert multiple rows of data in SQL Server 2008 or later:\nINSERT INTO USERS VALUES\n(2, 'Michael', 'Blythe'),\n(3, 'Linda', 'Mitchell'),\n(4, 'Jillian', 'Carson'),\n(5, 'Garrett', 'Vargas');\nTo insert multiple rows of data in earlier versions of SQL Server, use \"UNION ALL\" like so:\nINSERT INTO USERS (FIRST_NAME, LAST_NAME)\nSELECT 'James', 'Bond' UNION ALL\nSELECT 'Miss', 'Moneypenny' UNION ALL\nSELECT 'Raoul', 'Silva'\nNote, the \"INTO\" keyword is optional in INSERT queries. Another warning is that SQL server only supports 1000\nrows in one INSERT so you have to split them in batches.\nSection 15.2: Use OUTPUT to get the new Id\nWhen INSERTing, you can use OUTPUT INSERTED.ColumnName to get values from the newly inserted row, for example\nthe newly generated Id - useful if you have an IDENTITY column or any sort of default or calculated value.\nWhen programmatically calling this (e.g., from ADO.net) you would treat it as a normal query and read the values as\nif you would've made a SELECT-statement.\n-- CREATE TABLE OutputTest ([Id] INT NOT NULL PRIMARY KEY IDENTITY, [Name] NVARCHAR(50))\nINSERT INTO OutputTest ([Name])\nOUTPUT INSERTED.[Id]\nVALUES ('Testing')\nIf the ID of the recently added row is required inside the same set of query or stored procedure.\n-- CREATE a table variable having column with the same datatype of the ID\nDECLARE @LastId TABLE ( id int);\nINSERT INTO OutputTest ([Name])\nOUTPUT INSERTED.[Id] INTO @LastId\nVALUES ('Testing')\nSELECT id FROM @LastId\n-- We can set the value in a variable and use later in procedure\nDECLARE @LatestId int = (SELECT id FROM @LastId)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 47\nSection 15.3: INSERT from SELECT Query Results\nTo insert data retrieved from SQL query (single or multiple rows)\nINSERT INTO Table_name (FirstName, LastName, Position)\nSELECT FirstName, LastName, 'student' FROM Another_table_name\nNote, 'student' in SELECT is a string constant that will be inserted in each row.\nIf required, you can select and insert data from/into the same table\nSection 15.4: INSERT a single row of data\nA single row of data can be inserted in two ways:\nINSERT INTO USERS(Id, FirstName, LastName)\nVALUES (1, 'Mike', 'Jones');\nOr\nINSERT INTO USERS\nVALUES (1, 'Mike', 'Jones');\nNote that the second insert statement only allows the values in exactly the same order as the table columns\nwhereas in the ﬁrst insert, the order of the values can be changed like:\nINSERT INTO USERS(FirstName, LastName, Id)\nVALUES ('Mike', 'Jones', 1);\nSection 15.5: INSERT on speciﬁc columns\nTo do an insert on speciﬁc columns (as opposed to all of them) you must specify the columns you want to update.\nINSERT INTO USERS (FIRST_NAME, LAST_NAME)\nVALUES ('Stephen', 'Jiang');\nThis will only work if the columns that you did not list are nullable, identity, timestamp data type or computed\ncolumns; or columns that have a default value constraint. Therefore, if any of them are non-nullable, non-identity,\nnon-timestamp, non-computed, non-default valued columns...then attempting this kind of insert will trigger an\nerror message telling you that you have to provide a value for the applicable ﬁeld(s).\nSection 15.6: INSERT Hello World INTO table\nCREATE TABLE MyTableName\n(\n    Id INT,\n    MyColumnName NVARCHAR(1000)\n)\nGO\nINSERT INTO MyTableName (Id, MyColumnName)\nVALUES (1, N'Hello World!')\nGO    \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 48\nChapter 16: MERGE\nStarting with SQL Server 2008, it is possible to perform insert, update, or delete operations in a single statement\nusing the MERGE statement.\nThe MERGE statement allows you to join a data source with a target table or view, and then perform multiple\nactions against the target based on the results of that join.\nSection 16.1: MERGE to Insert / Update / Delete\nMERGE INTO targetTable\nUSING sourceTable\nON (targetTable.PKID = sourceTable.PKID)\nWHEN MATCHED AND (targetTable.PKID > 100) THEN\n    DELETE\nWHEN MATCHED AND (targetTable.PKID <= 100) THEN\n    UPDATE SET\n        targetTable.ColumnA = sourceTable.ColumnA,\n        targetTable.ColumnB = sourceTable.ColumnB\nWHEN NOT MATCHED THEN\n    INSERT (ColumnA, ColumnB) VALUES (sourceTable.ColumnA, sourceTable.ColumnB);\nWHEN NOT MATCHED BY SOURCE THEN\n    DELETE\n; --< Required\nDescription:\nMERGE INTO targetTable - table to be modiﬁed\nUSING sourceTable - source of data (can be table or view or table valued function)\nON ... - join condition between targetTable and sourceTable.\nWHEN MATCHED - actions to take when a match is found\nAND (targetTable.PKID > 100) - additional condition(s) that must be satisﬁed in order for the action\nto be taken\nTHEN DELETE - delete matched record from the targetTable\nTHEN UPDATE - update columns of matched record speciﬁed by SET ....\nWHEN NOT MATCHED - actions to take when match is not found in targetTable\nWHEN NOT MATCHED BY SOURCE - actions to take when match is not found in sourceTable\nComments:\nIf a speciﬁc action is not needed then omit the condition e.g. removing WHEN NOT MATCHED THEN INSERT will prevent\nrecords from being inserted\nMerge statement requires a terminating semicolon.\nRestrictions:\nWHEN MATCHED does not allow INSERT action\nUPDATE action can update a row only once. This implies that the join condition must produce unique matches.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 49\nSection 16.2: Merge Using CTE Source\nWITH SourceTableCTE AS\n(\n    SELECT * FROM SourceTable\n)\nMERGE  \n TargetTable AS target\nUSING SourceTableCTE AS source  \nON (target.PKID = source.PKID)\nWHEN MATCHED THEN    \n    UPDATE SET target.ColumnA = source.ColumnA\nWHEN NOT MATCHED THEN\n    INSERT (ColumnA) VALUES (Source.ColumnA);\nSection 16.3: Merge Example - Synchronize Source And Target\nTable\nTo Illustrate the MERGE Statement, consider the following two tables -\ndbo.Product : This table contains information about the product that company is currently selling1.\ndbo.ProductNew: This table contains information about the product that the company will sell in the future.2.\nThe following T-SQL will create and populate these two tables\nIF OBJECT_id(N'dbo.Product',N'U') IS NOT NULL\nDROP TABLE dbo.Product\nGO\nCREATE TABLE dbo.Product (\nProductID INT PRIMARY KEY,\nProductName NVARCHAR(64),\nPRICE MONEY\n)\nIF OBJECT_id(N'dbo.ProductNew',N'U') IS NOT NULL\nDROP TABLE dbo.ProductNew\nGO\nCREATE TABLE dbo.ProductNew (\nProductID INT PRIMARY KEY,\nProductName NVARCHAR(64),\nPRICE MONEY\n)\nINSERT INTO dbo.Product VALUES(1,'IPod',300)\n,(2,'IPhone',400)\n,(3,'ChromeCast',100)\n,(4,'raspberry pi',50)\nINSERT INTO dbo.ProductNew VALUES(1,'Asus Notebook',300)\n,(2,'Hp Notebook',400)\n,(3,'Dell Notebook',100)\n,(4,'raspberry pi',50)\nNow, Suppose we want to synchoronize the dbo.Product Target Table with the dbo.ProductNew table. Here is the\ncriterion for this task:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 50\nProduct that exist in both the dbo.ProductNew source table and the dbo.Product target table are updated in1.\nthe dbo.Product target table with new new Products.\nAny product in the dbo.ProductNew source table that do not exist in the dob.Product target table are2.\ninserted into the dbo.Product target table.\nAny Product in the dbo.Product target table that do not exist in the dbo.ProductNew source table must be3.\ndeleted from the dbo.Product target table. Here is the MERGE statement to perform this task.\nMERGE dbo.Product AS SourceTbl\nUSING dbo.ProductNew AS TargetTbl ON (SourceTbl.ProductID = TargetTbl.ProductID)\nWHEN MATCHED\n            AND SourceTbl.ProductName <> TargetTbl.ProductName\n            OR SourceTbl.Price <> TargetTbl.Price\n    THEN UPDATE SET SourceTbl.ProductName = TargetTbl.ProductName,\n                SourceTbl.Price = TargetTbl.Price\nWHEN NOT MATCHED\n    THEN INSERT (ProductID, ProductName, Price)\n         VALUES (TargetTbl.ProductID, TargetTbl.ProductName, TargetTbl.Price)\nWHEN NOT MATCHED BY SOURCE\n    THEN DELETE\nOUTPUT $action, INSERTED.*, DELETED.*;\nNote:Semicolon must be present in the end of MERGE statement.\nSection 16.4: MERGE using Derived Source Table\nMERGE INTO TargetTable  AS Target  \nUSING (VALUES (1,'Value1'), (2, 'Value2'), (3,'Value3'))  \n       AS Source (PKID, ColumnA)  \nON Target.PKID = Source.PKID\nWHEN MATCHED THEN\n    UPDATE SET target.ColumnA= source.ColumnA\nWHEN NOT MATCHED THEN\n    INSERT (PKID, ColumnA) VALUES (Source.PKID, Source.ColumnA);\nSection 16.5: Merge using EXCEPT\nUse EXCEPT to prevent updates to unchanged records\nMERGE TargetTable targ\nUSING SourceTable AS src\n    ON src.id = targ.id\nWHEN MATCHED\n    AND EXISTS (\n        SELECT src.field\n        EXCEPT\n        SELECT targ.field\n        )\n    THEN\n        UPDATE\n        SET field = src.field\nWHEN NOT MATCHED BY TARGET\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 51\n    THEN\n        INSERT (\n            id\n            ,field\n            )\n        VALUES (\n            src.id\n            ,src.field\n            )\nWHEN NOT MATCHED BY SOURCE\n    THEN\n        DELETE;\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 52\nChapter 17: CREATE VIEW\nSection 17.1: CREATE Indexed VIEW\nTo create a view with an index, the view must be created using the WITH SCHEMABINDING keywords:\nCREATE VIEW view_EmployeeInfo\nWITH SCHEMABINDING\nAS  \n    SELECT EmployeeID,\n        FirstName,\n        LastName,\n        HireDate  \n    FROM [dbo].Employee\nGO\nAny clustered or non-clustered indexes can be now be created:\nCREATE UNIQUE CLUSTERED INDEX IX_view_EmployeeInfo\nON view_EmployeeInfo\n(\n     EmployeeID ASC\n)\nThere Are some limitations to indexed Views:\nThe view deﬁnition can reference one or more tables in the same database.\nOnce the unique clustered index is created, additional nonclustered indexes can be created against the view.\nYou can update the data in the underlying tables – including inserts, updates, deletes, and even truncates.\nYou can’t modify the underlying tables and columns. The view is created with the WITH SCHEMABINDING\noption.\nIt can’t contain COUNT, MIN, MAX, TOP, outer joins, or a few other keywords or elements.\nFor more information about creating indexed Views you can read this MSDN article\nSection 17.2: CREATE VIEW\nCREATE VIEW view_EmployeeInfo\nAS  \n    SELECT EmployeeID,\n        FirstName,\n        LastName,\n        HireDate  \n    FROM Employee\nGO\nRows from views can be selected much like tables:\nSELECT FirstName\nFROM view_EmployeeInfo\nYou may also create a view with a calculated column. We can modify the view above as follows by adding a\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 53\ncalculated column:\nCREATE VIEW view_EmployeeReport\nAS  \n    SELECT EmployeeID,\n        FirstName,\n        LastName,\n        Coalesce(FirstName,'') + ' ' + Coalesce(LastName,'') as FullName,\n        HireDate  \n    FROM Employee\nGO\nThis view adds an additional column that will appear when you SELECT rows from it. The values in this additional\ncolumn will be dependent on the ﬁelds FirstName and LastName in the table Employee and will automatically\nupdate behind-the-scenes when those ﬁelds are updated.\nSection 17.3: CREATE VIEW With Encryption\nCREATE VIEW view_EmployeeInfo\nWITH ENCRYPTION\nAS  \nSELECT EmployeeID, FirstName, LastName, HireDate  \nFROM Employee\nGO\nSection 17.4: CREATE VIEW With INNER JOIN\nCREATE VIEW view_PersonEmployee\nAS  \n    SELECT P.LastName,\n        P.FirstName,\n        E.JobTitle\n    FROM Employee AS E\n    INNER JOIN Person AS P  \n        ON P.BusinessEntityID = E.BusinessEntityID\nGO\nViews can use joins to select data from numerous sources like tables, table functions, or even other views. This\nexample uses the FirstName and LastName columns from the Person table and the JobTitle column from the\nEmployee table.\nThis view can now be used to see all corresponding rows for Managers in the database:\nSELECT *\nFROM view_PersonEmployee\nWHERE JobTitle LIKE '%Manager%'\nSection 17.5: Grouped VIEWs\nA grouped VIEW is based on a query with a GROUP BY clause. Since each of the groups may have more than one\nrow in the base from which it was built, these are necessarily read-only VIEWs. Such VIEWs usually have one or\nmore aggregate functions and they are used for reporting purposes. They are also handy for working around\nweaknesses in SQL. Consider a VIEW that shows the largest sale in each state. The query is straightforward:\nhttps://www.simple-talk.com/sql/t-sql-programming/sql-view-beyond-the-basics/\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 54\nCREATE VIEW BigSales (state_code, sales_amt_total)\nAS SELECT state_code, MAX(sales_amt)\n     FROM Sales\n    GROUP BY state_code;\nSection 17.6: UNION-ed VIEWs\nVIEWs based on a UNION or UNION ALL operation are read-only because there is no single way to map a change\nonto just one row in one of the base tables. The UNION operator will remove duplicate rows from the results. Both\nthe UNION and UNION ALL operators hide which table the rows came from. Such VIEWs must use a , because the\ncolumns in a UNION [ALL] have no names of their own. In theory, a UNION of two disjoint tables, neither of which\nhas duplicate rows in itself should be updatable.\nhttps://www.simple-talk.com/sql/t-sql-programming/sql-view-beyond-the-basics/\nCREATE VIEW DepTally2 (emp_nbr, dependent_cnt)\nAS (SELECT emp_nbr, COUNT(*)\n      FROM Dependents\n     GROUP BY emp_nbr)\n   UNION\n   (SELECT emp_nbr, 0\n      FROM Personnel AS P2\n     WHERE NOT EXISTS\n          (SELECT *\n             FROM Dependents AS D2\n            WHERE D2.emp_nbr = P2.emp_nbr));\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 55\nChapter 18: Views\nSection 18.1: Create a view with schema binding\nIf a view is created WITH SCHEMABINDING, the underlying table(s) can't be dropped or modiﬁed in such a way that\nthey would break the view. For example, a table column referenced in a view can't be removed.\nCREATE VIEW dbo.PersonsView\nWITH SCHEMABINDING\nAS\nSELECT\n    name,\n    address\nFROM dbo.PERSONS  -- database schema must be specified when WITH SCHEMABINDING is present\nViews without schema binding can break if their underlying table(s) change or get dropped. Querying a broken view\nresults in an error message. sp_refreshview can be used to ensure existing views without schema binding aren't\nbroken.\nSection 18.2: Create a view\nCREATE VIEW dbo.PersonsView\nAS\nSELECT\n    name,\n    address\nFROM persons;\nSection 18.3: Create or replace view\nThis query will drop the view - if it already exists - and create a new one.\nIF OBJECT_ID('dbo.PersonsView', 'V') IS NOT NULL\n    DROP VIEW dbo.PersonsView\nGO\nCREATE VIEW dbo.PersonsView\nAS\nSELECT\n    name,\n    address\nFROM persons;\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 56\nChapter 19: UNION\nSection 19.1: Union and union all\nUnion operation combines the results of two or more queries into a single result set that includes all the rows that\nbelong to all queries in the union and will ignore any duplicates that exist. Union all also does the same thing but\ninclude even the duplicate values. The concept of union operation will be clear from the example below. Few things\nto consider while using union are:\n1.The number and the order of the columns must be the same in all queries.\n2.The data types must be compatible.\nExample:\nWe have three tables : Marksheet1, Marksheet2 and Marksheet3. Marksheet3 is the duplicate table of Marksheet2\nwhich contains same values as that of Marksheet2.\nTable1: Marksheet1\nTable2: Marksheet2\nTable3: Marksheet3\nUnion on tables Marksheet1 and Marksheet2\nSELECT SubjectCode, SubjectName, MarksObtained\nFROM Marksheet1\nUNION\nSELECT CourseCode, CourseName, MarksObtained\nFROM Marksheet2\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 57\nNote: The output for union of the three tables will also be same as union on Marksheet1 and Marksheet2 because\nunion operation does not take duplicate values.\nSELECT SubjectCode, SubjectName, MarksObtained\nFROM Marksheet1\nUNION\nSELECT CourseCode, CourseName, MarksObtained\nFROM Marksheet2  \nUNION\nSELECT SubjectCode, SubjectName, MarksObtained\nFROM Marksheet3\nOUTPUT\nUnion All\nSELECT SubjectCode, SubjectName, MarksObtained\nFROM Marksheet1\nUNION ALL\nSELECT CourseCode, CourseName, MarksObtained\nFROM Marksheet2\nUNION ALL\nSELECT SubjectCode, SubjectName, MarksObtained\nFROM Marksheet3\nOUTPUT\n\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 58\nYou will notice here that the duplicate values from Marksheet3 are also displayed using union all.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 59\nChapter 20: TRY/CATCH\nSection 20.1: Transaction in a TRY/CATCH\nThis will rollback both inserts due to an invalid datetime:\nBEGIN TRANSACTION\nBEGIN TRY\n    INSERT INTO dbo.Sale(Price, SaleDate, Quantity)\n    VALUES (5.2, GETDATE(), 1)\n    INSERT INTO dbo.Sale(Price, SaleDate, Quantity)\n    VALUES (5.2, 'not a date', 1)\n    COMMIT TRANSACTION\nEND TRY\nBEGIN CATCH        \n    ROLLBACK TRANSACTION -- First Rollback and then throw.\n    THROW\nEND CATCH\nThis will commit both inserts:\nBEGIN TRANSACTION\nBEGIN TRY\n    INSERT INTO dbo.Sale(Price, SaleDate, Quantity)\n    VALUES (5.2, GETDATE(), 1)\n    INSERT INTO dbo.Sale(Price, SaleDate, Quantity)\n    VALUES (5.2, GETDATE(), 1)\n    COMMIT TRANSACTION\nEND TRY\nBEGIN CATCH\n    THROW\n    ROLLBACK TRANSACTION\nEND CATCH\nSection 20.2: Raising errors in try-catch block\nRAISERROR function will generate error in the TRY CATCH block:\nDECLARE @msg nvarchar(50) = 'Here is a problem!'\nBEGIN TRY\n    print 'First statement';\n    RAISERROR(@msg, 11, 1);\n    print 'Second statement';\nEND TRY\nBEGIN CATCH\n    print 'Error: ' + ERROR_MESSAGE();\nEND CATCH\nRAISERROR with second parameter greater than 10 (11 in this example) will stop execution in TRY BLOCK and raise\nan error that will be handled in CATCH block. You can access error message using ERROR_MESSAGE() function.\nOutput of this sample is:\nFirst statement\nError: Here is a problem!\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 60\nSection 20.3: Raising info messages in try catch block\nRAISERROR with severity (second parameter) less or equal to 10 will not throw exception.\nBEGIN TRY\n    print 'First statement';\n    RAISERROR( 'Here is a problem!', 10, 15);\n    print 'Second statement';\nEND TRY\nBEGIN CATCH    \n    print 'Error: ' + ERROR_MESSAGE();\nEND CATCH\nAfter RAISERROR statement, third statement will be executed and CATCH block will not be invoked. Result of\nexecution is:\nFirst statement\nHere is a problem!\nSecond statement\nSection 20.4: Re-throwing exception generated by\nRAISERROR\nYou can re-throw error that you catch in CATCH block using TRHOW statement:\nDECLARE @msg nvarchar(50) = 'Here is a problem! Area: ''%s'' Line:''%i'''\nBEGIN TRY\n    print 'First statement';\n    RAISERROR(@msg, 11, 1, 'TRY BLOCK', 2);\n    print 'Second statement';\nEND TRY\nBEGIN CATCH\n    print 'Error: ' + ERROR_MESSAGE();\n    THROW;\nEND CATCH\nNote that in this case we are raising error with formatted arguments (fourth and ﬁfth parameter). This might be\nuseful if you want to add more info in message. Result of execution is:\nFirst statement\nError: Here is a problem! Area: 'TRY BLOCK' Line:'2'\nMsg 50000, Level 11, State 1, Line 26\nHere is a problem! Area: 'TRY BLOCK' Line:'2'\nSection 20.5: Throwing exception in TRY/CATCH blocks\nYou can throw exception in try catch block:\nDECLARE @msg nvarchar(50) = 'Here is a problem!'\nBEGIN TRY\n    print 'First statement';\n    THROW 51000, @msg, 15;\n    print 'Second statement';\nEND TRY\nBEGIN CATCH\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 61\n    print 'Error: ' + ERROR_MESSAGE();\n    THROW;\nEND CATCH\nException with be handled in CATCH block and then re-thrown using THROW without parameters.\nFirst statement\nError: Here is a problem!\nMsg 51000, Level 16, State 15, Line 39\nHere is a problem!\nTHROW is similar to RAISERROR with following diﬀerences:\nRecommendation is that new applications should use THROW instead of RASIERROR.\nTHROW can use any number as ﬁrst argument (error number), RAISERROR can use only ids in sys.messages\nview\nTHROW has severity 16 (cannot be changed)\nTHROW cannot format arguments like RAISERROR. Use FORMATMESSAGE function as an argument of\nRAISERROR if you need this feature.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 62\nChapter 21: WHILE loop\nSection 21.1: Using While loop\nThe WHILE loop can be used as an alternative to CURSORS. The following example will print numbers from 0 to 99.\n DECLARE @i int = 0;\n WHILE(@i < 100)\n BEGIN\n    PRINT @i;\n    SET @i = @i+1\n END\nSection 21.2: While loop with min aggregate function usage\nDECLARE @ID AS INT;\nSET @ID = (SELECT MIN(ID) from TABLE);\nWHILE @ID IS NOT NULL\nBEGIN\n    PRINT @ID;\n    SET @ID = (SELECT MIN(ID) FROM TABLE WHERE ID > @ID);\nEND\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 63\nChapter 22: OVER Clause\nParameter Details\nPARTITION BY The ﬁeld(s) that follows PARTITION BY is the one that the 'grouping' will be based on\nSection 22.1: Cumulative Sum\nUsing the Item Sales Table, we will try to ﬁnd out how the sales of our items are increasing through dates. To do so\nwe will calculate the Cumulative Sum of total sales per Item order by the sale date.\nSELECT item_id, sale_Date\n       SUM(quantity * price) OVER(PARTITION BY item_id ORDER BY sale_Date ROWS BETWEEN UNBOUNDED\nPRECEDING) AS SalesTotal\n  FROM SalesTable\nSection 22.2: Using Aggregation functions with OVER\nUsing the Cars Table, we will calculate the total, max, min and average amount of money each costumer spent and\nhaw many times (COUNT) she brought a car for repairing.\nId CustomerId MechanicId Model Status Total Cost\nSELECT CustomerId,  \n       SUM(TotalCost) OVER(PARTITION BY CustomerId) AS Total,\n       AVG(TotalCost) OVER(PARTITION BY CustomerId) AS Avg,\n       COUNT(TotalCost) OVER(PARTITION BY CustomerId) AS Count,\n       MIN(TotalCost) OVER(PARTITION BY CustomerId) AS Min,\n       MAX(TotalCost) OVER(PARTITION BY CustomerId) AS Max\n  FROM CarsTable\n WHERE Status = 'READY'\nBeware that using OVER in this fashion will not aggregate the rows returned. The above query will return the\nfollowing:\nCustomerId Total Avg Count Min Max\n1 430 215 2 200 230\n1 430 215 2 200 230\nThe duplicated row(s) may not be that useful for reporting purposes.\nIf you wish to simply aggregate data, you will be better oﬀ using the GROUP BY clause along with the appropriate\naggregate functions Eg:\nSELECT CustomerId,  \n       SUM(TotalCost) AS Total,\n       AVG(TotalCost) AS Avg,\n       COUNT(TotalCost) AS Count,\n       MIN(TotalCost) AS Min,\n       MAX(TotalCost)  AS Max\n  FROM CarsTable\n WHERE Status = 'READY'\nGROUP BY CustomerId\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 64\nSection 22.3: Dividing Data into equally-partitioned buckets\nusing NTILE\nLet's say that you have exam scores for several exams and you want to divide them into quartiles per exam.\n-- Setup data:\ndeclare @values table(Id int identity(1,1) primary key, [Value] float, ExamId int)\ninsert into @values ([Value], ExamId) values\n(65, 1), (40, 1), (99, 1), (100, 1), (90, 1), -- Exam 1 Scores\n(91, 2), (88, 2), (83, 2), (91, 2), (78, 2), (67, 2), (77, 2) -- Exam 2 Scores\n-- Separate into four buckets per exam:\nselect ExamId,\n       ntile(4) over (partition by ExamId order by [Value] desc) as Quartile,\n       Value, Id\nfrom @values\norder by ExamId, Quartile\nntile works great when you really need a set number of buckets and each ﬁlled to approximately the same level.\nNotice that it would be trivial to separate these scores into percentiles by simply using ntile(100).\nSection 22.4: Using Aggregation funtions to ﬁnd the most\nrecent records\nUsing the Library Database, we try to ﬁnd the last book added to the database for each author. For this simple\nexample we assume an always incrementing Id for each record added.\nSELECT MostRecentBook.Name, MostRecentBook.Title\nFROM ( SELECT Authors.Name,\n              Books.Title,\n              RANK() OVER (PARTITION BY Authors.Id ORDER BY Books.Id DESC) AS NewestRank\n       FROM Authors\n       JOIN Books ON Books.AuthorId = Authors.Id\n     ) MostRecentBook\nWHERE MostRecentBook.NewestRank = 1\nInstead of RANK, two other functions can be used to order. In the previous example the result will be the same, but\nthey give diﬀerent results when the ordering gives multiple rows for each rank.\nRANK(): duplicates get the same rank, the next rank takes the number of duplicates in the previous rank into\naccount\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 65\nDENSE_RANK(): duplicates get the same rank, the next rank is always one higher than the previous\nROW_NUMBER(): will give each row a unique 'rank', 'ranking' the duplicates randomly\nFor example, if the table had a non-unique column CreationDate and the ordering was done based on that, the\nfollowing query:\nSELECT Authors.Name,\n       Books.Title,\n       Books.CreationDate,\n       RANK() OVER (PARTITION BY Authors.Id ORDER BY Books.CreationDate DESC) AS RANK,\n       DENSE_RANK() OVER (PARTITION BY Authors.Id ORDER BY Books.CreationDate DESC) AS DENSE_RANK,\n       ROW_NUMBER() OVER (PARTITION BY Authors.Id ORDER BY Books.CreationDate DESC) AS ROW_NUMBER,\nFROM Authors\nJOIN Books ON Books.AuthorId = Authors.Id\nCould result in:\nAuthor Title CreationDate RANK DENSE_RANK ROW_NUMBER\nAuthor 1 Book 1 22/07/2016 1 1 1\nAuthor 1 Book 2 22/07/2016 1 1 2\nAuthor 1 Book 3 21/07/2016 3 2 3\nAuthor 1 Book 4 21/07/2016 3 2 4\nAuthor 1 Book 5 21/07/2016 3 2 5\nAuthor 1 Book 6 04/07/2016 6 3 6\nAuthor 2 Book 7 04/07/2016 1 1 1\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 66\nChapter 23: GROUP BY\nSection 23.1: Simple Grouping\nOrders Table\nCustomerId ProductId Quantity Price\n1 2 5 100\n1 3 2 200\n1 4 1 500\n2 1 4 50\n3 5 6 700\nWhen grouping by a speciﬁc column, only unique values of this column are returned.\nSELECT customerId\nFROM orders\nGROUP BY customerId;\nReturn value:\ncustomerId\n1\n2\n3\nAggregate functions like count() apply to each group and not to the complete table:\nSELECT customerId,\n       COUNT(productId) as numberOfProducts,\n       sum(price) as totalPrice\nFROM orders\nGROUP BY customerId;\nReturn value:\ncustomerId numberOfProducts totalPrice\n1 3 800\n2 1 50\n3 1 700\nSection 23.2: GROUP BY multiple columns\nOne might want to GROUP BY more than one column\ndeclare @temp table(age int, name varchar(15))\ninsert into @temp\nselect 18, 'matt' union all\nselect 21, 'matt' union all\nselect 21, 'matt' union all\nselect 18, 'luke' union all\nselect 18, 'luke' union all\nselect 21, 'luke' union all\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 67\nselect 18, 'luke' union all\nselect 21, 'luke'\nSELECT Age, Name, count(1) count\nFROM @temp\nGROUP BY Age, Name\nwill group by both age and name and will produce:\nAge Name count\n18 luke 3\n21 luke 2\n18 matt 1\n21 matt 2\nSection 23.3: GROUP BY with ROLLUP and CUBE\nThe ROLLUP operator is useful in generating reports that contain subtotals and totals.\nCUBE generates a result set that shows aggregates for all combinations of values in the selected columns.\nROLLUP generates a result set that shows aggregates for a hierarchy of values in the selected columns.\nItem Color Quantity\nTable Blue 124\nTable Red 223\nChair Blue 101\nChair Red 210\nSELECT CASE WHEN (GROUPING(Item) = 1) THEN 'ALL'\n            ELSE ISNULL(Item, 'UNKNOWN')\n       END AS Item,\n       CASE WHEN (GROUPING(Color) = 1) THEN 'ALL'\n            ELSE ISNULL(Color, 'UNKNOWN')\n       END AS Color,\n       SUM(Quantity) AS QtySum\nFROM Inventory\nGROUP BY Item, Color WITH ROLLUP\nItem                 Color                QtySum                    \n-------------------- -------------------- --------------------------\nChair                Blue                 101.00                    \nChair                Red                  210.00                    \nChair                ALL                  311.00                    \nTable                Blue                 124.00                    \nTable                Red                  223.00                    \nTable                ALL                  347.00                    \nALL                  ALL                  658.00\n(7 row(s) aﬀected)\nIf the ROLLUP keyword in the query is changed to CUBE, the CUBE result set is the same, except these two\nadditional rows are returned at the end:\nALL                  Blue                 225.00                    \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 68\nALL                  Red                  433.00\nhttps://technet.microsoft.com/en-us/library/ms189305(v=sql.90).aspx\nSection 23.4: Group by with multiple tables, multiple columns\nGroup by is often used with join statement. Let's assume we have two tables. The ﬁrst one is the table of students:\nId Full Name Age\n1 Matt Jones 20\n2 Frank Blue 21\n3 Anthony Angel 18\nSecond table is the table of subject each student can take:\nSubject_Id Subject\n1 Maths\n2 P.E.\n3 Physics\nAnd because one student can attend many subjects and one subject can be attended by many students (therefore\nN:N relationship) we need to have third \"bounding\" table. Let's call the table Students_subjects:\nSubject_Id Student_Id\n1 1\n2 2\n2 1\n3 2\n1 3\n1 1\nNow lets say we want to know the number of subjects each student is attending. Here the standalone GROUP BY\nstatement is not suﬃcient as the information is not available through single table. Therefore we need to use GROUP\nBY with the JOIN statement:\nSelect Students.FullName, COUNT(Subject Id) as SubjectNumber FROM Students_Subjects\nLEFT JOIN Students\nON Students_Subjects.Student_id = Students.Id\nGROUP BY Students.FullName\nThe result of the given query is as follows:\nFullName SubjectNumber\nMatt Jones 3\nFrank Blue 2\nAnthony Angel 1\nFor an even more complex example of GROUP BY usage, let's say student might be able to assign the same subject\nto his name more than once (as shown in table Students_Subjects). In this scenario we might be able to count\nnumber of times each subject was assigned to a student by GROUPing by more than one column:\nSELECT Students.FullName, Subjects.Subject,\nCOUNT(Students_subjects.Subject_id) AS NumberOfOrders\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 69\nFROM ((Students_Subjects\nINNER JOIN Students\nON Students_Subjcets.Student_id=Students.Id)\nINNER JOIN Subjects\nON Students_Subjects.Subject_id=Subjects.Subject_id)\nGROUP BY Fullname,Subject\nThis query gives the following result:\nFullName Subject SubjectNumber\nMatt Jones Maths 2\nMatt Jones P.E 1\nFrank Blue P.E 1\nFrank Blue Physics 1\nAnthony Angel Maths 1\nSection 23.5: HAVING\nBecause the WHERE clause is evaluated before GROUP BY, you cannot use WHERE to pare down results of the grouping\n(typically an aggregate function, such as COUNT(*)). To meet this need, the HAVING clause can be used.\nFor example, using the following data:\nDECLARE @orders TABLE(OrderID INT, Name NVARCHAR(100))\nINSERT INTO @orders VALUES\n( 1, 'Matt' ),\n( 2, 'John' ),\n( 3, 'Matt' ),\n( 4, 'Luke' ),\n( 5, 'John' ),\n( 6, 'Luke' ),\n( 7, 'John' ),\n( 8, 'John' ),\n( 9, 'Luke' ),\n( 10, 'John' ),\n( 11, 'Luke' )\nIf we want to get the number of orders each person has placed, we would use\nSELECT Name, COUNT(*) AS 'Orders'\nFROM @orders\nGROUP BY Name\nand get\nName Orders\nMatt 2\nJohn 5\nLuke 4\nHowever, if we want to limit this to individuals who have placed more than two orders, we can add a HAVING clause.\nSELECT Name, COUNT(*) AS 'Orders'\nFROM @orders\nGROUP BY Name\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 70\nHAVING COUNT(*) > 2\nwill yield\nName Orders\nJohn 5\nLuke 4\nNote that, much like GROUP BY, the columns put in HAVING must exactly match their counterparts in the SELECT\nstatement. If in the above example we had instead said\nSELECT Name, COUNT(DISTINCT OrderID)\nour HAVING clause would have to say\nHAVING COUNT(DISTINCT OrderID) > 2\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 71\nChapter 24: ORDER BY\nSection 24.1: Simple ORDER BY clause\nUsing the Employees Table, below is an example to return the Id, FName and LName columns in (ascending) LName\norder:\nSELECT Id, FName, LName FROM Employees\nORDER BY LName\nReturns:\nId FName LName\n2 John Johnson\n1 James Smith\n4 Johnathon Smith\n3 Michael Williams\nTo sort in descending order add the DESC keyword after the ﬁeld parameter, e.g. the same query in LName\ndescending order is:\nSELECT Id, FName, LName FROM Employees\nORDER BY LName DESC\nSection 24.2: ORDER BY multiple ﬁelds\nMultiple ﬁelds can be speciﬁed for the ORDER BY clause, in either ASCending or DESCending order.\nFor example, using the\nhttp://stackoverﬂow.com/documentation/sql/280/example-databases/1207/item-sales-table#t=2016072113140664\n34211 table, we can return a query that sorts by SaleDate in ascending order, and Quantity in descending order.\nSELECT ItemId, SaleDate, Quantity\nFROM [Item Sales]\nORDER BY SaleDate ASC, Quantity DESC\nNote that the ASC keyword is optional, and results are sorted in ascending order of a given ﬁeld by default.\nSection 24.3: Custom Ordering\nIf you want to order by a column using something other than alphabetical/numeric ordering, you can use case to\nspecify the order you want.\norder by Group returns:\nGroup Count\nNot Retired 6\nRetired 4\nTotal 10\norder by case group when 'Total' then 1 when 'Retired' then 2 else 3 end returns:\nGroup Count\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 72\nTotal 10\nRetired 4\nNot Retired 6\nSection 24.4: ORDER BY with complex logic\nIf we want to order the data diﬀerently for per group, we can add a CASE syntax to the ORDER BY. In this example,\nwe want to order employees from Department 1 by last name and employees from Department 2 by salary.\nId FName LName PhoneNumber ManagerId DepartmentId Salary HireDate\n1 James Smith 1234567890 NULL 1 1000 01-01-2002\n2 John Johnson 2468101214 1 1 400 23-03-2005\n3 Michael Williams 1357911131 1 2 600 12-05-2009\n4 Johnathon Smith 1212121212 2 1 500 24-07-2016\n5 Sam Saxon 1372141312 2 2 400 25-03-2015\nThe following query will provide the required results:\nSELECT Id, FName, LName, Salary FROM Employees\nORDER BY Case When DepartmentId = 1 then LName else Salary end\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 73\nChapter 25: The STUFF Function\nParameter Details\ncharacter_expression the existing string in your data\nstart_position the position in character_expression to delete length and then insert the\nreplacement_string\nlength the number of characters to delete from character_expression\nreplacement_string the sequence of characters to insert in character_expression\nSection 25.1: Using FOR XML to Concatenate Values from\nMultiple Rows\nOne common use for the FOR XML function is to concatenate the values of multiple rows.\nHere's an example using the Customers table:\nSELECT\n    STUFF( (SELECT ';' + Email\n        FROM Customers\n        where (Email is not null and Email <> '')\n        ORDER BY Email ASC\n        FOR XML PATH('')),\n    1, 1, '')\nIn the example above, FOR XML PATH('')) is being used to concatenate email addresses, using ; as the delimiter\ncharacter. Also, the purpose of STUFF is to remove the leading ; from the concatenated string. STUFF is also\nimplicitly casting the concatenated string from XML to varchar.\nNote: the result from the above example will be XML-encoded, meaning it will replace < characters with &lt; etc. If\nyou don't want this, change FOR XML PATH('')) to FOR XML PATH, TYPE).value('.[1]','varchar(MAX)'), e.g.:\nSELECT\n    STUFF( (SELECT ';' + Email\n        FROM Customers\n        where (Email is not null and Email <> '')\n        ORDER BY Email ASC\n        FOR XML PATH, TYPE).value('.[1]','varchar(900)'),\n    1, 1, '')\nThis can be used to achieve a result similar to GROUP_CONCAT in MySQL or string_agg in PostgreSQL 9.0+, although\nwe use subqueries instead of GROUP BY aggregates. (As an alternative, you can install a user-deﬁned aggregate\nsuch as this one if you're looking for functionality closer to that of GROUP_CONCAT).\nSection 25.2: Basic Character Replacement with STUFF()\nThe STUFF() function inserts a string into another string by ﬁrst deleting a speciﬁed number of characters. The\nfollowing example, deletes \"Svr\" and replaces it with \"Server\". This happens by specifying the start_position and\nlength of the replacement.\nSELECT STUFF('SQL Svr Documentation', 5, 3, 'Server')\nExecuting this example will result in returning SQL Server Documentation instead of SQL Svr Documentation.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 74\nSection 25.3: Basic Example of STUFF() function\nSTUFF(Original_Expression, Start, Length, Replacement_expression)\nSTUFF() function inserts Replacement_expression, at the start position speciﬁed, along with removing the\ncharacters speciﬁed using Length parameter.\n Select FirstName, LastName,Email, STUFF(Email, 2, 3, '*****') as StuffedEmail From Employee\nExecuting this example will result in returning the given table\nFirstName LastName Email StuﬀedEmail\nJomes Hunter James@hotmail.com J*****s@hotmail.com\nShyam rathod Shyam@hotmail.com S*****m@hotmail.com\nRam shinde Ram@hotmail.com R*****hotmail.com\nSection 25.4: stu for comma separated in sql server\nFOR XML PATH and STUFF to concatenate the multiple rows into a single row:\n  select distinct t1.id,\n      STUFF(\n             (SELECT ', ' + convert(varchar(10), t2.date, 120)\n              FROM yourtable t2\n              where t1.id = t2.id\n              FOR XML PATH (''))\n              , 1, 1, '')  AS date\n    from yourtable t1;\nSection 25.5: Obtain column names separated with comma\n(not a list)\n/*\nThe result can be use for fast way to use columns on Insertion/Updates.\nWorks with tables and views.\nExample: eTableColumns  'Customers'\nColumnNames\n------------------------------------------------------\nId, FName, LName, Email, PhoneNumber, PreferredContact\nINSERT INTO Customers (Id, FName, LName, Email, PhoneNumber, PreferredContact)\n    VALUES (5, 'Ringo', 'Star', 'two@beatles.now', NULL, 'EMAIL')\n*/\nCREATE PROCEDURE eTableColumns (@Table VARCHAR(100))\nAS\nSELECT ColumnNames =\n   STUFF( (SELECT ', ' +  c.name\nFROM    \n    sys.columns c\nINNER JOIN\n    sys.types t ON c.user_type_id = t.user_type_id\nWHERE\n    c.object_id = OBJECT_ID( @Table)\n        FOR XML PATH, TYPE).value('.[1]','varchar(2000)'),\n    1, 1, '')\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 75\nGO\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 76\nChapter 26: JSON in SQL Server\nParameters Details\nexpression Typically the name of a variable or a column that contains JSON text.\npath A JSON path expression that speciﬁes the property to update. path has the following syntax:\n[append] [ lax | strict ] $.<json path>\njsonExpression Is a Unicode character expression containing the JSON text.\nSection 26.1: Index on JSON properties by using computed\ncolumns\nWhen storing JSON documents in SQL Server, We need to be able to eﬃciently ﬁlter and sort query results on\nproperties of the JSON documents.\nCREATE TABLE JsonTable\n(\n    id int identity primary key,\n    jsonInfo nvarchar(max),\n    CONSTRAINT [Content should be formatted as JSON]\n    CHECK (ISJSON(jsonInfo)>0)\n)\nINSERT INTO JsonTable\nVALUES(N'{\"Name\":\"John\",\"Age\":23}'),\n(N'{\"Name\":\"Jane\",\"Age\":31}'),\n(N'{\"Name\":\"Bob\",\"Age\":37}'),\n(N'{\"Name\":\"Adam\",\"Age\":65}')\nGO\nGiven the above table If we want to ﬁnd the row with the name = 'Adam', we would execute the following query.\nSELECT *\nFROM JsonTable Where\nJSON_VALUE(jsonInfo, '$.Name') = 'Adam'\nHowever this will require SQL server to perform a full table which on a large table is not eﬃcent.\nTo speed this up we would like to add an index, however we cannot directly reference properties in the JSON\ndocument. The solution is to add a computed column on the JSON path $.Name, then add an index on the\ncomputed column.\nALTER TABLE JsonTable\nADD vName as JSON_VALUE(jsonInfo, '$.Name')\nCREATE INDEX idx_name\nON JsonTable(vName)\nNow when we execute the same query, instead of a full table scan SQL server uses an index to seek into the non-\nclustered index and ﬁnd the rows that satisfy the speciﬁed conditions.\nNote: For SQL server to use the index, you must create the computed column with the same expression that you\nplan to use in your queries - in this example JSON_VALUE(jsonInfo, '$.Name'), however you can also use the\nname of computed column vName\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 77\nSection 26.2: Join parent and child JSON entities using CROSS\nAPPLY OPENJSON\nJoin parent objects with their child entities, for example we want a relational table of each person and their hobbies\nDECLARE @json nvarchar(1000) =\nN'[\n    {\n        \"id\":1,\n        \"user\":{\"name\":\"John\"},\n        \"hobbies\":[\n            {\"name\": \"Reading\"},\n            {\"name\": \"Surfing\"}\n        ]\n    },\n    {\n        \"id\":2,\n        \"user\":{\"name\":\"Jane\"},\n        \"hobbies\":[\n            {\"name\": \"Programming\"},\n            {\"name\": \"Running\"}\n        ]\n    }\n ]'\nQuery\nSELECT\n    JSON_VALUE(person.value, '$.id') as Id,\n    JSON_VALUE(person.value, '$.user.name') as PersonName,\n    JSON_VALUE(hobbies.value, '$.name') as Hobby\nFROM OPENJSON (@json) as person\n    CROSS APPLY OPENJSON(person.value, '$.hobbies') as hobbies\nAlternatively this query can be written using the WITH clause.\nSELECT\n    Id, person.PersonName, Hobby\nFROM OPENJSON (@json)\nWITH(\n    Id int '$.id',\n    PersonName nvarchar(100) '$.user.name',\n    Hobbies nvarchar(max) '$.hobbies' AS JSON\n) as person\nCROSS APPLY OPENJSON(Hobbies)\nWITH(\n    Hobby nvarchar(100) '$.name'\n)\nResult\nId PersonName Hobby\n1 John Reading\n1 John Surﬁng\n2 Jane Programming\n2 Jane Running\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 78\nSection 26.3: Format Query Results as JSON with FOR JSON\nInput table data (People table)\nId Name Age\n1 John 23\n2 Jane 31\nQuery\nSELECT Id, Name, Age\nFROM People\nFOR JSON PATH\nResult\n[\n    {\"Id\":1,\"Name\":\"John\",\"Age\":23},\n    {\"Id\":2,\"Name\":\"Jane\",\"Age\":31}\n]\nSection 26.4: Parse JSON text\nJSON_VALUE and JSON_QUERY functions parse JSON text and return scalar values or objects/arrays on the path in\nJSON text.\nDECLARE @json NVARCHAR(100) = '{\"id\": 1, \"user\":{\"name\":\"John\"}, \"skills\":[\"C#\",\"SQL\"]}'\nSELECT\n    JSON_VALUE(@json, '$.id') AS Id,\n    JSON_VALUE(@json, '$.user.name') AS Name,\n    JSON_QUERY(@json, '$.user') AS UserObject,\n    JSON_QUERY(@json, '$.skills') AS Skills,\n    JSON_VALUE(@json, '$.skills[0]') AS Skill0\nResult\nId Name UserObject Skills Skill0\n1 John {\"name\":\"John\"} [\"C#\",\"SQL\"] C#\nSection 26.5: Format one table row as a single JSON object\nusing FOR JSON\nWITHOUT_ARRAY_WRAPPER option in FOR JSON clause will remove array brackets from the JSON output. This is\nuseful if you are returning single row in the query.\nNote: this option will produce invalid JSON output if more than one row is returned.\nInput table data (People table)\nId Name Age\n1 John 23\n2 Jane 31\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 79\nQuery\nSELECT Id, Name, Age\nFROM People\nWHERE Id = 1\nFOR JSON PATH, WITHOUT_ARRAY_WRAPPER\nResult\n{\"Id\":1,\"Name\":\"John\",\"Age\":23}\nSection 26.6: Parse JSON text using OPENJSON function\nOPENJSON function parses JSON text and returns multiple outputs. Values that should be returned are speciﬁed\nusing the paths deﬁned in the WITH clause. If a path is not speciﬁed for some column, the column name is used as\na path. This function casts returned values to the SQL types deﬁned in the WITH clause. AS JSON option must be\nspeciﬁed in the column deﬁnition if some object/array should be returned.\nDECLARE @json NVARCHAR(100) = '{\"id\": 1, \"user\":{\"name\":\"John\"}, \"skills\":[\"C#\",\"SQL\"]}'\nSELECT *\nFROM OPENJSON (@json)\n    WITH(Id int '$.id',\n        Name nvarchar(100) '$.user.name',\n        UserObject nvarchar(max) '$.user' AS JSON,\n        Skills nvarchar(max) '$.skills' AS JSON,\n        Skill0 nvarchar(20) '$.skills[0]')\nResult\nId Name UserObject Skills Skill0\n1 John {\"name\":\"John\"} [\"C#\",\"SQL\"] C#\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 80\nChapter 27: OPENJSON\nSection 27.1: Transform JSON array into set of rows\nOPENJSON function parses collection of JSON objects and returns values from JSON text as set of rows.\ndeclare @json nvarchar(4000) = N'[\n  {\"Number\":\"SO43659\",\"Date\":\"2011-05-31T00:00:00\",\"Customer\": \"MSFT\",\"Price\":59.99,\"Quantity\":1},\n  {\"Number\":\"SO43661\",\"Date\":\"2011-06-01T00:00:00\",\"Customer\":\"Nokia\",\"Price\":24.99,\"Quantity\":3}\n]'\nSELECT    *\nFROM OPENJSON (@json)\n    WITH (\n          Number   varchar(200),\n          Date     datetime,\n          Customer varchar(200),\n          Quantity int\n  )\nIn the WITH clause is speciﬁed return schema of OPENJSON function. Keys in the JSON objects are fetched by\ncolumn names. If some key in JSON is not speciﬁed in the WITH clause (e.g. Price in this example) it will be ignored.\nValues are automatically converted into speciﬁed types.\nNumber Date Customer Quantity\nSO43659 2011-05-31T00:00:00 MSFT 1\nSO43661 2011-06-01T00:00:00 Nokia 3\nSection 27.2: Get key:value pairs from JSON text\nOPENJSON function parse JSON text and returns all key:value pairs at the ﬁrst level of JSON:\ndeclare @json NVARCHAR(4000) = N'{\"Name\":\"Joe\",\"age\":27,\"skills\":[\"C#\",\"SQL\"]}';\nSELECT * FROM OPENJSON(@json);\nkey value type\nName Joe 1\nage 27 2\nskills [\"C#\",\"SQL\"] 4\nColumn type describe the type of value, i.e. null(0), string(1), number(2), boolean(3), array(4), and object(5).\nSection 27.3: Transform nested JSON ﬁelds into set of rows\nOPENJSON function parses collection of JSON objects and returns values from JSON text as set of rows. If the values\nin input object are nested, additional mapping parameter can be speciﬁed in each column in WITH clause:\ndeclare @json nvarchar(4000) = N'[\n \n{\"data\":{\"num\":\"SO43659\",\"date\":\"2011-05-31T00:00:00\"},\"info\":{\"customer\":\"MSFT\",\"Price\":59.99,\"qty\n\":1}},\n \n{\"data\":{\"number\":\"SO43661\",\"date\":\"2011-06-01T00:00:00\"},\"info\":{\"customer\":\"Nokia\",\"Price\":24.99,\n\"qty\":3}}\n]'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 81\nSELECT    *\nFROM OPENJSON (@json)\n    WITH (\n          Number   varchar(200) '$.data.num',\n          Date     datetime '$.data.date',\n          Customer varchar(200) '$.info.customer',\n          Quantity int '$.info.qty',\n  )\nIn the WITH clause is speciﬁed return schema of OPENJSON function. After the type is speciﬁed path to the JSON\nnodes where returned value should be found. Keys in the JSON objects are fetched by these paths. Values are\nautomatically converted into speciﬁed types.\nNumber Date Customer Quantity\nSO43659 2011-05-31T00:00:00 MSFT 1\nSO43661 2011-06-01T00:00:00 Nokia 3\nSection 27.4: Extracting inner JSON sub-objects\nOPENJSON can extract fragments of JSON objects inside the JSON text. In the column deﬁnition that references\nJSON sub-object set the type nvarchar(max) and AS JSON option:\ndeclare @json nvarchar(4000) = N'[\n \n{\"Number\":\"SO43659\",\"Date\":\"2011-05-31T00:00:00\",\"info\":{\"customer\":\"MSFT\",\"Price\":59.99,\"qty\":1}},\n \n{\"Number\":\"SO43661\",\"Date\":\"2011-06-01T00:00:00\",\"info\":{\"customer\":\"Nokia\",\"Price\":24.99,\"qty\":3}}\n]'\nSELECT    *\nFROM OPENJSON (@json)\n    WITH (\n          Number   varchar(200),\n          Date     datetime,\n          Info nvarchar(max) '$.info' AS JSON\n  )\nInfo column will be mapped to \"Info\" object. Results will be:\nNumber Date Info\nSO43659 2011-05-31T00:00:00 {\"customer\":\"MSFT\",\"Price\":59.99,\"qty\":1}\nSO43661 2011-06-01T00:00:00 {\"customer\":\"Nokia\",\"Price\":24.99,\"qty\":3}\nSection 27.5: Working with nested JSON sub-arrays\nJSON may have complex structure with inner arrays. In this example, we have array of orders with nested sub array\nof OrderItems.\ndeclare @json nvarchar(4000) = N'[\n  {\"Number\":\"SO43659\",\"Date\":\"2011-05-31T00:00:00\",\n    \"Items\":[{\"Price\":11.99,\"Quantity\":1},{\"Price\":12.99,\"Quantity\":5}]},\n  {\"Number\":\"SO43661\",\"Date\":\"2011-06-01T00:00:00\",\n   \n\"Items\":[{\"Price\":21.99,\"Quantity\":3},{\"Price\":22.99,\"Quantity\":2},{\"Price\":23.99,\"Quantity\":2}]}\n]'\nWe can parse root level properties using OPENJSON that will return Items array AS JSON fragment. Then we can\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 82\napply OPENJSON again on Items array and open inner JSON table. First level table and inner table will be \"joined\"\nlike in the JOIN between standard tables:\nSELECT    *\nFROM\n    OPENJSON (@json)\n    WITH (  Number varchar(200), Date datetime,\n            Items nvarchar(max) AS JSON )\n        CROSS APPLY\n            OPENJSON (Items)\n            WITH ( Price float, Quantity int)\nResults:\nNumber Date Items Price Quantity\nSO43659 2011-05-31\n00:00:00.000 [{\"Price\":11.99,\"Quantity\":1},{\"Price\":12.99,\"Quantity\":5}] 11.99 1\nSO43659 2011-05-31\n00:00:00.000 [{\"Price\":11.99,\"Quantity\":1},{\"Price\":12.99,\"Quantity\":5}] 12.99 5\nSO43661 2011-06-01\n00:00:00.000 [{\"Price\":21.99,\"Quantity\":3},{\"Price\":22.99,\"Quantity\":2},{\"Price\":23.99,\"Quantity\":2}] 21.99 3\nSO43661 2011-06-01\n00:00:00.000 [{\"Price\":21.99,\"Quantity\":3},{\"Price\":22.99,\"Quantity\":2},{\"Price\":23.99,\"Quantity\":2}] 22.99 2\nSO43661 2011-06-01\n00:00:00.000 [{\"Price\":21.99,\"Quantity\":3},{\"Price\":22.99,\"Quantity\":2},{\"Price\":23.99,\"Quantity\":2}] 23.99 2\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 83\nChapter 28: FOR JSON\nSection 28.1: FOR JSON PATH\nFormats results of SELECT query as JSON text. FOR JSON PATH clause is added after query:\nSELECT top 3 object_id, name, type, principal_id FROM sys.objects\nFOR JSON PATH\nColumn names will be used as keys in JSON, and cell values will be generated as JSON values. Result of the query\nwould be an array of JSON objects:\n[\n  {\"object_id\":3,\"name\":\"sysrscols\",\"type\":\"S \"},      \n  {\"object_id\":5,\"name\":\"sysrowsets\",\"type\":\"S \"},\n  {\"object_id\":6,\"name\":\"sysclones\",\"type\":\"S \"}\n]\nNULL values in principal_id column will be ignored (they will not be generated).\nSection 28.2: FOR JSON PATH with column aliases\nFOR JSON PATH enables you to control format of the output JSON using column aliases:\nSELECT top 3 object_id as id, name as [data.name], type as [data.type]\nFROM sys.objects\nFOR JSON PATH\nColumn alias will be used as a key name. Dot-separated column aliases (data.name and data.type) will be generated\nas nested objects. If two column have the same preﬁx in dot notation, they will be grouped together in single object\n(data in this example):\n[\n  {\"id\":3,\"data\":{\"name\":\"sysrscols\",\"type\":\"S \"}},\n  {\"id\":5,\"data\":{\"name\":\"sysrowsets\",\"type\":\"S \"}},\n  {\"id\":6,\"data\":{\"name\":\"sysclones\",\"type\":\"S \"}}\n]\nSection 28.3: FOR JSON clause without array wrapper (single\nobject in output)\nWITHOUT_ARRAY_WRAPPER option enables you to generate a single object instead of the array. Use this option if\nyou know that you will return single row/object:\nSELECT top 3 object_id, name, type, principal_id\nFROM sys.objects\nWHERE object_id = 3\nFOR JSON PATH, WITHOUT_ARRAY_WRAPPER\nSingle object will be returned in this case:\n{\"object_id\":3,\"name\":\"sysrscols\",\"type\":\"S \"}\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 84\nSection 28.4: INCLUDE_NULL_VALUES\nFOR JSON clause ignores NULL values in cells. If you want to generate \"key\": null pairs for cells that contain NULL\nvalues, add INCLUDE_NULL_VALUES option in the query:\nSELECT top 3 object_id, name, type, principal_id\nFROM sys.objects\nFOR JSON PATH, INCLUDE_NULL_VALUES\nNULL values in principal_id column will be generated:\n[\n  {\"object_id\":3,\"name\":\"sysrscols\",\"type\":\"S \",\"principal_id\":null},\n  {\"object_id\":5,\"name\":\"sysrowsets\",\"type\":\"S \",\"principal_id\":null},\n  {\"object_id\":6,\"name\":\"sysclones\",\"type\":\"S \",\"principal_id\":null}\n]\nSection 28.5: Wrapping results with ROOT object\nWraps returned JSON array in additional root object with speciﬁed key:\nSELECT top 3 object_id, name, type FROM sys.objects\nFOR JSON PATH, ROOT('data')\nResult of the query would be array of JSON objects inside the wrapper object:\n{\n  \"data\":[\n           {\"object_id\":3,\"name\":\"sysrscols\",\"type\":\"S \"},\n           {\"object_id\":5,\"name\":\"sysrowsets\",\"type\":\"S \"},\n           {\"object_id\":6,\"name\":\"sysclones\",\"type\":\"S \"}\n         ]\n}\nSection 28.6: FOR JSON AUTO\nAutomatically nests values from the second table as a nested sub-array of JSON objects:\nSELECT top 5 o.object_id, o.name, c.column_id, c.name\nFROM sys.objects o\n    JOIN sys.columns c ON o.object_id = c.object_id\nFOR JSON AUTO\nResult of the query would be array of JSON objects:\n[\n  {\n   \"object_id\":3,\n   \"name\":\"sysrscols\",\n   \"c\":[\n        {\"column_id\":12,\"name\":\"bitpos\"},\n        {\"column_id\":6,\"name\":\"cid\"}\n       ]\n  },\n  {\n    \"object_id\":5,\n    \"name\":\"sysrowsets\",\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 85\n    \"c\":[\n         {\"column_id\":13,\"name\":\"colguid\"},\n         {\"column_id\":3,\"name\":\"hbcolid\"},\n         {\"column_id\":8,\"name\":\"maxinrowlen\"}\n     ]\n  }\n]\nSection 28.7: Creating custom nested JSON structure\nIf you need some complex JSON structure that cannot be created using FOR JSON PATH or FOR JSON AUTO, you can\ncustomize your JSON output by putting FOR JSON sub-queries as column expressions:\nSELECT top 5 o.object_id, o.name,\n        (SELECT column_id, c.name\n            FROM sys.columns c WHERE o.object_id = c.object_id\n            FOR JSON PATH) as columns,\n        (SELECT parameter_id, name\n            FROM sys.parameters p WHERE o.object_id = p.object_id\n            FOR JSON PATH) as parameters\nFROM sys.objects o\nFOR JSON PATH\nEach sub-query will produce JSON result that will be included in the main JSON content.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 86\nChapter 29: Queries with JSON data\nSection 29.1: Using values from JSON in query\nJSON_VALUE function enables you to take a data from JSON text on the path speciﬁed as the second argument, and\nuse this value in any part of the select query:\nselect ProductID, Name, Color, Size, Price, JSON_VALUE(Data, '$.Type') as Type\nfrom Product\nwhere JSON_VALUE(Data, '$.Type') = 'part'\nSection 29.2: Using JSON values in reports\nOnce JSON values are extracted from JSON text, you can use them ina any part of the query. You can create some\nkind of report on JSON data with grouping aggregations, etc:\nselect JSON_VALUE(Data, '$.Type') as type,\n        AVG( cast(JSON_VALUE(Data, '$.ManufacturingCost') as float) ) as cost\nfrom Product\ngroup by JSON_VALUE(Data, '$.Type')\nhaving JSON_VALUE(Data, '$.Type') is not null\nSection 29.3: Filter-out bad JSON text from query results\nIf some JSON text might not be properly formatted, you can remove those entries from query using ISJSON\nfunction.\nselect ProductID, Name, Color, Size, Price, JSON_VALUE(Data, '$.Type') as Type\nfrom Product\nwhere JSON_VALUE(Data, '$.Type') = 'part'\nand ISJSON(Data) > 0\nSection 29.4: Update value in JSON column\nJSON_MODIFY function can be used to update value on some path. You can use this function to modify original\nvalue of JSON cell in UPDATE statement:\nupdate Product\nset Data = JSON_MODIFY(Data, '$.Price', 24.99)\nwhere ProductID = 17;\nJSON_MODIFY function will update or create Price key (if it does not exists). If new value is NULL, the key will be\nremoved. JSON_MODIFY function will treat new value as string (escape special characters, wrap it with double\nquotes to create proper JSON string). If your new value is JSON fragment, you should wrap it with JSON_QUERY\nfunction:\nupdate Product\nset Data = JSON_MODIFY(Data, '$.tags', JSON_QUERY('[\"promo\",\"new\"]'))\nwhere ProductID = 17;\nJSON_QUERY function without second parameter behaves like a \"cast to JSON\". Since the result of JSON_QUERY is\nvalid JSON fragment (object or array), JSON_MODIFY will no escape this value when modiﬁes input JSON.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 87\nSection 29.5: Append new value into JSON array\nJSON_MODIFY function can be used to append new value to some array inside JSON:\nupdate Product\nset Data = JSON_MODIFY(Data, 'append $.tags', \"sales\")\nwhere ProductID = 17;\nNew value will be appended at the end of the array, or a new array with value [\"sales\"] will be created.\nJSON_MODIFY function will treat new value as string (escape special characters, wrap it with double quotes to\ncreate proper JSON string). If your new value is JSON fragment, you should wrap it with JSON_QUERY function:\nupdate Product\nset Data = JSON_MODIFY(Data, 'append $.tags', JSON_QUERY('{\"type\":\"new\"}'))\nwhere ProductID = 17;\nJSON_QUERY function without second parameter behaves like a \"cast to JSON\". Since the result of JSON_QUERY is\nvalid JSON fragment (object or array), JSON_MODIFY will no escape this value when modiﬁes input JSON.\nSection 29.6: JOIN table with inner JSON collection\nIf you have a \"child table\" formatted as JSON collection and stored in-row as JSON column, you can unpack this\ncollection, transform it to table and join it with parent row. Instead of the standard JOIN operator, you should use\nCROSS APPLY. In this example, product parts are formatted as collection of JSON objects in and stored in Data\ncolumn:\nselect ProductID, Name, Size, Price, Quantity, PartName, Code\nfrom Product\n    CROSS APPLY OPENJSON(Data, '$.Parts') WITH (PartName varchar(20), Code varchar(5))\nResult of the query is equivalent to the join between Product and Part tables.\nSection 29.7: Finding rows that contain value in the JSON\narray\nIn this example, Tags array may contain various keywords like [\"promo\", \"sales\"], so we can open this array and\nﬁlter values:\nselect ProductID, Name, Color, Size, Price, Quantity\nfrom Product\n    CROSS APPLY OPENJSON(Data, '$.Tags')\nwhere value = 'sales'\nOPENJSON will open inner collection of tags and return it as table. Then we can ﬁlter results by some value in the\ntable.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 88\nChapter 30: Storing JSON in SQL tables\nSection 30.1: JSON stored as text column\nJSON is textual format, so it is stored in standard NVARCHAR columns. NoSQL collection is equivalent to two column\nkey value table:\nCREATE TABLE ProductCollection (\n  Id int identity primary key,\n  Data nvarchar(max)\n)\nUse nvarchar(max) as you are not sure what would be the size of your JSON documents. nvarchar(4000) and\nvarchar(8000) have better performance but with size limit to 8KB.\nSection 30.2: Ensure that JSON is properly formatted using\nISJSON\nSince JSON is stored textual column, you might want to ensure that it is properly formatted. You can add CHECK\nconstraint on JSON column that checks is text properly formatted JSON:\nCREATE TABLE ProductCollection (\n  Id int identity primary key,\n  Data nvarchar(max)\n       CONSTRAINT [Data should be formatted as JSON]\n       CHECK (ISJSON(Data) > 0)\n)\nIf you already have a table, you can add check constraint using the ALTER TABLE statement:\nALTER TABLE ProductCollection\n    ADD CONSTRAINT [Data should be formatted as JSON]\n        CHECK (ISJSON(Data) > 0)\nSection 30.3: Expose values from JSON text as computed\ncolumns\nYou can expose values from JSON column as computed columns:\nCREATE TABLE ProductCollection (\n  Id int identity primary key,\n  Data nvarchar(max),\n  Price AS JSON_VALUE(Data, '$.Price'),\n  Color JSON_VALUE(Data, '$.Color') PERSISTED\n)\nIf you add PERSISTED computed column, value from JSON text will be materialized in this column. This way your\nqueries can faster read value from JSON text because no parsing is needed. Each time JSON in this row changes,\nvalue will be re-calculated.\nSection 30.4: Adding index on JSON path\nQueries that ﬁlter or sort data by some value in JSON column usually use full table scan.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 89\nSELECT * FROM ProductCollection\nWHERE JSON_VALUE(Data, '$.Color') = 'Black'\nTo optimize these kind of queries, you can add non-persisted computed column that exposes JSON expression\nused in ﬁlter or sort (in this example JSON_VALUE(Data, '$.Color')), and create index on this column:\nALTER TABLE ProductCollection\nADD vColor as JSON_VALUE(Data, '$.Color')\nCREATE INDEX idx_JsonColor\nON ProductCollection(vColor)\nQueries will use the index instead of plain table scan.\nSection 30.5: JSON stored in in-memory tables\nIf you can use memory-optimized tables, you can store JSON as text:\nCREATE TABLE ProductCollection (\n  Id int identity primary key nonclustered,\n  Data nvarchar(max)\n) WITH (MEMORY_OPTIMIZED=ON)\nAdvantages of JSON in in-memory:\nJSON data is always in memory so there is no disk access\nThere are no locks and latches while working with JSON\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 90\nChapter 31: Modify JSON text\nSection 31.1: Modify value in JSON text on the speciﬁed path\nJSON_MODIFY function uses JSON text as input parameter, and modiﬁes a value on the speciﬁed path using third\nargument:\ndeclare @json nvarchar(4000) = N'{\"Id\":1,\"Name\":\"Toy Car\",\"Price\":34.99}'\nset @json = JSON_MODIFY(@json, '$.Price', 39.99)\nprint @json -- Output: {\"Id\":1,\"Name\":\"Toy Car\",\"Price\":39.99}\nAs a result, we will have new JSON text with \"Price\":39.99 and other value will not be changed. If object on the\nspeciﬁed path does not exists, JSON_MODIFY will insert key:value pair.\nIn order to delete key:value pair, put NULL as new value:\ndeclare @json nvarchar(4000) = N'{\"Id\":1,\"Name\":\"Toy Car\",\"Price\":34.99}'\nset @json = JSON_MODIFY(@json, '$.Price', NULL)\nprint @json -- Output: {\"Id\":1,\"Name\":\"Toy Car\"}\nJSON_MODIFY will by default delete key if it does not have value so you can use it to delete a key.\nSection 31.2: Append a scalar value into a JSON array\nJSON_MODIFY has 'append' mode that appends value into array.\ndeclare @json nvarchar(4000) = N'{\"Id\":1,\"Name\":\"Toy Car\",\"Tags\":[\"toy\",\"game\"]}'\nset @json = JSON_MODIFY(@json, 'append $.Tags', 'sales')\nprint @json -- Output: {\"Id\":1,\"Name\":\"Toy Car\",\"Tags\":[\"toy\",\"game\",\"sales\"]}\nIf array on the speciﬁed path does not exists, JSON_MODIFY(append) will create new array with a single element:\ndeclare @json nvarchar(4000) = N'{\"Id\":1,\"Name\":\"Toy Car\",\"Price\":34.99}'\nset @json = JSON_MODIFY(@json, 'append $.Tags', 'sales')\nprint @json -- Output {\"Id\":1,\"Name\":\"Toy Car\",\"Tags\":[\"sales\"]}\nSection 31.3: Insert new JSON Object in JSON text\nJSON_MODIFY function enables you to insert JSON objects into JSON text:\ndeclare @json nvarchar(4000) = N'{\"Id\":1,\"Name\":\"Toy Car\"}'\nset @json = JSON_MODIFY(@json, '$.Price',\n                        JSON_QUERY('{\"Min\":34.99,\"Recommended\":45.49}'))\nprint @json\n-- Output: {\"Id\":1,\"Name\":\"Toy Car\",\"Price\":{\"Min\":34.99,\"Recommended\":45.49}}\nSince third parameter is text you need to wrap it with JSON_QUERY function to \"cast\" text to JSON. Without this\n\"cast\", JSON_MODIFY will treat third parameter as plain text and escape characters before inserting it as string\nvalue. Without JSON_QUERY results will be:\n{\"Id\":1,\"Name\":\"Toy Car\",\"Price\":'{\\\"Min\\\":34.99,\\\"Recommended\\\":45.49}'}\nJSON_MODIFY will insert this object if it does not exist, or delete it if value of third parameter is NULL.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 91\nSection 31.4: Insert new JSON array generated with FOR\nJSON query\nYou can generate JSON object using standard SELECT query with FOR JSON clause and insert it into JSON text as\nthird parameter:\ndeclare @json nvarchar(4000) = N'{\"Id\":17,\"Name\":\"WWI\"}'\nset @json = JSON_MODIFY(@json, '$.tables',\n                        (select name from sys.tables FOR JSON PATH) )\nprint @json\n(1 row(s) affected)\n{\"Id\":1,\"Name\":\"master\",\"tables\":[{\"name\":\"Colors\"},{\"name\":\"Colors_Archive\"},{\"name\":\"OrderLines\"}\n,{\"name\":\"PackageTypes\"},{\"name\":\"PackageTypes_Archive\"},{\"name\":\"StockGroups\"},{\"name\":\"StockItemS\ntockGroups\"},{\"name\":\"StockGroups_Archive\"},{\"name\":\"StateProvinces\"},{\"name\":\"CustomerTransactions\n\"},{\"name\":\"StateProvinces_Archive\"},{\"name\":\"Cities\"},{\"name\":\"Cities_Archive\"},{\"name\":\"SystemPar\nameters\"},{\"name\":\"InvoiceLines\"},{\"name\":\"Suppliers\"},{\"name\":\"StockItemTransactions\"},{\"name\":\"Su\nppliers_Archive\"},{\"name\":\"Customers\"},{\"name\":\"Customers_Archive\"},{\"name\":\"PurchaseOrders\"},{\"nam\ne\":\"Orders\"},{\"name\":\"People\"},{\"name\":\"StockItems\"},{\"name\":\"People_Archive\"},{\"name\":\"ColdRoomTem\nperatures\"},{\"name\":\"ColdRoomTemperatures_Archive\"},{\"name\":\"VehicleTemperatures\"},{\"name\":\"StockIt\nems_Archive\"},{\"name\":\"Countries\"},{\"name\":\"StockItemHoldings\"},{\"name\":\"sysdiagrams\"},{\"name\":\"Pur\nchaseOrderLines\"},{\"name\":\"Countries_Archive\"},{\"name\":\"DeliveryMethods\"},{\"name\":\"DeliveryMethods_\nArchive\"},{\"name\":\"PaymentMethods\"},{\"name\":\"SupplierTransactions\"},{\"name\":\"PaymentMethods_Archive\n\"},{\"name\":\"TransactionTypes\"},{\"name\":\"SpecialDeals\"},{\"name\":\"TransactionTypes_Archive\"},{\"name\":\n\"SupplierCategories\"},{\"name\":\"SupplierCategories_Archive\"},{\"name\":\"BuyingGroups\"},{\"name\":\"Invoic\nes\"},{\"name\":\"BuyingGroups_Archive\"},{\"name\":\"CustomerCategories\"},{\"name\":\"CustomerCategories_Arch\nive\"}]}\nJSON_MODIFY will know that select query with FOR JSON clause generates valid JSON array and it will just insert it\ninto JSON text.\nYou can use all FOR JSON options in SELECT query, except WITHOUT_ARRAY_WRAPPER, which will\ngenerate single object instead of JSON array. See other example in this topic to see how insert single JSON\nobject.\nSection 31.5: Insert single JSON object generated with FOR\nJSON clause\nYou can generate JSON object using standard SELECT query with FOR JSON clause and WITHOUT_ARRAY_WRAPPER\noption, and insert it into JSON text as a third parameter:\ndeclare @json nvarchar(4000) = N'{\"Id\":17,\"Name\":\"WWI\"}'\nset @json = JSON_MODIFY(@json, '$.table',\n                        JSON_QUERY(\n                         (select name, create_date, schema_id\n                           from sys.tables\n                           where name = 'Colors'\n                           FOR JSON PATH, WITHOUT_ARRAY_WRAPPER)))\nprint @json\n(1 row(s) affected)\n{\"Id\":17,\"Name\":\"WWI\",\"table\":{\"name\":\"Colors\",\"create_date\":\"2016-06-02T10:04:03.280\",\"schema_id\":\n13}}\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 92\nFOR JSON with WITHOUT_ARRAY_WRAPPER option may generate invalid JSON text if SELECT query returns more\nthan one result (you should use TOP 1 or ﬁlter by primary key in this case). Therefore, JSON_MODIFY will assume\nthat returned result is just a plain text and escape it like any other text if you don't wrap it with JSON_QUERY\nfunction.\nYou should wrap FOR JSON, WITHOUT_ARRAY_WRAPPER query with JSON_QUERY function in order to\ncast result to JSON.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 93\nChapter 32: FOR XML PATH\nSection 32.1: Using FOR XML PATH to concatenate values\nThe FOR XML PATH can be used for concatenating values into string. The example below concatenates values into a\nCSV string:\nDECLARE @DataSource TABLE\n(\n    [rowID] TINYINT\n   ,[FirstName] NVARCHAR(32)\n);\nINSERT INTO @DataSource ([rowID], [FirstName])\nVALUES (1, 'Alex')\n      ,(2, 'Peter')\n      ,(3, 'Alexsandyr')\n      ,(4, 'George');\nSELECT STUFF\n(\n    (\n        SELECT ',' + [FirstName]\n        FROM @DataSource\n        ORDER BY [rowID] DESC\n        FOR XML PATH(''), TYPE\n    ).value('.', 'NVARCHAR(MAX)')\n    ,1\n    ,1\n    ,''\n);\nFew important notes:\nthe ORDER BY clause can be used to order the values in a preferred way\nif a longer value is used as the concatenation separator, the STUFF function parameter must be changed too;\nSELECT STUFF\n(\n    (\n        SELECT '---' + [FirstName]\n        FROM @DataSource\n        ORDER BY [rowID] DESC\n        FOR XML PATH(''), TYPE\n    ).value('.', 'NVARCHAR(MAX)')\n    ,1\n    ,3 -- the \"3\" could also be represented as: LEN('---') for clarity\n    ,''\n);\nas the TYPE option and .value function are used, the concatenation works with NVARCHAR(MAX) string\nSection 32.2: Specifying namespaces\nVersion ≥ SQL Server 2008\nWITH XMLNAMESPACES (\n    DEFAULT 'http://www.w3.org/2000/svg',\n    'http://www.w3.org/1999/xlink' AS xlink\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 94\n)\nSELECT\n    'example.jpg' AS 'image/@xlink:href',\n    '50px' AS 'image/@width',\n    '50px' AS 'image/@height'\nFOR XML PATH('svg')\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" xmlns=\"http://www.w3.org/2000/svg\">\n    <image xlink:href=\"firefox.jpg\" width=\"50px\" height=\"50px\"/>\n</svg>\nSection 32.3: Specifying structure using XPath expressions\nSELECT\n    'XPath example' AS 'head/title',\n    'This example demonstrates ' AS 'body/p',\n    'https://www.w3.org/TR/xpath/' AS 'body/p/a/@href',\n    'XPath expressions' AS 'body/p/a'\nFOR XML PATH('html')\n<html>\n    <head>\n        <title>XPath example</title>\n    </head>\n    <body>\n        <p>This example demonstrates <a href=\"https://www.w3.org/TR/xpath/\">XPath\nexpressions</a></p>\n    </body>\n</html>\nIn FOR XML PATH, columns without a name become text nodes. NULL or '' therefore become empty text nodes.\nNote: you can convert a named column to an unnamed one by using AS *\nDECLARE @tempTable TABLE (Ref INT, Des NVARCHAR(100), Qty INT)\nINSERT INTO @tempTable VALUES (100001, 'Normal', 1), (100002, 'Foobar', 1), (100003, 'Hello World',\n2)\nSELECT ROW_NUMBER() OVER (ORDER BY Ref) AS '@NUM',\n     'REF' AS 'FLD/@NAME', REF AS 'FLD', '',\n     'DES' AS 'FLD/@NAME', DES AS 'FLD', '',\n     'QTY' AS 'FLD/@NAME', QTY AS 'FLD'\nFROM @tempTable\nFOR XML PATH('LIN'), ROOT('row')\n<row>\n  <LIN NUM=\"1\">\n    <FLD NAME=\"REF\">100001</FLD>\n    <FLD NAME=\"DES\">Normal</FLD>\n    <FLD NAME=\"QTY\">1</FLD>\n  </LIN>\n  <LIN NUM=\"2\">\n    <FLD NAME=\"REF\">100002</FLD>\n    <FLD NAME=\"DES\">Foobar</FLD>\n    <FLD NAME=\"QTY\">1</FLD>\n  </LIN>\n  <LIN NUM=\"3\">\n    <FLD NAME=\"REF\">100003</FLD>\n    <FLD NAME=\"DES\">Hello World</FLD>\n    <FLD NAME=\"QTY\">2</FLD>\n  </LIN>\n</row>\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 95\nUsing (empty) text nodes helps to separate the previously output node from the next one, so that SQL Server\nknows to start a new element for the next column. Otherwise, it gets confused when the attribute already exists on\nwhat it thinks is the \"current\" element.\nFor example, without the the empty strings between the element and the attribute in the SELECT statement, SQL\nServer gives an error:\nAttribute-centric column 'FLD/@NAME' must not come after a non-attribute-centric sibling in XML\nhierarchy in FOR XML PATH.\nAlso note that this example also wrapped the XML in a root element named row, speciﬁed by ROOT('row')\nSection 32.4: Hello World XML\nSELECT 'Hello World' FOR XML PATH('example')\n<example>Hello World</example>\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 96\nChapter 33: Join\nIn Structured Query Language (SQL), a JOIN is a method of linking two data tables in a single query, allowing the\ndatabase to return a set that contains data from both tables at once, or using data from one table to be used as a\nFilter on the second table. There are several types of JOINs deﬁned within the ANSI SQL standard.\nSection 33.1: Inner Join\nInner join returns only those records/rows that match/exists in both the tables based on one or more conditions\n(speciﬁed using ON keyword). It is the most common type of join. The general syntax for inner join is:\nSELECT *\nFROM table_1\nINNER JOIN table_2\n  ON table_1.column_name = table_2.column_name\nIt can also be simpliﬁed as just JOIN:\nSELECT *\nFROM table_1\nJOIN table_2\n  ON table_1.column_name = table_2.column_name\nExample\n/* Sample data. */\nDECLARE @Animal table (\n    AnimalId Int IDENTITY,\n    Animal Varchar(20)\n);\nDECLARE @AnimalSound table (\n    AnimalSoundId Int IDENTITY,\n    AnimalId Int,\n    Sound Varchar(20)\n);\nINSERT INTO @Animal (Animal) VALUES ('Dog');\nINSERT INTO @Animal (Animal) VALUES ('Cat');\nINSERT INTO @Animal (Animal) VALUES ('Elephant');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (1, 'Barks');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (2, 'Meows');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (3, 'Trumpets');\n/* Sample data prepared. */\nSELECT\n    *\nFROM\n    @Animal\n    JOIN @AnimalSound\n        ON @Animal.AnimalId = @AnimalSound.AnimalId;\nAnimalId    Animal               AnimalSoundId AnimalId    Sound\n----------- -------------------- ------------- ----------- --------------------\n1           Dog                  1             1           Barks\n2           Cat                  2             2           Meows\n3           Elephant             3             3           Trumpets\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 97\nUsing inner join with left outer join (Substitute for Not exists)\nThis query will return data from table 1 where ﬁelds matching with table2 with a key and data not in Table 1 when\ncomparing with Table2 with a condition and key\nselect *\n  from Table1 t1\n    inner join Table2 t2 on t1.ID_Column = t2.ID_Column\n    left  join Table3 t3 on t1.ID_Column = t3.ID_Column\n  where t2.column_name = column_value\n    and t3.ID_Column is null\n  order by t1.column_name;\nSection 33.2: Outer Join\nLeft Outer Join\nLEFT JOIN returns all rows from the left table, matched to rows from the right table where the ON clause conditions\nare met. Rows in which the ON clause is not met have NULL in all of the right table's columns. The syntax of a LEFT\nJOIN is:\nSELECT * FROM table_1 AS t1\nLEFT JOIN table_2 AS t2 ON t1.ID_Column = t2.ID_Column\nRight Outer Join\nRIGHT JOIN returns all rows from the right table, matched to rows from the left table where the ON clause\nconditions are met. Rows in which the ON clause is not met have NULL in all of the left table's columns. The syntax of\na RIGHT JOIN is:\nSELECT * FROM table_1 AS t1\nRIGHT JOIN table_2 AS t2 ON t1.ID_Column = t2.ID_Column\nFull Outer Join\nFULL JOIN combines LEFT JOIN and RIGHT JOIN. All rows are returned from both tables, regardless of whether the\nconditions in the ON clause are met. Rows that do not satisfy the ON clause are returned with NULL in all of the\nopposite table's columns (that is, for a row in the left table, all columns in the right table will contain NULL, and vice\nversa). The syntax of a FULL JOIN is:\nSELECT * FROM table_1 AS t1\nFULL JOIN table_2 AS t2 ON t1.ID_Column = t2.ID_Column  \nExamples\n/* Sample test data. */\nDECLARE @Animal table (\n    AnimalId Int IDENTITY,\n    Animal Varchar(20)\n);\nDECLARE @AnimalSound table (\n    AnimalSoundId Int IDENTITY,\n    AnimalId Int,\n    Sound Varchar(20)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 98\n);\nINSERT INTO @Animal (Animal) VALUES ('Dog');\nINSERT INTO @Animal (Animal) VALUES ('Cat');\nINSERT INTO @Animal (Animal) VALUES ('Elephant');\nINSERT INTO @Animal (Animal) VALUES ('Frog');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (1, 'Barks');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (2, 'Meows');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (3, 'Trumpet');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (5, 'Roars');\n/* Sample data prepared. */\nLEFT OUTER JOIN\nSELECT *\nFROM @Animal As t1\nLEFT JOIN @AnimalSound As t2 ON t1.AnimalId = t2.AnimalId;\nResults for LEFT JOIN\nAnimalId    Animal               AnimalSoundId AnimalId    Sound\n----------- -------------------- ------------- ----------- --------------------\n1           Dog                  1             1           Barks\n2           Cat                  2             2           Meows\n3           Elephant             3             3           Trumpet\n4           Frog                 NULL          NULL        NULL\nRIGHT OUTER JOIN\nSELECT *\nFROM @Animal As t1\nRIGHT JOIN @AnimalSound As t2 ON t1.AnimalId = t2.AnimalId;\nResults for RIGHT JOIN\nAnimalId    Animal               AnimalSoundId AnimalId    Sound\n----------- -------------------- ------------- ----------- --------------------\n1           Dog                  1             1           Barks\n2           Cat                  2             2           Meows\n3           Elephant             3             3           Trumpet\nNULL        NULL                 4             5           Roars\nFULL OUTER JOIN\nSELECT *\nFROM @Animal As t1\nFULL JOIN @AnimalSound As t2 ON t1.AnimalId = t2.AnimalId;\nResults for FULL JOIN\nAnimalId    Animal               AnimalSoundId AnimalId    Sound\n----------- -------------------- ------------- ----------- --------------------\n1           Dog                  1             1           Barks\n2           Cat                  2             2           Meows\n3           Elephant             3             3           Trumpet\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 99\n4           Frog                 NULL          NULL        NULL\nNULL        NULL                 4             5           Roars\nSection 33.3: Using Join in an Update\nJoins can also be used in an UPDATE statement:\nCREATE TABLE Users (\n    UserId int NOT NULL,\n    AccountId int NOT NULL,\n    RealName nvarchar(200) NOT NULL\n)\nCREATE TABLE Preferences (\n    UserId int NOT NULL,\n    SomeSetting bit NOT NULL\n)\nUpdate the SomeSetting column of the Preferences table ﬁltering by a predicate on the Users table as follows:\nUPDATE p\nSET p.SomeSetting = 1\nFROM Users u\nJOIN Preferences p ON u.UserId = p.UserId\nWHERE u.AccountId = 1234\np is an alias for Preferences deﬁned in the FROM clause of the statement. Only rows with a matching AccountId\nfrom the Users table will be updated.\nUpdate with left outer join statements\nUpdate t\nSET  t.Column1=100\nFROM Table1 t LEFT JOIN Table12 t2\nON t2.ID=t.ID\nUpdate tables with inner join and aggregate function\nUPDATE t1\nSET t1.field1 = t2.field2Sum\nFROM table1 t1\nINNER JOIN (select field3, sum(field2) as field2Sum\nfrom table2\ngroup by field3) as t2\non t2.field3 = t1.field3  \nSection 33.4: Join on a Subquery\nJoining on a subquery is often used when you want to get aggregate data (such as Count, Avg, Max, or Min) from a\nchild/details table and display that along with records from the parent/header table. For example, you may want to\nretrieve the top/ﬁrst child row based on Date or Id or maybe you want a Count of all Child Rows or an Average.\nThis example uses aliases which makes queries easier to read when you have multiple tables involved. In this case\nwe are retrieving all rows from the parent table Purchase Orders and retrieving only the last (or most recent) child\nrow from the child table PurchaseOrderLineItems. This example assumes the child table uses incremental numeric\nId's.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 100\nSELECT po.Id, po.PODate, po.VendorName, po.Status, item.ItemNo,\n  item.Description, item.Cost, item.Price\nFROM PurchaseOrders po\nLEFT JOIN\n     (\n       SELECT l.PurchaseOrderId, l.ItemNo, l.Description, l.Cost, l.Price, Max(l.id) as Id\n       FROM PurchaseOrderLineItems l\n       GROUP BY l.PurchaseOrderId, l.ItemNo, l.Description, l.Cost, l.Price\n     ) AS item ON item.PurchaseOrderId = po.Id\nSection 33.5: Cross Join\nA cross join is a Cartesian join, meaning a Cartesian product of both the tables. This join does not need any\ncondition to join two tables. Each row in the left table will join to each row of the right table. Syntax for a cross join:\nSELECT * FROM table_1\nCROSS JOIN table_2\nExample:\n/* Sample data. */\nDECLARE @Animal table (\n    AnimalId Int IDENTITY,\n    Animal Varchar(20)\n);\nDECLARE @AnimalSound table (\n    AnimalSoundId Int IDENTITY,\n    AnimalId Int,\n    Sound Varchar(20)\n);\nINSERT INTO @Animal (Animal) VALUES ('Dog');\nINSERT INTO @Animal (Animal) VALUES ('Cat');\nINSERT INTO @Animal (Animal) VALUES ('Elephant');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (1, 'Barks');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (2, 'Meows');\nINSERT INTO @AnimalSound (AnimalId, Sound) VALUES (3, 'Trumpet');\n/* Sample data prepared. */\nSELECT\n    *\nFROM\n    @Animal\n    CROSS JOIN @AnimalSound;\nResults:\nAnimalId    Animal               AnimalSoundId AnimalId    Sound\n----------- -------------------- ------------- ----------- --------------------\n1           Dog                  1             1           Barks\n2           Cat                  1             1           Barks\n3           Elephant             1             1           Barks\n1           Dog                  2             2           Meows\n2           Cat                  2             2           Meows\n3           Elephant             2             2           Meows\n1           Dog                  3             3           Trumpet\n2           Cat                  3             3           Trumpet\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 101\n3           Elephant             3             3           Trumpet\nNote that there are other ways that a CROSS JOIN can be applied. This is a an \"old style\" join (deprecated since ANSI\nSQL-92) with no condition, which results in a cross/Cartesian join:\nSELECT *\nFROM @Animal, @AnimalSound;\nThis syntax also works due to an \"always true\" join condition, but is not recommended and should be avoided, in\nfavor of explicit CROSS JOIN syntax, for the sake of readability.\nSELECT *\nFROM\n    @Animal\n    JOIN @AnimalSound\n        ON 1=1\nSection 33.6: Self Join\nA table can be joined onto itself in what is known as a self join, combining records in the table with other records in\nthe same table. Self joins are typically used in queries where a hierarchy in the table's columns is deﬁned.\nConsider the sample data in a table called Employees:\nID Name Boss_ID\n1 Bob 3\n2 Jim 1\n3 Sam 2\nEach employee's Boss_ID maps to another employee's ID. To retrieve a list of employees with their respective boss'\nname, the table can be joined on itself using this mapping. Note that joining a table in this manner requires the use\nof an alias (Bosses in this case) on the second reference to the table to distinguish itself from the original table.\nSELECT Employees.Name,\n    Bosses.Name AS Boss\nFROM Employees\nINNER JOIN Employees AS Bosses\n    ON Employees.Boss_ID = Bosses.ID\nExecuting this query will output the following results:\nName Boss\nBob Sam\nJim Bob\nSam Jim\nSection 33.7: Accidentally turning an outer join into an inner\njoin\nOuter joins return all the rows from one or both tables, plus matching rows.\nTable People\nPersonID FirstName\n       1 Alice\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 102\n       2 Bob\n       3 Eve\nTable Scores\nPersonID Subject Score\n       1 Math    100\n       2 Math     54\n       2 Science  98\nLeft joining the tables:\nSelect * from People a\nleft join Scores b\non a.PersonID = b.PersonID\nReturns:\nPersonID FirstName PersonID Subject Score\n       1 Alice            1 Math    100\n       2 Bob              2 Math     54\n       2 Bob              2 Science  98\n       3 Eve           NULL NULL   NULL\nIf you wanted to return all the people, with any applicable math scores, a common mistake is to write:\nSelect * from People a\nleft join Scores b\non a.PersonID = b.PersonID\nwhere Subject = 'Math'\nThis would remove Eve from your results, in addition to removing Bob's science score, as Subject is NULL for her.\nThe correct syntax to remove non-Math records while retaining all individuals in the People table would be:\nSelect * from People a\nleft join Scores b\non a.PersonID = b.PersonID\nand b.Subject = 'Math'\nSection 33.8: Delete using Join\nJoins can also be used in a DELETE statement. Given a schema as follows:\nCREATE TABLE Users (\n    UserId int NOT NULL,\n    AccountId int NOT NULL,\n    RealName nvarchar(200) NOT NULL\n)\nCREATE TABLE Preferences (\n    UserId int NOT NULL,\n    SomeSetting bit NOT NULL\n)\nWe can delete rows from the Preferences table, ﬁltering by a predicate on the Users table as follows:\nDELETE p\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 103\nFROM Users u\nINNER JOIN Preferences p ON u.UserId = p.UserId\nWHERE u.AccountId = 1234\nHere p is an alias for Preferences deﬁned in the FROM clause of the statement and we only delete rows that have a\nmatching AccountId from the Users table.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 104\nChapter 34: cross apply\nSection 34.1: Join table rows with dynamically generated rows\nfrom a cell\nCROSS APPLY enables you to \"join\" rows from a table with dynamically generated rows returned by some table-\nvalue function.\nImagine that you have a Company table with a column that contains an array of products (ProductList column), and\na function that parse these values and returns a set of products. You can select all rows from a Company table,\napply this function on a ProductList column and \"join\" generated results with parent Company row:\nSELECT *\nFROM Companies c\n     CROSS APPLY dbo.GetProductList( c.ProductList ) p\nFor each row, value of ProductList cell will be provided to the function, and the function will return those products as\na set of rows that can be joined with the parent row.\nSection 34.2: Join table rows with JSON array stored in cell\nCROSS APPLY enables you to \"join\" rows from a table with collection of JSON objects stored in a column.\nImagine that you have a Company table with a column that contains an array of products (ProductList column)\nformatted as JSON array. OPENJSON table value function can parse these values and return the set of products. You\ncan select all rows from a Company table, parse JSON products with OPENJSON and \"join\" generated results with\nparent Company row:\nSELECT *\nFROM Companies c\n     CROSS APPLY OPENJSON( c.ProductList )\n                 WITH ( Id int, Title nvarchar(30), Price money)\nFor each row, value of ProductList cell will be provided to OPENJSON function that will transform JSON objects to\nrows with the schema deﬁned in WITH clause.\nSection 34.3: Filter rows by array values\nIf you store a list of tags in a row as coma separated values, STRING_SPLIT function enables you to transform list of\ntags into a table of values. CROSS APPLY enables you to \"join\" values parsed by STRING_SPLIT function with a parent\nrow.\nImagine that you have a Product table with a column that contains an array of comma separated tags (e.g.\npromo,sales,new). STRING_SPLIT and CROSS APPLY enable you to join product rows with their tags so you can ﬁlter\nproducts by tags:\nSELECT *\nFROM Products p\n     CROSS APPLY STRING_SPLIT( p.Tags, ',' ) tags\nWHERE tags.value = 'promo'\nFor each row, value of Tags cell will be provided to STRING_SPLIT function that will return tag values. Then you can\nﬁlter rows by these values.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 105\nNote: STRING_SPLIT function is not available before SQL Server 2016\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 106\nChapter 35: Computed Columns\nSection 35.1: A column is computed from an expression\nA computed column is computed from an expression that can use other columns in the same table. The expression\ncan be a noncomputed column name, constant, function, and any combination of these connected by one or more\noperators.\nCreate table with a computed column\nCreate table NetProfit\n(\n    SalaryToEmployee            int,    \n    BonusDistributed            int,\n    BusinessRunningCost         int,    \n    BusinessMaintenanceCost     int,\n    BusinessEarnings            int,\n    BusinessNetIncome\n                As BusinessEarnings - (SalaryToEmployee          +\n                                       BonusDistributed          +\n                                       BusinessRunningCost       +\n                                       BusinessMaintenanceCost    )\n                                           \n)\nValue is computed and stored in the computed column automatically on inserting other values.\nInsert Into NetProfit\n    (SalaryToEmployee,\n     BonusDistributed,\n     BusinessRunningCost,\n     BusinessMaintenanceCost,\n     BusinessEarnings)\nValues        \n    (1000000,\n     10000,\n     1000000,\n     50000,\n     2500000)    \nSection 35.2: Simple example we normally use in log tables\nCREATE TABLE [dbo].[ProcessLog](\n[LogId] [int] IDENTITY(1,1) NOT NULL,\n[LogType] [varchar](20) NULL,\n[StartTime] [datetime] NULL,\n[EndTime] [datetime] NULL,\n[RunMinutes]  AS (datediff(minute,coalesce([StartTime],getdate()),coalesce([EndTime],getdate())))\nThis gives run diﬀerence in minutes for runtime which will be very handy..\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 107\nChapter 36: Common Table Expressions\nSection 36.1: Generate a table of dates using CTE\nDECLARE @startdate CHAR(8), @numberDays TINYINT\nSET @startdate = '20160101'\nSET @numberDays = 10;\nWITH CTE_DatesTable\nAS\n(\n  SELECT CAST(@startdate as date) AS [date]\n  UNION ALL\n  SELECT DATEADD(dd, 1, [date])\n  FROM CTE_DatesTable\n  WHERE DATEADD(dd, 1, [date]) <= DateAdd(DAY, @numberDays-1, @startdate)\n)\nSELECT [date] FROM CTE_DatesTable\nOPTION (MAXRECURSION 0)\nThis example returns a single-column table of dates, starting with the date speciﬁed in the @startdate variable, and\nreturning the next @numberDays worth of dates.\nSection 36.2: Employee Hierarchy\nTable Setup\nCREATE TABLE dbo.Employees\n(\n    EmployeeID INT NOT NULL PRIMARY KEY,\n    FirstName NVARCHAR(50) NOT NULL,\n    LastName NVARCHAR(50) NOT NULL,\n    ManagerID INT NULL\n)\nGO\nINSERT INTO Employees VALUES (101, 'Ken', 'Sánchez', NULL)\nINSERT INTO Employees VALUES (102, 'Keith', 'Hall', 101)\nINSERT INTO Employees VALUES (103, 'Fred', 'Bloggs', 101)\nINSERT INTO Employees VALUES (104, 'Joseph', 'Walker', 102)\nINSERT INTO Employees VALUES (105, 'Žydrė', 'Klybė', 101)\nINSERT INTO Employees VALUES (106, 'Sam', 'Jackson', 105)\nINSERT INTO Employees VALUES (107, 'Peter', 'Miller', 103)\nINSERT INTO Employees VALUES (108, 'Chloe', 'Samuels', 105)\nINSERT INTO Employees VALUES (109, 'George', 'Weasley', 105)\nINSERT INTO Employees VALUES (110, 'Michael', 'Kensington', 106)\nCommon Table Expression\n;WITH cteReports (EmpID, FirstName, LastName, SupervisorID, EmpLevel) AS\n(\n    SELECT EmployeeID, FirstName, LastName, ManagerID, 1\n    FROM Employees\n    WHERE ManagerID IS NULL\n    UNION ALL\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 108\n    SELECT e.EmployeeID, e.FirstName, e.LastName, e.ManagerID, r.EmpLevel + 1\n    FROM Employees        AS e\n    INNER JOIN cteReports AS r ON e.ManagerID = r.EmpID\n)\nSELECT\n    FirstName + ' ' + LastName AS FullName,\n    EmpLevel,\n    (SELECT FirstName + ' ' + LastName FROM Employees WHERE EmployeeID = cteReports.SupervisorID)\nAS ManagerName\nFROM cteReports\nORDER BY EmpLevel, SupervisorID\nOutput:\nFullName EmpLevel ManagerName\nKen Sánchez 1 null\nKeith Hall 2 Ken Sánchez\nFred Bloggs 2 Ken Sánchez\nŽydre Klybe 2 Ken Sánchez\nJoseph Walker 3 Keith Hall\nPeter Miller 3 Fred Bloggs\nSam Jackson 3 Žydre Klybe\nChloe Samuels 3 Žydre Klybe\nGeorge Weasley 3 Žydre Klybe\nMichael Kensington 4 Sam Jackson\nSection 36.3: Recursive CTE\nThis example shows how to get every year from this year to 2011 (2012 - 1).\nWITH yearsAgo\n(\n    myYear\n)\nAS\n(\n     -- Base Case: This is where the recursion starts\n     SELECT DATEPART(year, GETDATE()) AS myYear\n     UNION ALL  -- This MUST be UNION ALL (cannot be UNION)\n     -- Recursive Section: This is what we're doing with the recursive call\n     SELECT yearsAgo.myYear - 1\n     FROM yearsAgo\n     WHERE yearsAgo.myYear >= 2012\n)\n     SELECT myYear FROM yearsAgo;  -- A single SELECT, INSERT, UPDATE, or DELETE\nmyYear\n2016\n2015\n2014\n2013\n2012\n2011\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 109\nYou can control the recursion (think stack overﬂow in code) with MAXRECURSION as a query option that will limit\nthe number of recursive calls.\nWITH yearsAgo\n(\n    myYear\n)\nAS\n(\n     -- Base Case\n     SELECT DATEPART(year , GETDATE()) AS myYear\n     UNION ALL\n     -- Recursive Section\n     SELECT yearsAgo.myYear - 1\n     FROM yearsAgo\n     WHERE yearsAgo.myYear >= 2002\n)\n     SELECT * FROM yearsAgo\n     OPTION (MAXRECURSION 10);\nMsg 530, Level 16, State 1, Line 2The statement terminated. The maximum recursion 10 has been\nexhausted before statement completion.\nSection 36.4: Delete duplicate rows using CTE\nEmployees table :\n|  ID  | FirstName | LastName | Gender | Salary |\n+------+-----------+----------+--------+--------+\n|  1   | Mark      | Hastings | Male   | 60000  |\n|  1   | Mark      | Hastings | Male   | 60000  |\n|  2   | Mary      | Lambeth  | Female | 30000  |\n|  2   | Mary      | Lambeth  | Female | 30000  |\n|  3   | Ben       | Hoskins  | Male   | 70000  |\n|  3   | Ben       | Hoskins  | Male   | 70000  |\n|  3   | Ben       | Hoskins  | Male   | 70000  |\n+------+-----------+----------+--------+--------+\nCTE (Common Table Expression) :\nWITH EmployeesCTE AS\n(\n   SELECT *, ROW_NUMBER()OVER(PARTITION BY ID ORDER BY ID) AS RowNumber\n   FROM Employees\n)\nDELETE FROM EmployeesCTE WHERE RowNumber > 1\nExecution result :\n|  ID  | FirstName | LastName | Gender | Salary |\n+------+-----------+----------+--------+--------+\n|  1   | Mark      | Hastings | Male   | 60000  |\n|  2   | Mary      | Lambeth  | Female | 30000  |\n|  3   | Ben       | Hoskins  | Male   | 70000  |\n+------+-----------+----------+--------+--------+\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 110\nSection 36.5: CTE with multiple AS statements\n;WITH cte_query_1\nAS\n(\n    SELECT *\n    FROM database.table1\n),\ncte_query_2\nAS\n(\n    SELECT *\n    FROM database.table2\n)\nSELECT *\nFROM cte_query_1\nWHERE cte_query_one.fk IN\n(\n    SELECT PK\n    FROM cte_query_2\n)\nWith common table expressions, it is possible to create multiple queries using comma-separated AS statements. A\nquery can then reference any or all of those queries in many diﬀerent ways, even joining them.\nSection 36.6: Find nth highest salary using CTE\nEmployees table :\n|  ID  | FirstName | LastName | Gender | Salary |\n+------+-----------+----------+--------+--------+\n| 1    | Jahangir  | Alam     | Male   | 70000  |\n| 2    | Arifur    | Rahman   | Male   | 60000  |\n| 3    | Oli       | Ahammed  | Male   | 45000  |\n| 4    | Sima      | Sultana  | Female | 70000  |\n| 5    | Sudeepta  | Roy      | Male   | 80000  |\n+------+-----------+----------+--------+--------+\nCTE (Common Table Expression) :\n WITH RESULT AS\n(\n    SELECT SALARY,\n           DENSE_RANK() OVER (ORDER BY SALARY DESC) AS DENSERANK\n    FROM EMPLOYEES\n)\nSELECT TOP 1 SALARY\nFROM RESULT\nWHERE DENSERANK = 1\nTo ﬁnd 2nd highest salary simply replace N with 2. Similarly, to ﬁnd 3rd highest salary, simply replace N with 3.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 111\nChapter 37: Move and copy data around\ntables\nSection 37.1: Copy data from one table to another\nThis code selects data out of a table and displays it in the query tool (usually SSMS)\nSELECT Column1, Column2, Column3 FROM MySourceTable;\nThis code inserts that data into a table:\nINSERT INTO MyTargetTable (Column1, Column2, Column3)\nSELECT Column1, Column2, Column3 FROM MySourceTable;\nSection 37.2: Copy data into a table, creating that table on\nthe ﬂy\nThis code selects data out of a table:\nSELECT Column1, Column2, Column3 FROM MySourceTable;\nThis code creates a new table called MyNewTable and puts that data into it\nSELECT Column1, Column2, Column3\nINTO MyNewTable\nFROM MySourceTable;\nSection 37.3: Move data into a table (assuming unique keys\nmethod)\nTo move data you ﬁrst insert it into the target, then delete whatever you inserted from the source table. This is not a\nnormal SQL operation but it may be enlightening\nWhat did you insert? Normally in databases you need to have one or more columns that you can use to uniquely\nidentify rows so we will assume that and make use of it.\nThis statement selects some rows\nSELECT Key1, Key2, Column3, Column4 FROM MyTable;\nFirst we insert these into our target table:\nINSERT INTO TargetTable (Key1, Key2, Column3, Column4)\nSELECT Key1, Key2, Column3, Column4 FROM MyTable;\nNow assuming records in both tables are unique on Key1,Key2, we can use that to ﬁnd and delete data out of the\nsource table\nDELETE MyTable\nWHERE EXISTS (\n    SELECT * FROM TargetTable\n    WHERE TargetTable.Key1 = SourceTable.Key1\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 112\n    AND TargetTable.Key2 = SourceTable.Key2\n);\nThis will only work correctly if Key1, Key2 are unique in both tables\nLastly, we don't want the job half done. If we wrap this up in a transaction then either all data will be moved, or\nnothing will happen. This ensures we don't insert the data in then ﬁnd ourselves unable to delete the data out of\nthe source.\nBEGIN TRAN;\nINSERT INTO TargetTable (Key1, Key2, Column3, Column4)\nSELECT Key1, Key2, Column3, Column4 FROM MyTable;\nDELETE MyTable\nWHERE EXISTS (\n    SELECT * FROM TargetTable\n    WHERE TargetTable.Key1 = SourceTable.Key1\n    AND TargetTable.Key2 = SourceTable.Key2\n);\nCOMMIT TRAN;\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 113\nChapter 38: Limit Result Set\nParameter Details\nTOP Limiting keyword. Use with a number.\nPERCENT Percentage keyword. Comes after TOP and limiting number.\nAs database tables grow, it's often useful to limit the results of queries to a ﬁxed number or percentage. This can be\nachieved using SQL Server's TOP keyword or OFFSET FETCH clause.\nSection 38.1: Limiting With PERCENT\nThis example limits SELECT result to 15 percentage of total row count.\nSELECT TOP 15 PERCENT *\nFROM table_name\nSection 38.2: Limiting with FETCH\nVersion ≥ SQL Server 2012\nFETCH is generally more useful for pagination, but can be used as an alternative to TOP:\nSELECT *\nFROM table_name\nORDER BY 1\nOFFSET 0 ROWS\nFETCH NEXT 50 ROWS ONLY\nSection 38.3: Limiting With TOP\nThis example limits SELECT result to 100 rows.\nSELECT TOP 100 *\nFROM table_name;\nIt is also possible to use a variable to specify the number of rows:\nDECLARE @CountDesiredRows int = 100;\nSELECT TOP (@CountDesiredRows) *\nFROM table_name;\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 114\nChapter 39: Retrieve Information about\nyour Instance\nSection 39.1: General Information about Databases, Tables,\nStored procedures and how to search them\nQuery to search last executed sp's in db\nSELECT execquery.last_execution_time AS [Date Time], execsql.text AS [Script]\nFROM sys.dm_exec_query_stats AS execquery\nCROSS APPLY sys.dm_exec_sql_text(execquery.sql_handle) AS execsql\nORDER BY execquery.last_execution_time DESC\nQuery to search through Stored procedures\nSELECT o.type_desc AS ROUTINE_TYPE,o.[name] AS ROUTINE_NAME,\nm.definition AS ROUTINE_DEFINITION\nFROM sys.sql_modules AS m INNER JOIN sys.objects AS o\nON m.object_id = o.object_id WHERE m.definition LIKE '%Keyword%'\norder by ROUTINE_NAME\nQuery to Find Column From All Tables of Database\nSELECT t.name AS table_name,\nSCHEMA_NAME(schema_id) AS schema_name,\nc.name AS column_name\nFROM sys.tables AS t\nINNER JOIN sys.columns c ON t.OBJECT_ID = c.OBJECT_ID\nwhere c.name like 'Keyword%'\nORDER BY schema_name, table_name;\nQuery to to check restore details\nWITH LastRestores AS\n(\nSELECT\n    DatabaseName = [d].[name] ,\n    [d].[create_date] ,\n    [d].[compatibility_level] ,\n    [d].[collation_name] ,\n    r.*,\n    RowNum = ROW_NUMBER() OVER (PARTITION BY d.Name ORDER BY r.[restore_date] DESC)\nFROM master.sys.databases d\nLEFT OUTER JOIN msdb.dbo.[restorehistory] r ON r.[destination_database_name] = d.Name\n)\nSELECT *\nFROM [LastRestores]\nWHERE [RowNum] = 1\nQuery to to ﬁnd the log\nselect top 100 * from databaselog\nOrder by Posttime desc\nQuery to to check the Sps details\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 115\nSELECT name, create_date, modify_date\nFROM sys.objects\nWHERE type = 'P'\nOrder by modify_date desc\nSection 39.2: Get information on current sessions and query\nexecutions\nsp_who2\nThis procedure can be used to ﬁnd information on current SQL server sessions. Since it is a procedure, it's often\nhelpful to store the results into a temporary table or table variable so one can order, ﬁlter, and transform the\nresults as needed.\nThe below can be used for a queryable version of sp_who2:\n-- Create a variable table to hold the results of sp_who2 for querying purposes\nDECLARE @who2 TABLE (\n      SPID INT NULL,\n      Status VARCHAR(1000) NULL,\n      Login SYSNAME NULL,\n      HostName SYSNAME NULL,\n      BlkBy SYSNAME NULL,\n      DBName SYSNAME NULL,\n      Command VARCHAR(8000) NULL,\n      CPUTime INT NULL,\n      DiskIO INT NULL,\n      LastBatch VARCHAR(250) NULL,\n      ProgramName VARCHAR(250) NULL,\n      SPID2 INT NULL, -- a second SPID for some reason...?\n      REQUESTID INT NULL\n)\nINSERT INTO @who2\nEXEC sp_who2\nSELECT    *\nFROM    @who2 w\nWHERE    1=1\nExamples:\n-- Find specific user sessions:\nSELECT  *\nFROM    @who2 w\nWHERE   1=1\n    and  login = 'userName'\n-- Find longest CPUTime queries:\nSELECT  top 5 *\nFROM    @who2 w\nWHERE   1=1\norder   by CPUTime desc\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 116\nSection 39.3: Information about SQL Server version\nTo discover SQL Server's edition, product level and version number as well as the host machine name and the\nserver type:\nSELECT    SERVERPROPERTY('MachineName') AS Host,\n          SERVERPROPERTY('InstanceName') AS Instance,\n          DB_NAME() AS DatabaseContext,\n          SERVERPROPERTY('Edition') AS Edition,\n          SERVERPROPERTY('ProductLevel') AS ProductLevel,\n          CASE SERVERPROPERTY('IsClustered')\n            WHEN 1 THEN 'CLUSTERED'\n            ELSE 'STANDALONE' END AS ServerType,\n          @@VERSION AS VersionNumber;\nSection 39.4: Retrieve Edition and Version of Instance\nSELECT    SERVERPROPERTY('ProductVersion') AS ProductVersion,  \n          SERVERPROPERTY('ProductLevel') AS ProductLevel,  \n          SERVERPROPERTY('Edition') AS Edition,  \n          SERVERPROPERTY('EngineEdition') AS EngineEdition;  \nSection 39.5: Retrieve Instance Uptime in Days\nSELECT  DATEDIFF(DAY, login_time, getdate()) UpDays\nFROM    master..sysprocesses\nWHERE   spid = 1\nSection 39.6: Retrieve Local and Remote Servers\nTo retrieve a list of all servers registered on the instance:\nEXEC sp_helpserver;\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 117\nChapter 40: With Ties Option\nSection 40.1: Test Data\nCREATE TABLE #TEST\n(\nId INT,\nName VARCHAR(10)\n)\nInsert Into #Test\nselect 1,'A'\nUnion All\nSelect 1,'B'\nunion all\nSelect 1,'C'\nunion all\nSelect 2,'D'\nBelow is the output of above table,As you can see Id Column is repeated three times..\nId   Name\n1    A\n1    B\n1    C\n2    D\nNow Lets check the output using simple order by..\nSelect Top (1)  Id,Name From\n#test\nOrder By Id ;\nOutput :(Output of above query is not guaranteed to be same every time )\nId   Name\n1    B\nLets run the Same query With Ties Option..\nSelect Top (1) With Ties Id,Name\n From\n#test\nOrder By Id\nOutput :\nId   Name\n1    A\n1    B\n1    C\nAs you can see SQL Server outputs all the Rows which are tied with Order by Column. Lets see one more Example\nto understand this better..\nSelect Top (1) With Ties Id,Name\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 118\n From\n#test\nOrder By Id ,Name\nOutput:\nId   Name\n1    A\nIn Summary ,when we use with Ties Option,SQL Server Outputs all the Tied rows irrespective of limit we impose\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 119\nChapter 41: String Functions\nSection 41.1: Quotename\nReturns a Unicode string surrounded by delimiters to make it a valid SQL Server delimited identiﬁer.\nParameters:\ncharacter string. A string of Unicode data, up to 128 characters (sysname). If an input string is longer than 1281.\ncharacters function returns null.\nquote character. Optional. A single character to use as a delimiter. Can be a single quotation mark (' or ``),2.\na left or right bracket ({,[,(,< or >,),],}) or a double quotation mark (\"). Any other value will return null.\nDefault value is square brackets.\nSELECT QUOTENAME('what''s my name?')      -- Returns [what's my name?]\nSELECT QUOTENAME('what''s my name?', '[') -- Returns [what's my name?]\nSELECT QUOTENAME('what''s my name?', ']') -- Returns [what's my name?]\nSELECT QUOTENAME('what''s my name?', '''') -- Returns 'what''s my name?'\nSELECT QUOTENAME('what''s my name?', '\"') -- Returns \"what's my name?\"\nSELECT QUOTENAME('what''s my name?', ')') -- Returns (what's my name?)\nSELECT QUOTENAME('what''s my name?', '(') -- Returns (what's my name?)\nSELECT QUOTENAME('what''s my name?', '<') -- Returns <what's my name?>\nSELECT QUOTENAME('what''s my name?', '>') -- Returns <what's my name?>\nSELECT QUOTENAME('what''s my name?', '{') -- Returns {what's my name?}\nSELECT QUOTENAME('what''s my name?', '}') -- Returns {what's my name?}\nSELECT QUOTENAME('what''s my name?', '`') -- Returns `what's my name?`\nSection 41.2: Replace\nReturns a string (varchar or nvarchar) where all occurrences of a speciﬁed sub string is replaced with another sub\nstring.\nParameters:\nstring expression. This is the string that would be searched. It can be a character or binary data type.1.\npattern. This is the sub string that would be replaced. It can be a character or binary data type. The pattern2.\nargument cannot be an empty string.\nreplacement. This is the sub string that would replace the pattern sub string. It can be a character or binary3.\ndata.\nSELECT REPLACE('This is my string', 'is', 'XX') -- Returns 'ThXX XX my string'.\nNotes:\nIf string expression is not of type varchar(max) or nvarchar(max), the replace function truncates the return\nvalue at 8,000 chars.\nReturn data type depends on input data types - returns nvarchar if one of the input values is nvarchar, or\nvarchar otherwise.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 120\nReturn NULL if any of the input parameters is NULL\nSection 41.3: Substring\nReturns a substring that starts with the char that's in the speciﬁed start index and the speciﬁed max length.\nParameters:\nCharacter expression. The character expression can be of any data type that can be implicitly converted to1.\nvarchar or nvarchar, except for text or ntext.\nStart index. A number (int or bigint) that speciﬁes the start index of the requested substring. (Note: strings2.\nin sql server are base 1 index, meaning that the ﬁrst character of the string is index 1). This number can be\nless then 1. In this case, If the sum of start index and max length is greater then 0, the return string would be\na string starting from the ﬁrst char of the character expression and with the length of (start index + max\nlength - 1). If it's less then 0, an empty string would be returned.\nMax length. An integer number between 0 and bigint max value (9,223,372,036,854,775,807). If the max3.\nlength parameter is negative, an error will be raised.\nSELECT SUBSTRING('This is my string', 6, 5) -- returns 'is my'\nIf the max length + start index is more then the number of characters in the string, the entier string is returned.\nSELECT SUBSTRING('Hello World',1,100) -- returns 'Hello World'\nIf the start index is bigger then the number of characters in the string, an empty string is returned.\nSELECT SUBSTRING('Hello World',15,10) -- returns ''\nSection 41.4: String_Split\nVersion ≥ SQL Server 2016\nSplits a string expression using a character separator. Note that STRING_SPLIT() is a table-valued function and\ntherefore must be used within FROM clause.\nParameters:\nstring. Any character type expression (char, nchar, varchar or nvarchar)1.\nseperator. A single character expression of any type (char(1), nchar(1), varchar(1) or nvarchar(1)).2.\nReturns a single column table where each row contains a fragment of the string. The name of the columns is value,\nand the datatype is nvarchar if any of the parameters is either nchar or nvarchar, otherwise varchar.\nThe following example splits a string using space as a separator:\nSELECT value FROM STRING_SPLIT('Lorem ipsum dolor sit amet.', ' ');\nResult:\nvalue\n-----\nLorem\nipsum\ndolor\nsit\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 121\namet.\nRemarks:\nThe STRING_SPLIT function is available only under compatibility level 130. If your database compatibility\nlevel is lower than 130, SQL Server will not be able to ﬁnd and execute STRING_SPLIT function. You can\nchange the compatibility level of a database using the following command:\nALTER DATABASE [database_name] SET COMPATIBILITY_LEVEL = 130\nVersion < SQL Server 2016\nOlder versions of sql server does not have a built in split string function. There are many user deﬁned functions\nthat handles the problem of splitting a string. You can read Aaron Bertrand's article Split strings the right way – or\nthe next best way for a comprehensive comparison of some of them.\nSection 41.5: Left\nReturns a sub string starting with the left most char of a string and up to the maximum length speciﬁed.\nParameters:\ncharacter expression. The character expression can be of any data type that can be implicitly converted to1.\nvarchar or nvarchar, except for text or ntext\nmax length. An integer number between 0 and bigint max value (9,223,372,036,854,775,807).2.\nIf the max length parameter is negative, an error will be raised.\nSELECT LEFT('This is my string', 4) -- result: 'This'\nIf the max length is more then the number of characters in the string, the entier string is returned.\nSELECT LEFT('This is my string', 50) -- result: 'This is my string'\nSection 41.6: Right\nReturns a sub string that is the right most part of the string, with the speciﬁed max length.\nParameters:\ncharacter expression. The character expression can be of any data type that can be implicitly converted to1.\nvarchar or nvarchar, except for text or ntext\nmax length. An integer number between 0 and bigint max value (9,223,372,036,854,775,807). If the max2.\nlength parameter is negative, an error will be raised.\nSELECT RIGHT('This is my string', 6) -- returns 'string'\nIf the max length is more then the number of characters in the string, the entier string is returned.\nSELECT RIGHT('This is my string', 50) -- returns 'This is my string'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 122\nSection 41.7: Soundex\nReturns a four-character code (varchar) to evaluate the phonetic similarity of two strings.\nParameters:\ncharacter expression. An alphanumeric expression of character data.1.\nThe soundex function creates a four-character code that is based on how the character expression would sound\nwhen spoken. the ﬁrst char is the the upper case version of the ﬁrst character of the parameter, the rest 3\ncharacters are numbers representing the letters in the expression (except a, e, i, o, u, h, w and y that are ignored).\nSELECT SOUNDEX ('Smith') -- Returns 'S530'\nSELECT SOUNDEX ('Smythe') -- Returns 'S530'\nSection 41.8: Format\nVersion ≥ SQL Server 2012\nReturns a NVARCHAR value formatted with the speciﬁed format and culture (if speciﬁed). This is primarily used for\nconverting date-time types to strings.\nParameters:\nvalue. An expression of a supported data type to format. valid types are listed below.1.\nformat. An NVARCHAR format pattern. See Microsoft oﬃcial documentation for standard and custom format2.\nstrings.\nculture. Optional. nvarchar argument specifying a culture. The default value is the culture of the current3.\nsession.\nDATE\nUsing standard format strings:\nDECLARE @d DATETIME = '2016-07-31';  \nSELECT\n    FORMAT ( @d, 'd', 'en-US' ) AS 'US English Result' -- Returns '7/31/2016'\n   ,FORMAT ( @d, 'd', 'en-gb' ) AS 'Great Britain English Result' -- Returns '31/07/2016'\n   ,FORMAT ( @d, 'd', 'de-de' ) AS 'German Result' -- Returns '31.07.2016'\n   ,FORMAT ( @d, 'd', 'zh-cn' ) AS 'Simplified Chinese (PRC) Result' -- Returns '2016/7/31'\n   ,FORMAT ( @d, 'D', 'en-US' ) AS 'US English Result' -- Returns 'Sunday, July 31, 2016'\n   ,FORMAT ( @d, 'D', 'en-gb' ) AS 'Great Britain English Result' -- Returns '31 July 2016'\n   ,FORMAT ( @d, 'D', 'de-de' ) AS 'German Result' -- Returns 'Sonntag, 31. Juli 2016'\nUsing custom format strings:\nSELECT FORMAT( @d, 'dd/MM/yyyy', 'en-US' ) AS 'DateTime Result' -- Returns '31/07/2016'\n      ,FORMAT(123456789,'###-##-####') AS 'Custom Number Result' -- Returns '123-45-6789',\n      ,FORMAT( @d,'dddd, MMMM dd, yyyy hh:mm:ss tt','en-US') AS 'US' -- Returns 'Sunday, July 31,\n2016 12:00:00 AM'\n      ,FORMAT( @d,'dddd, MMMM dd, yyyy hh:mm:ss tt','hi-IN') AS 'Hindi' -- Returns रवि◌व◌ार, ज◌ुल◌ाई\n31, 2016 12:00:00 प◌ूर◌्व◌ाह◌्\n      ,FORMAT ( @d, 'dddd', 'en-US' )  AS 'US' -- Returns 'Sunday'\n      ,FORMAT ( @d, 'dddd', 'hi-IN' )  AS 'Hindi' -- Returns 'रवि◌व◌ार'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 123\nFORMAT can also be used for formatting CURRENCY,PERCENTAGE and NUMBERS.\nCURRENCY\nDECLARE @Price1 INT = 40\nSELECT FORMAT(@Price1,'c','en-US') AS 'CURRENCY IN US Culture' -- Returns '$40.00'      \n       ,FORMAT(@Price1,'c','de-DE') AS 'CURRENCY IN GERMAN Culture' -- Returns '40,00 €'\nWe can specify the number of digits after the decimal.\nDECLARE @Price DECIMAL(5,3) = 40.356\nSELECT FORMAT( @Price, 'C') AS 'Default', -- Returns '$40.36'\n       FORMAT( @Price, 'C0') AS 'With 0 Decimal', -- Returns '$40'\n       FORMAT( @Price, 'C1') AS 'With 1 Decimal', -- Returns '$40.4'\n       FORMAT( @Price, 'C2') AS 'With 2 Decimal', -- Returns '$40.36'\nPERCENTAGE\n   DECLARE @Percentage float = 0.35674\n   SELECT FORMAT( @Percentage, 'P') AS '% Default', -- Returns '35.67 %'\n   FORMAT( @Percentage, 'P0') AS '% With 0 Decimal', -- Returns '36 %'\n   FORMAT( @Percentage, 'P1') AS '% with 1 Decimal'  -- Returns '35.7 %'\nNUMBER\nDECLARE @Number AS DECIMAL(10,2) = 454545.389\nSELECT FORMAT( @Number, 'N','en-US') AS 'Number Format in US', -- Returns '454,545.39'\nFORMAT( @Number, 'N','en-IN')  AS 'Number Format in INDIA', -- Returns '4,54,545.39'\nFORMAT( @Number, '#.0')     AS 'With 1 Decimal', -- Returns '454545.4'\nFORMAT( @Number, '#.00')    AS 'With 2 Decimal', -- Returns '454545.39'\nFORMAT( @Number, '#,##.00') AS 'With Comma and 2 Decimal', -- Returns '454,545.39'\nFORMAT( @Number, '##.00')   AS 'Without Comma and 2 Decimal', -- Returns '454545.39'\nFORMAT( @Number, '000000000') AS 'Left-padded to nine digits' -- Returns '000454545'\nValid value types list: (source)\nCategory         Type             .Net type\n-------------------------------------------\nNumeric          bigint           Int64\nNumeric          int              Int32\nNumeric          smallint         Int16\nNumeric          tinyint          Byte\nNumeric          decimal          SqlDecimal\nNumeric          numeric          SqlDecimal\nNumeric          float            Double\nNumeric          real             Single\nNumeric          smallmoney       Decimal\nNumeric          money            Decimal\nDate and Time    date             DateTime\nDate and Time    time             TimeSpan\nDate and Time    datetime         DateTime\nDate and Time    smalldatetime    DateTime\nDate and Time    datetime2        DateTime\nDate and Time    datetimeoffset   DateTimeOffset\nImportant Notes:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 124\nFORMAT returns NULL for errors other than a culture that is not valid. For example, NULL is returned if the value\nspeciﬁed in format is not valid.\nFORMAT relies on the presence of the .NET Framework Common Language Runtime (CLR).\nFORMAT relies upon CLR formatting rules which dictate that colons and periods must be escaped. Therefore,\nwhen the format string (second parameter) contains a colon or period, the colon or period must be escaped\nwith backslash when an input value (ﬁrst parameter) is of the time data type.\nSee also Date & Time Formatting using FORMAT documentation example.\nSection 41.9: String_escape\nVersion ≥ SQL Server 2016\nEscapes special characters in texts and returns text (nvarchar(max)) with escaped characters.\nParameters:\ntext. is a nvarchar expression representing the string that should be escaped.1.\ntype. Escaping rules that will be applied. Currently the only supported value is 'json'.2.\nSELECT STRING_ESCAPE('\\   /  \n\\\\    \"     ', 'json') -- returns '\\\\\\t\\/\\n\\\\\\\\\\t\\\"\\t'\nList of characters that will be escaped:\nSpecial character    Encoded sequence\n-------------------------------------\nQuotation mark (\")   \\\"\nReverse solidus (\\)  \\\\\nSolidus (/)          \\/\nBackspace            \\b\nForm feed            \\f\nNew line             \\n\nCarriage return      \\r\nHorizontal tab       \\t\nControl character    Encoded sequence\n------------------------------------\nCHAR(0)            \\u0000\nCHAR(1)            \\u0001\n...                ...\nCHAR(31)           \\u001f\nSection 41.10: ASCII\nReturns an int value representing the ASCII code of the leftmost character of a string.\nSELECT ASCII('t') -- Returns 116\nSELECT ASCII('T') -- Returns 84\nSELECT ASCII('This') -- Returns 84\nIf the string is Unicode and the leftmost character is not ASCII but representable in the current collation, a value\ngreater than 127 can be returned:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 125\nSELECT ASCII(N'ï') -- returns 239 when `SERVERPROPERTY('COLLATION') =\n'SQL_Latin1_General_CP1_CI_AS'`\nIf the string is Unicode and the leftmost character cannot be represented in the current collation, the int value of 63\nis returned: (which represents question mark in ASCII):\nSELECT ASCII(N'\u0000') -- returns 63\nSELECT ASCII(nchar(2039)) -- returns 63\nSection 41.11: Char\nReturns a char represented by an int ASCII code.\nSELECT CHAR(116) -- Returns 't'\nSELECT CHAR(84)  -- Returns 'T'\nThis can be used to introduce new line/line feed CHAR(10), carriage returns CHAR(13), etc. See AsciiTable.com for\nreference.\nIf the argument value is not between 0 and 255, the CHAR function returns NULL.\nThe return data type of the CHAR function is char(1)\nSection 41.12: Concat\nVersion ≥ SQL Server 2012\nReturns a string that is the result of two or more strings joined together. CONCAT accepts two or more arguments.\nSELECT CONCAT('This', ' is', ' my', ' string') -- returns 'This is my string'\nNote: Unlike concatenating strings using the string concatenation operator (+), when passing a null value to the\nconcat function it will implicitly convert it to an empty string:\nSELECT CONCAT('This', NULL, ' is', ' my', ' string'), -- returns 'This is my string'\n       'This' + NULL + ' is' + ' my' + ' string' -- returns NULL.\nAlso arguments of a non-string type will be implicitly converted to a string:\nSELECT CONCAT('This', ' is my ', 3, 'rd string') -- returns 'This is my 3rd string'\nNon-string type variables will also be converted to string format, no need to manually covert or cast it to string:\nDECLARE @Age INT=23;\nSELECT CONCAT('Ram is ', @Age,' years old');  -- returns 'Ram is 23 years old'\nVersion < SQL Server 2012\nOlder versions do not support CONCAT function and must use the string concatenation operator (+) instead. Non-\nstring types must be cast or converted to string types in order to concatenate them this way.\nSELECT 'This is the number ' + CAST(42 AS VARCHAR(5)) --returns 'This is the number 42'\nSection 41.13: LTrim\nReturns a character expression (varchar or nvarchar) after removing all leading white spaces, i.e., white spaces\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 126\nfrom the left through to the ﬁrst non-white space character.\nParameters:\ncharacter expression. Any expression of character or binary data that can be implicitly converted to varcher,1.\nexcept text, ntext and image.\nSELECT LTRIM('    This is my string') -- Returns 'This is my string'\nSection 41.14: RTrim\nReturns a character expression (varchar or nvarchar) after removing all trailing white spaces, i.e., spaces from the\nright end of the string up until the ﬁrst non-white space character to the left.\nParameters:\ncharacter expression. Any expression of character or binary data that can be implicitly converted to varcher,1.\nexcept text, ntext and image.\nSELECT RTRIM('This is my string     ') -- Returns 'This is my string'\nSection 41.15: PatIndex\nReturns the starting position of the ﬁrst occurrence of a the speciﬁed pattern in the speciﬁed expression.\nParameters:\npattern. A character expression the contains the sequence to be found. Limited to A maximum length of1.\n8000 chars. Wildcards (%, _) can be used in the pattern. If the pattern does not start with a wildcard, it may\nonly match whatever is in the beginning of the expression. If it doesn't end with a wildcard, it may only match\nwhatever is in the end of the expression.\nexpression. Any string data type.2.\nSELECT PATINDEX('%ter%', 'interesting') -- Returns 3.\nSELECT PATINDEX('%t_r%t%', 'interesting') -- Returns 3.\nSELECT PATINDEX('ter%', 'interesting') -- Returns 0, since 'ter' is not at the start.\nSELECT PATINDEX('inter%', 'interesting') -- Returns 1.\nSELECT PATINDEX('%ing', 'interesting') -- Returns 9.\nSection 41.16: Space\nReturns a string (varchar) of repeated spaces.\nParameters:\ninteger expression. Any integer expression, up to 8000. If negative, null is returned. if 0, an empty string is1.\nreturned. (To return a string longer then 8000 spaces, use Replicate.\nSELECT SPACE(-1) -- Returns NULL\nSELECT SPACE(0)  -- Returns an empty string\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 127\nSELECT SPACE(3)  -- Returns '   ' (a string containing 3 spaces)\nSection 41.17: Dierence\nReturns an integer (int) value that indicates the diﬀerence between the soundex values of two character\nexpressions.\nParameters:\ncharacter expression 1.1.\ncharacter expression 2.2.\nBoth parameters are alphanumeric expressions of character data.\nThe integer returned is the number of chars in the soundex values of the parameters that are the same, so 4 means\nthat the expressions are very similar and 0 means that they are very diﬀerent.\nSELECT  SOUNDEX('Green'),  -- G650\n        SOUNDEX('Greene'),  -- G650\n        DIFFERENCE('Green','Greene') -- Returns 4\n       \nSELECT  SOUNDEX('Blotchet-Halls'),  -- B432\n        SOUNDEX('Greene'),  -- G650\n        DIFFERENCE('Blotchet-Halls', 'Greene') -- Returns 0\nSection 41.18: Len\nReturns the number of characters of a string.\nNote: the LEN function ignores trailing spaces:\nSELECT LEN('My string'), -- returns 9\n       LEN('My string   '), -- returns 9\n       LEN('   My string') -- returns 12\nIf the length including trailing spaces is desired there are several techniques to achieve this, although each has its\ndrawbacks. One technique is to append a single character to the string, and then use the LEN minus one:\nDECLARE @str varchar(100) = 'My string   '\nSELECT LEN(@str + 'x') - 1 -- returns 12\nThe drawback to this is if the type of the string variable or column is of the maximum length, the append of the\nextra character is discarded, and the resulting length will still not count trailing spaces. To address that, the\nfollowing modiﬁed version solves the problem, and gives the correct results in all cases at the expense of a small\namount of additional execution time, and because of this (correct results, including with surrogate pairs, and\nreasonable execution speed) appears to be the best technique to use:\nSELECT LEN(CONVERT(NVARCHAR(MAX), @str) + 'x') - 1\nAnother technique is to use theDATALENGTH function.\nDECLARE @str varchar(100) = 'My string   '\nSELECT DATALENGTH(@str) -- returns 12\nIt's important to note though that DATALENGTH returns the length in bytes of the string in memory. This will be\ndiﬀerent for varchar vs. nvarchar.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 128\nDECLARE @str nvarchar(100) = 'My string   '\nSELECT DATALENGTH(@str) -- returns 24\nYou can adjust for this by dividing the datalength of the string by the datalength of a single character (which must\nbe of the same type). The example below does this, and also handles the case where the target string happens to\nbe empty, thus avoiding a divide by zero.\nDECLARE @str nvarchar(100) = 'My string   '\nSELECT DATALENGTH(@str) / DATALENGTH(LEFT(LEFT(@str, 1) + 'x', 1)) -- returns 12\nEven this, though, has a problem in SQL Server 2012 and above. It will produce incorrect results when the string\ncontains surrogate pairs (some characters can occupy more bytes than other characters in the same string).\nAnother technique is to use REPLACE to convert spaces to a non-space character, and take the LEN of the result. This\ngives correct results in all cases, but has very poor execution speed with long strings.\nSection 41.19: Lower\nReturns a character expression (varchar or nvarchar) after converting all uppercase characters to lowercase.\nParameters:\nCharacter expression. Any expression of character or binary data that can be implicitly converted to varchar.1.\nSELECT LOWER('This IS my STRING') -- Returns 'this is my string'\nDECLARE @String nchar(17) = N'This IS my STRING';\nSELECT LOWER(@String) -- Returns 'this is my string'\nSection 41.20: Upper\nReturns a character expression (varchar or nvarchar) after converting all lowercase characters to uppercase.\nParameters:\nCharacter expression. Any expression of character or binary data that can be implicitly converted to varchar.1.\nSELECT UPPER('This IS my STRING') -- Returns 'THIS IS MY STRING'\nDECLARE @String nchar(17) = N'This IS my STRING';\nSELECT UPPER(@String) -- Returns 'THIS IS MY STRING'\nSection 41.21: Unicode\nReturns the integer value representing the Unicode value of the ﬁrst character of the input expression.\nParameters:\nUnicode character expression. Any valid nchar or nvarchar expression.1.\nSELECT UNICODE(N'Ɛ') -- Returns 400\nDECLARE @Unicode nvarchar(11) = N'Ɛ is a char'\nSELECT UNICODE(@Unicode) -- Returns 400\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 129\nSection 41.22: NChar\nReturns the Unicode character(s) (nchar(1) or nvarchar(2)) corresponding to the integer argument it receives, as\ndeﬁned by the Unicode standard.\nParameters:\ninteger expression. Any integer expression that is a positive number between 0 and 65535, or if the collation1.\nof the database supports supplementary character (CS) ﬂag, the supported range is between 0 to 1114111. If\nthe integer expression does not fall inside this range, null is returned.\nSELECT NCHAR(257) -- Returns 'ā'\nSELECT NCHAR(400) -- Returns 'Ɛ'\nSection 41.23: Str\nReturns character data (varchar) converted from numeric data.\nParameters:\nﬂoat expression. An approximate numeric data type with a decimal point.1.\nlength. optional. The total length of the string expression that would return, including digits, decimal point2.\nand leading spaces (if needed). The default value is 10.\ndecimal. optional. The number of digits to the right of the decimal point. If higher then 16, the result would3.\nbe truncated to sixteen places to the right of the decimal point.\nSELECT STR(1.2) -- Returns '         1'\nSELECT STR(1.2, 3) -- Returns '  1'\nSELECT STR(1.2, 3, 2) -- Returns '1.2'\nSELECT STR(1.2, 5, 2) -- Returns ' 1.20'\nSELECT STR(1.2, 5, 5) -- Returns '1.200'\nSELECT STR(1, 5, 2) -- Returns ' 1.00'\nSELECT STR(1) -- Returns '         1'\nSection 41.24: Reverse\nReturns a string value in reversed order.\nParameters:\nstring expression. Any string or binary data that can be implicitly converted to varchar.1.\nSelect REVERSE('Sql Server') -- Returns 'revreS lqS'\nSection 41.25: Replicate\nRepeats a string value a speciﬁed number of times.\nParameters:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 130\nstring expression. String expression can be a character string or binary data.1.\ninteger expression. Any integer type, including bigint. If negative, null is returned. If 0, an empty string is2.\nreturned.\nSELECT REPLICATE('a', -1)  -- Returns NULL\nSELECT REPLICATE('a', 0)  -- Returns ''\nSELECT REPLICATE('a', 5)  -- Returns 'aaaaa'\nSELECT REPLICATE('Abc', 3) -- Returns 'AbcAbcAbc'\nNote: If string expression is not of type varchar(max) or nvarchar(max), the return value will not exceed 8000\nchars. Replicate will stop before adding the string that will cause the return value to exceed that limit:\nSELECT LEN(REPLICATE('a b c d e f g h i j k l', 350)) -- Returns 7981\nSELECT LEN(REPLICATE(cast('a b c d e f g h i j k l' as varchar(max)), 350)) -- Returns 8050\nSection 41.26: CharIndex\nReturns the start index of a the ﬁrst occurrence of string expression inside another string expression.\nParameters list:\nString to ﬁnd (up to 8000 chars)1.\nString to search (any valid character data type and length, including binary)2.\n(Optional) index to start. A number of type int or big int. If omitted or less then 1, the search starts at the3.\nbeginning of the string.\nIf the string to search is varchar(max), nvarchar(max) or varbinary(max), the CHARINDEX function will return a\nbigint value. Otherwise, it will return an int.\nSELECT CHARINDEX('is', 'this is my string') -- returns 3\nSELECT CHARINDEX('is', 'this is my string', 4) -- returns 6\nSELECT CHARINDEX(' is', 'this is my string') -- returns 5\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 131\nChapter 42: Logical Functions\nSection 42.1: CHOOSE\nVersion ≥ SQL Server 2012\nReturns the item at the speciﬁed index from a list of values. If index exceeds the bounds of values then NULL is\nreturned.\nParameters:\nindex: integer, index to item in values. 1-based.1.\nvalues: any type, comma separated list2.\nSELECT CHOOSE (1, 'apples', 'pears', 'oranges', 'bananas') AS chosen_result\nchosen_result\n-------------\napples\nSection 42.2: IIF\nVersion ≥ SQL Server 2012\nReturns one of two values, depending on whether a given Boolean expression evaluates to true or false.\nParameters:\nboolean_expression evaluated to dtermine what value to return1.\ntrue_value returned if boolean_expression evaluates to true2.\nfalse_value returned if boolean_expression evaluates to false3.\nSELECT IIF (42 > 23, 'I knew that!', 'That is not true.') AS iif_result\niif_result\n------------\nI knew that!\nVersion < SQL Server 2012\nIIF may be replaced by a CASE statement. The above example my be written as\nSELECT CASE WHEN 42 > 23 THEN 'I knew that!' ELSE 'That is not true.' END AS iif_result\niif_result\n------------\nI knew that!\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 132\nChapter 43: Aggregate Functions\nAggregate functions in SQL Server run calculations on sets of values, returning a single value.\nSection 43.1: SUM()\nReturns sum of numeric values in a given column.\nWe have table as shown in ﬁgure that will be used to perform diﬀerent aggregate functions. The table name is\nMarksheet.\nSelect SUM(MarksObtained) From Marksheet\nThe sum function doesn't consider rows with NULL value in the ﬁeld used as parameter\nIn the above example if we have another row like this:\n106    Italian    NULL\nThis row will not be consider in sum calculation\nSection 43.2: AVG()\nReturns average of numeric values in a given column.\nWe have table as shown in ﬁgure that will be used to perform diﬀerent aggregate functions. The table name is\nMarksheet.\nSelect AVG(MarksObtained) From Marksheet\nThe average function doesn't consider rows with NULL value in the ﬁeld used as parameter\nIn the above example if we have another row like this:\n106    Italian    NULL\nThis row will not be consider in average calculation\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 133\nSection 43.3: MAX()\nReturns the largest value in a given column.\nWe have table as shown in ﬁgure that will be used to perform diﬀerent aggregate functions. The table name is\nMarksheet.\nSelect MAX(MarksObtained) From Marksheet\nSection 43.4: MIN()\nReturns the smallest value in a given column.\nWe have table as shown in ﬁgure that will be used to perform diﬀerent aggregate functions. The table name is\nMarksheet.\nSelect MIN(MarksObtained) From Marksheet\nSection 43.5: COUNT()\nReturns the total number of values in a given column.\nWe have table as shown in ﬁgure that will be used to perform diﬀerent aggregate functions. The table name is\nMarksheet.\nSelect COUNT(MarksObtained) From Marksheet\nThe count function doesn't consider rows with NULL value in the ﬁeld used as parameter. Usually the count\nparameter is * (all ﬁelds) so only if all ﬁelds of row are NULLs this row will not be considered\nIn the above example if we have another row like this:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 134\n106    Italian    NULL\nThis row will not be consider in count calculation\nNOTE\nThe function COUNT(*) returns the number of rows in a table. This value can also be obtained by using a constant\nnon-null expression that contains no column references, such as COUNT(1).\nExample\nSelect COUNT(1) From Marksheet\nSection 43.6: COUNT(Column_Name) with GROUP BY\nColumn_Name\nMost of the time we like to get the total number of occurrence of a column value in a table for example:\nTABLE NAME : REPORTS\nReportName ReportPrice\nTest 10.00 $\nTest 10.00 $\nTest 10.00 $\nTest 2 11.00 $\nTest 10.00 $\nTest 3 14.00 $\nTest 3 14.00 $\nTest 4 100.00 $\nSELECT  \n    ReportName AS REPORT NAME,\n    COUNT(ReportName) AS COUNT\nFROM    \n    REPORTS\nGROUP BY\n    ReportName\nREPORT NAME COUNT\nTest 4\nTest 2 1\nTest 3 2\nTest 4 1\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 135\nChapter 44: String Aggregate functions in\nSQL Server\nSection 44.1: Using STUFF for string aggregation\nWe have a Student table with SubjectId. Here the requirement is to concatenate based on subjectId.\nAll SQL Server versions\ncreate table #yourstudent (subjectid int, studentname varchar(10))\ninsert into #yourstudent (subjectid, studentname) values\n ( 1       ,'Mary'    )\n,( 1       ,'John'    )\n,( 1       ,'Sam'    )\n,( 2       ,'Alaina')\n,( 2       ,'Edward')\nselect subjectid,  stuff(( select concat( ',', studentname) from #yourstudent y where y.subjectid =\nu.subjectid for xml path('')),1,1, '')\n    from #yourstudent u\n    group by subjectid\nSection 44.2: String_Agg for String Aggregation\nIn case of SQL Server 2017 or vnext we can use in-built STRING_AGG for this aggregation. For same student table,\ncreate table #yourstudent (subjectid int, studentname varchar(10))\ninsert into #yourstudent (subjectid, studentname) values\n ( 1       ,'Mary'    )\n,( 1       ,'John'    )\n,( 1       ,'Sam'    )\n,( 2       ,'Alaina')\n,( 2       ,'Edward')\nselect subjectid, string_agg(studentname, ',') from #yourstudent\n    group by subjectid\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 136\nChapter 45: Ranking Functions\nArguments Details\n<partition_by_clause>\nDivides the result set produced by the FROM clause into partitions to which the\nDENSE_RANK function is applied. For the PARTITION BY syntax, see OVER Clause\n(Transact-SQL).\n<order_by_clause> Determines the order in which the DENSE_RANK function is applied to the rows in\na partition.\nOVER ( [ partition_by_clause\n] order_by_clause)\npartition_by_clause divides the result set produced by the FROM clause into\npartitions to which the function is applied. If not speciﬁed, the function treats all\nrows of the query result set as a single group. order_by_clause determines the\norder of the data before the function is applied. The order_by_clause is required.\nThe <rows or range clause> of the OVER clause cannot be speciﬁed for the RANK\nfunction. For more information, see OVER Clause (Transact-SQL).\nSection 45.1: DENSE_RANK ()\nSame as that of RANK(). It returns rank without any gaps:\nSelect  Studentid, Name,Subject,Marks,\nDENSE_RANK() over(partition by name order by Marks desc)Rank\nFrom Exam\norder by name\nStudentid    Name    Subject    Marks    Rank\n101          Ivan    Science    80       1\n101          Ivan    Maths      70       2\n101          Ivan    Social     60       3\n102          Ryan    Social     70       1\n102          Ryan    Maths      60       2\n102          Ryan    Science    50       3\n103          Tanvi   Maths      90       1\n103          Tanvi   Science    90       1\n103          Tanvi   Social     80       2\nSection 45.2: RANK()\nA RANK() Returns the rank of each row in the result set of partitioned column.\nEg :\nSelect Studentid,Name,Subject,Marks,\nRANK() over(partition by name order by Marks desc)Rank\nFrom Exam\norder by name,subject\n   Studentid    Name    Subject    Marks    Rank\n    101         Ivan    Maths       70       2\n    101         Ivan    Science     80       1\n    101         Ivan    Social      60       3\n    102         Ryan    Maths       60       2\n    102         Ryan    Science     50       3\n    102         Ryan    Social      70       1\n    103         Tanvi   Maths       90       1\n    103         Tanvi   Science     90       1\n    103         Tanvi   Social      80       3\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 137\nChapter 46: Window functions\nSection 46.1: Centered Moving Average\nCalculate a 6-month (126-business-day) centered moving average of a price:\nSELECT TradeDate, AVG(Px) OVER (ORDER BY TradeDate ROWS BETWEEN 63 PRECEDING AND 63 FOLLOWING) AS\nPxMovingAverage\nFROM HistoricalPrices\nNote that, because it will take up to 63 rows before and after each returned row, at the beginning and end of the\nTradeDate range it will not be centered: When it reaches the largest TradeDate it will only be able to ﬁnd 63\npreceding values to include in the average.\nSection 46.2: Find the single most recent item in a list of\ntimestamped events\nIn tables recording events there is often a datetime ﬁeld recording the time an event happened. Finding the single\nmost recent event can be diﬃcult because it's always possible that two events were recorded with exactly identical\ntimestamps. You can use row_number() over (order by ...) to make sure all records are uniquely ranked, and select\nthe top one (where my_ranking=1)\nselect *\nfrom (\n    select\n        *,\n        row_number() over (order by crdate desc) as my_ranking\n    from sys.sysobjects\n) g\nwhere my_ranking=1\nThis same technique can be used to return a single row from any dataset with potentially duplicate values.\nSection 46.3: Moving Average of last 30 Items\nMoving Average of last 30 Items sold\nSELECT\n    value_column1,\n    (   SELECT\n            AVG(value_column1) AS moving_average\n        FROM Table1 T2\n        WHERE ( SELECT\n                    COUNT(*)\n                FROM Table1 T3\n                WHERE date_column1 BETWEEN T2.date_column1 AND T1.date_column1\n                ) BETWEEN 1 AND 30\n    ) as MovingAvg\nFROM Table1 T1\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 138\nChapter 47: PIVOT / UNPIVOT\nSection 47.1: Dynamic PIVOT\nOne problem with the PIVOT query is that you have to specify all values inside the IN selection if you want to see\nthem as columns. A quick way to circumvent this problem is to create a dynamic IN selection making your PIVOT\ndynamic.\nFor demonstration we will use a table Books in a Bookstore’s database. We assume that the table is quite de-\nnormalised and has following columns\nTable: Books\n-----------------------------\nBookId (Primary Key Column)\nName\nLanguage\nNumberOfPages\nEditionNumber\nYearOfPrint\nYearBoughtIntoStore\nISBN\nAuthorName\nPrice\nNumberOfUnitsSold\nCreation script for the table will be like:\nCREATE TABLE [dbo].[BookList](\n      [BookId] [int] NOT NULL,\n      [Name] [nvarchar](100) NULL,\n      [Language] [nvarchar](100) NULL,\n      [NumberOfPages] [int] NULL,\n      [EditionNumber] [nvarchar](10) NULL,\n      [YearOfPrint] [int] NULL,\n      [YearBoughtIntoStore] [int] NULL,\n[NumberOfBooks] [int] NULL,\n[ISBN] [nvarchar](30) NULL,\n      [AuthorName] [nvarchar](200) NULL,\n      [Price] [money] NULL,\n      [NumberOfUnitsSold] [int] NULL,\n CONSTRAINT [PK_BookList] PRIMARY KEY CLUSTERED\n(\n      [BookId] ASC\n)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON,\nALLOW_PAGE_LOCKS = ON) ON [PRIMARY]\n) ON [PRIMARY]\nGO\nNow if we need to query on the database and ﬁgure out number of books in English, Russian, German, Hindi, Latin\nlanguages bought into the bookstore every year and present our output in a small report format, we can use PIVOT\nquery like this\nSELECT * FROM\n  (\n   SELECT YearBoughtIntoStore AS [Year Bought],[Language], NumberOfBooks\n   FROM BookList\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 139\n  ) sourceData\n PIVOT\n  (\n  SUM(NumberOfBooks)\n  FOR [Language] IN (English, Russian, German, Hindi, Latin)\n  ) pivotrReport\nSpecial case is when we do not have a full list of the languages, so we'll use dynamic SQL like below\nDECLARE @query VARCHAR(4000)\nDECLARE @languages VARCHAR(2000)\nSELECT @languages =\n        STUFF((SELECT DISTINCT '],['+LTRIM([Language])FROM [dbo].[BookList]\n        ORDER BY '],['+LTRIM([Language]) FOR XML PATH('') ),1,2,'') + ']'\nSET @query=\n'SELECT * FROM\n  (SELECT YearBoughtIntoStore AS [Year Bought],[Language],NumberOfBooks\n   FROM BookList) sourceData\nPIVOT(SUM(NumberOfBooks)FOR [Language] IN ('+ @languages +')) pivotrReport' EXECUTE(@query)\nSection 47.2: Simple PIVOT & UNPIVOT (T-SQL)\nBelow is a simple example which shows average item's price of each item per weekday.\nFirst, suppose we have a table which keeps daily records of all items' prices.\nCREATE TABLE tbl_stock(item NVARCHAR(10), weekday NVARCHAR(10), price INT);\nINSERT INTO tbl_stock VALUES\n('Item1', 'Mon', 110), ('Item2', 'Mon', 230), ('Item3', 'Mon', 150),\n('Item1', 'Tue', 115), ('Item2', 'Tue', 231), ('Item3', 'Tue', 162),\n('Item1', 'Wed', 110), ('Item2', 'Wed', 240), ('Item3', 'Wed', 162),\n('Item1', 'Thu', 109), ('Item2', 'Thu', 228), ('Item3', 'Thu', 145),\n('Item1', 'Fri', 120), ('Item2', 'Fri', 210), ('Item3', 'Fri', 125),\n('Item1', 'Mon', 122), ('Item2', 'Mon', 225), ('Item3', 'Mon', 140),\n('Item1', 'Tue', 110), ('Item2', 'Tue', 235), ('Item3', 'Tue', 154),\n('Item1', 'Wed', 125), ('Item2', 'Wed', 220), ('Item3', 'Wed', 142);\nThe table should look like below:\n+========+=========+=======+\n|   item | weekday | price |\n+========+=========+=======+\n|  Item1 |    Mon  |   110 |\n+--------+---------+-------+\n|  Item2 |    Mon  |   230 |\n+--------+---------+-------+\n|  Item3 |    Mon  |   150 |\n+--------+---------+-------+\n|  Item1 |    Tue  |   115 |\n+--------+---------+-------+\n|  Item2 |    Tue  |   231 |\n+--------+---------+-------+\n|  Item3 |    Tue  |   162 |\n+--------+---------+-------+\n|          . . .           |\n+--------+---------+-------+\n|  Item2 |    Wed  |   220 |\n+--------+---------+-------+\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 140\n|  Item3 |    Wed  |   142 |\n+--------+---------+-------+\nIn order to perform aggregation which is to ﬁnd the average price per item for each week day, we are going to use\nthe relational operator PIVOT to rotate the column weekday of table-valued expression into aggregated row values\nas below:\nSELECT * FROM tbl_stock\nPIVOT (\n    AVG(price) FOR weekday IN ([Mon], [Tue], [Wed], [Thu], [Fri])\n) pvt;\nResult:\n+--------+------+------+------+------+------+\n|  item  |  Mon |  Tue |  Wed |  Thu |  Fri |\n+--------+------+------+------+------+------+\n|  Item1 |  116 |  112 |  117 |  109 |  120 |\n|  Item2 |  227 |  233 |  230 |  228 |  210 |\n|  Item3 |  145 |  158 |  152 |  145 |  125 |\n+--------+------+------+------+------+------+\nLastly, in order to perform the reverse operation of PIVOT, we can use the relational operator UNPIVOT to rotate\ncolumns into rows as below:\nSELECT * FROM tbl_stock\nPIVOT (\n    AVG(price) FOR weekday IN ([Mon], [Tue], [Wed], [Thu], [Fri])\n) pvt\nUNPIVOT (\n    price FOR weekday IN ([Mon], [Tue], [Wed], [Thu], [Fri])\n) unpvt;\nResult:\n+=======+========+=========+\n|  item |  price | weekday |\n+=======+========+=========+\n| Item1 |    116 |     Mon |\n+-------+--------+---------+\n| Item1 |    112 |     Tue |\n+-------+--------+---------+\n| Item1 |    117 |     Wed |\n+-------+--------+---------+\n| Item1 |    109 |     Thu |\n+-------+--------+---------+\n| Item1 |    120 |     Fri |\n+-------+--------+---------+\n| Item2 |    227 |     Mon |\n+-------+--------+---------+\n| Item2 |    233 |     Tue |\n+-------+--------+---------+\n| Item2 |    230 |     Wed |\n+-------+--------+---------+\n| Item2 |    228 |     Thu |\n+-------+--------+---------+\n| Item2 |    210 |     Fri |\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 141\n+-------+--------+---------+\n| Item3 |    145 |     Mon |\n+-------+--------+---------+\n| Item3 |    158 |     Tue |\n+-------+--------+---------+\n| Item3 |    152 |     Wed |\n+-------+--------+---------+\n| Item3 |    145 |     Thu |\n+-------+--------+---------+\n| Item3 |    125 |     Fri |\n+-------+--------+---------+\nSection 47.3: Simple Pivot - Static Columns\nUsing Item Sales Table from Example Database, let us calculate and show the total Quantity we sold of each\nProduct.\nThis can be easily done with a group by, but lets assume we to 'rotate' our result table in a way that for each\nProduct Id we have a column.\nSELECT [100], [145]\n  FROM (SELECT ItemId , Quantity\n          FROM #ItemSalesTable\n       ) AS pivotIntermediate\n PIVOT (   SUM(Quantity)\n           FOR ItemId IN ([100], [145])\n       ) AS pivotTable\nSince our 'new' columns are numbers (in the source table), we need to square brackets []\nThis will give us an output like\n100 145\n45 18\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 142\nChapter 48: Dynamic SQL Pivot\nThis topic covers how to do a dynamic pivot in SQL Server.\nSection 48.1: Basic Dynamic SQL Pivot\nif object_id('tempdb.dbo.#temp') is not null drop table #temp\ncreate table #temp\n(\n    dateValue datetime,\n    category varchar(3),\n    amount decimal(36,2)\n)\ninsert into #temp values ('1/1/2012', 'ABC', 1000.00)\ninsert into #temp values ('2/1/2012', 'DEF', 500.00)\ninsert into #temp values ('2/1/2012', 'GHI', 800.00)\ninsert into #temp values ('2/10/2012', 'DEF', 700.00)\ninsert into #temp values ('3/1/2012', 'ABC', 1100.00)\nDECLARE\n    @cols AS NVARCHAR(MAX),\n    @query  AS NVARCHAR(MAX);\nSET @cols = STUFF((SELECT distinct ',' + QUOTENAME(c.category)\n            FROM #temp c\n            FOR XML PATH(''), TYPE\n            ).value('.', 'NVARCHAR(MAX)')\n        ,1,1,'')\nset @query = '\n            SELECT\n                dateValue,\n                ' + @cols + '\n            from\n            (\n                select\n                     dateValue,\n                     amount,\n                     category\n                from #temp\n           ) x\n            pivot\n            (\n                 sum(amount)\n                for category in (' + @cols + ')\n            ) p '\nexec sp_executeSql @query\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 143\nChapter 49: Partitioning\nSection 49.1: Retrieve Partition Boundary Values\nSELECT        ps.name AS PartitionScheme\n            , fg.name AS [FileGroup]\n            , prv.*            \n            , LAG(prv.Value) OVER (PARTITION BY ps.name ORDER BY ps.name, boundary_id) AS\nPreviousBoundaryValue\nFROM        sys.partition_schemes ps\nINNER JOIN    sys.destination_data_spaces dds\n            ON dds.partition_scheme_id = ps.data_space_id\nINNER JOIN    sys.filegroups fg\n            ON dds.data_space_id = fg.data_space_id\nINNER JOIN    sys.partition_functions f\n            ON f.function_id = ps.function_id\nINNER JOIN    sys.partition_range_values prv\n            ON f.function_id = prv.function_id\n            AND dds.destination_id = prv.boundary_id\nSection 49.2: Switching Partitions\nAccording to this [TechNet Microsoft page][1],\nPartitioning data enables you to manage and access subsets of your data quickly and eﬃciently while\nmaintaining the integrity of the entire data collection.\nWhen you call the following query the data is not physically moved; only the metadata about the location of the\ndata changes.\nALTER TABLE [SourceTable] SWITCH TO [TargetTable]\nThe tables must have the same columns with the same data types and NULL settings, they need to be in the same\nﬁle group and the new target table must be empty. See the page link above for more info on switching partitions.\n[1]: https://technet.microsoft.com/en-us/library/ms191160(v=sql.105).aspx The column IDENTITY property may\ndiﬀer.\nSection 49.3: Retrieve partition table,column, scheme,\nfunction, total and min-max boundry values using single\nquery\nSELECT DISTINCT\n    object_name(i.object_id) AS [Object Name],\n    c.name AS [Partition Column],\n    s.name AS [Partition Scheme],\n    pf.name AS [Partition Function],\n    prv.tot AS [Partition Count],\n    prv.miVal AS [Min Boundry Value],\n    prv.maVal AS [Max Boundry Value]\nFROM sys.objects o\nINNER JOIN sys.indexes i ON i.object_id = o.object_id\nINNER JOIN sys.columns c ON c.object_id = o.object_id\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 144\nINNER JOIN sys.index_columns ic ON ic.object_id = o.object_id\n    AND ic.column_id = c.column_id\n    AND ic.partition_ordinal = 1\nINNER JOIN sys.partition_schemes s ON i.data_space_id = s.data_space_id\nINNER JOIN sys.partition_functions pf ON pf.function_id = s.function_id\nOUTER APPLY(SELECT\n                COUNT(*) tot, MIN(value) miVal, MAX(value) maVal\n            FROM sys.partition_range_values prv\n            WHERE prv.function_id = pf.function_id) prv\n--WHERE object_name(i.object_id) = 'table_name'\nORDER BY OBJECT_NAME(i.object_id)\nJust un-comment where clause and replace table_name with actual table name to view the detail of desired object.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 145\nChapter 50: Stored Procedures\nIn SQL Server, a procedure is a stored program that you can pass parameters into. It does not return a value like a\nfunction does. However, it can return a success/failure status to the procedure that called it.\nSection 50.1: Creating and executing a basic stored procedure\nUsing the Authors table in the Library Database\nCREATE PROCEDURE GetName\n(\n    @input_id INT = NULL,      --Input parameter,  id of the person, NULL default\n    @name VARCHAR(128) = NULL  --Input parameter, name of the person, NULL default\n)\nAS\nBEGIN\n    SELECT Name + ' is from ' + Country\n    FROM Authors\n    WHERE Id = @input_id OR Name = @name\nEND\nGO\nYou can execute a procedure with a few diﬀerent syntaxes. First, you can use EXECUTE or EXEC\nEXECUTE GetName @id = 1\nEXEC Getname @name = 'Ernest Hemingway'\nAdditionally, you can omit the EXEC command. Also, you don't have to specify what parameter you are passing in,\nas you pass in all parameters.\nGetName NULL, 'Ernest Hemingway'\nWhen you want to specify the input parameters in a diﬀerent order than how they are declared in the procedure\nyou can specify the parameter name and assign values. For example\n CREATE PROCEDURE dbo.sProcTemp\n (\n    @Param1 INT,\n    @Param2 INT\n)\nAS\nBEGIN\n    SELECT\n        Param1 = @Param1,\n        Param2 = @Param2\nEND\nthe normal order to execute this procedure is to specify the value for @Param1 ﬁrst and then @Param2 second. So\nit will look something like this\n  EXEC dbo.sProcTemp @Param1 = 0,@Param2=1\nBut it's also possible that you can use the following\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 146\n  EXEC dbo.sProcTemp @Param2 = 0,@Param1=1\nin this, you are specifying the value for @param2 ﬁrst and @Param1 second. Which means you do not have to keep\nthe same order as it is declared in the procedure but you can have any order as you wish. but you will need to\nspecify to which parameter you are setting the value\nAccess stored procedure from any database\nAnd also you can create a procedure with a preﬁx sp_ these procuedres, like all system stored procedures, can be\nexecuted without specifying the database because of the default behavior of SQL Server. When you execute a\nstored procedure that starts with \"sp_\", SQL Server looks for the procedure in the master database ﬁrst. If the\nprocedure is not found in master, it looks in the active database. If you have a stored procedure that you want to\naccess from all your databases, create it in master and use a name that includes the \"sp_\" preﬁx.\nUse Master\nCREATE PROCEDURE sp_GetName\n(\n    @input_id INT = NULL,      --Input parameter,  id of the person, NULL default\n    @name VARCHAR(128) = NULL  --Input parameter, name of the person, NULL default\n)\nAS\nBEGIN\n    SELECT Name + ' is from ' + Country\n    FROM Authors\n    WHERE Id = @input_id OR Name = @name\nEND\nGO\nSection 50.2: Stored Procedure with If...Else and Insert Into\noperation\nCreate example table Employee:\nCREATE TABLE Employee\n(\n    Id INT,\n    EmpName VARCHAR(25),\n    EmpGender VARCHAR(6),\n    EmpDeptId INT\n)\nCreates stored procedure that checks whether the values passed in stored procedure are not null or non empty\nand perform insert operation in Employee table.\nCREATE PROCEDURE spSetEmployeeDetails\n(\n    @ID int,\n    @Name VARCHAR(25),\n    @Gender VARCHAR(6),\n    @DeptId INT\n)\nAS\nBEGIN\n    IF (\n        (@ID IS NOT NULL AND LEN(@ID) !=0)\n        AND (@Name IS NOT NULL AND LEN(@Name) !=0)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 147\n        AND (@Gender IS NOT NULL AND LEN(@Gender) !=0)\n        AND (@DeptId IS NOT NULL AND LEN(@DeptId) !=0)\n    )\n    BEGIN\n        INSERT INTO Employee\n        (\n            Id,\n            EmpName,\n            EmpGender,\n            EmpDeptId\n        )\n        VALUES\n        (\n            @ID,\n            @Name,\n            @Gender,\n            @DeptId\n        )\n    END\nELSE\n    PRINT 'Incorrect Parameters'\nEND\nGO\nExecute the stored procedure\nDECLARE @ID INT,\n    @Name VARCHAR(25),\n    @Gender VARCHAR(6),\n    @DeptId INT\nEXECUTE spSetEmployeeDetails\n    @ID = 1,\n    @Name = 'Subin Nepal',\n    @Gender = 'Male',\n    @DeptId = 182666\nSection 50.3: Dynamic SQL in stored procedure\nDynamic SQL enables us to generate and run SQL statements at run time. Dynamic SQL is needed when our SQL\nstatements contains identiﬁer that may change at diﬀerent compile times.\nSimple Example of dynamic SQL:\nCREATE PROC sp_dynamicSQL\n@table_name      NVARCHAR(20),\n@col_name        NVARCHAR(20),\n@col_value       NVARCHAR(20)\nAS\nBEGIN\nDECLARE  @Query  NVARCHAR(max)\nSET      @Query = 'SELECT * FROM ' + @table_name\nSET      @Query = @Query + ' WHERE ' + @col_name + ' = ' + ''''+@col_value+''''\nEXEC     (@Query)\nEND\nIn the above sql query, we can see that we can use above query by deﬁning values in @table_name, @col_name,\nand @col_value at run time. The query is generated at runtime and executed. This is technique in which we can\ncreate whole scripts as string in a variable and execute it. We can create more complex queries using dynamic SQL\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 148\nand concatenation concept. This concept is very powerful when you want to create a script that can be used under\nseveral conditions.\nExecuting stored procedure\nDECLARE @table_name     NVARCHAR(20) = 'ITCompanyInNepal',\n        @col_name       NVARCHAR(20) = 'Headquarter',\n        @col_value      NVARCHAR(20) = 'USA'\n   \nEXEC    sp_dynamicSQL   @table_name,    \n                        @col_name,\n                        @col_value\nTable I have used\nOutput\nSection 50.4: STORED PROCEDURE with OUT parameters\nStored procedures can return values using the OUTPUT keyword in its parameter list.\nCreating a stored procedure with a single out parameter\nCREATE PROCEDURE SprocWithOutParams\n(\n    @InParam VARCHAR(30),\n    @OutParam VARCHAR(30) OUTPUT\n)\nAS\nBEGIN\n    SELECT @OutParam = @InParam + ' must come out'  \n    RETURN\nEND  \nGO\nExecuting the stored procedure\nDECLARE @OutParam VARCHAR(30)    \nEXECUTE SprocWithOutParams 'what goes in', @OutParam OUTPUT  \nPRINT @OutParam\nCreating a stored procedure with multiple out parameters\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 149\nCREATE PROCEDURE SprocWithOutParams2\n(\n    @InParam VARCHAR(30),\n    @OutParam VARCHAR(30) OUTPUT,\n    @OutParam2 VARCHAR(30) OUTPUT\n)\nAS\nBEGIN\n    SELECT @OutParam = @InParam +' must come out'  \n    SELECT @OutParam2 = @InParam +' must come out'\n    RETURN\nEND    \nGO\nExecuting the stored procedure\nDECLARE @OutParam VARCHAR(30)    \nDECLARE @OutParam2 VARCHAR(30)  \nEXECUTE SprocWithOutParams2 'what goes in', @OutParam OUTPUT, @OutParam2 OUTPUT  \nPRINT @OutParam\nPRINT @OutParam2\nSection 50.5: Simple Looping\nFirst lets get some data into a temp table named #systables and ad a incrementing row number so we can query\none record at a time\nselect\n    o.name,\n    row_number() over (order by o.name) as rn\ninto\n    #systables\nfrom\n    sys.objects as o\nwhere\n    o.type = 'S'\nNext we declare some variables to control the looping and store the table name in this example\ndeclare\n    @rn int = 1,\n    @maxRn int = (\n                    select\n                        max(rn)\n                    from\n                        #systables as s\n                    )\ndeclare    @tablename sys name\nNow we can loop using a simple while. We increment @rn in the select statement but this could also have been a\nseparate statement for ex set @rn = @rn + 1 it will depend on your requirements. We also use the value of @rn\nbefore it's incremented to select a single record from #systables. Lastly we print the table name.\nwhile @rn <= @maxRn\n    begin\n        select\n            @tablename = name,\n            @rn = @rn + 1\n        from\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 150\n            #systables as s\n        where\n            s.rn = @rn\n        print @tablename\n    end\nSection 50.6: Simple Looping\nCREATE PROCEDURE SprocWithSimpleLoop\n(\n    @SayThis VARCHAR(30),\n    @ThisManyTimes INT\n)\nAS\nBEGIN\n    WHILE @ThisManyTimes > 0\n    BEGIN\n        PRINT @SayThis;\n        SET @ThisManyTimes = @ThisManyTimes - 1;\n    END\n   \n    RETURN;\nEND    \nGO\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 151\nChapter 51: Retrieve information about the\ndatabase\nSection 51.1: Retrieve a List of all Stored Procedures\nThe following queries will return a list of all Stored Procedures in the database, with basic information about each\nStored Procedure:\nVersion ≥ SQL Server 2005\nSELECT *\nFROM INFORMATION_SCHEMA.ROUTINES\nWHERE ROUTINE_TYPE = 'PROCEDURE'\nThe ROUTINE_NAME, ROUTINE_SCHEMA and ROUTINE_DEFINITION columns are generally the most useful.\nVersion ≥ SQL Server 2005\nSELECT *\nFROM sys.objects\nWHERE type = 'P'\nVersion ≥ SQL Server 2005\nSELECT *\nFROM sys.procedures\nNote that this version has an advantage over selecting from sys.objects since it includes the additional columns\nis_auto_executed, is_execution_replicated, is_repl_serializable, and skips_repl_constraints.\nVersion < SQL Server 2005\nSELECT *\nFROM sysobjects\nWHERE type = 'P'\nNote that the output contains many columns that will never relate to a stored procedure.\nThe next set of queries will return all Stored Procedures in the database that include the string 'SearchTerm':\nVersion < SQL Server 2005\nSELECT o.name\nFROM syscomments c\nINNER JOIN sysobjects o\n    ON c.id=o.id\nWHERE o.xtype = 'P'\n    AND c.TEXT LIKE '%SearchTerm%'\nVersion ≥ SQL Server 2005\nSELECT p.name\nFROM sys.sql_modules AS m\nINNER JOIN sys.procedures AS p\n    ON m.object_id = p.object_id\nWHERE definition LIKE '%SearchTerm%'\nSection 51.2: Get the list of all databases on a server\nMethod 1: Below query will be applicable for SQL Server 2000+ version (Contains 12 columns)\nSELECT * FROM dbo.sysdatabases\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 152\nMethod 2: Below query extract information about databases with more information (eg: State, Isolation, recovery\nmodel etc.)\nNote: This is a catalog view and will be available SQL SERVER 2005+ versions\nSELECT * FROM sys.databases\nMethod 3: To see just database names you can use undocumented sp_MSForEachDB\nEXEC sp_MSForEachDB 'SELECT ''?'' AS DatabaseName'\nMethod 4: Below SP will help you to provide database size along with databases name , owner, status etc. on the\nserver\nEXEC sp_helpdb\nMethod 5 Similarly, below stored procedure will give database name, database size and Remarks\nEXEC sp_databases\nSection 51.3: Count the Number of Tables in a Database\nThis query will return the number of tables in the speciﬁed database.\nUSE YourDatabaseName\nSELECT COUNT(*) from INFORMATION_SCHEMA.TABLES\nWHERE TABLE_TYPE = 'BASE TABLE'\nFollowing is another way this can be done for all user tables with SQL Server 2008+. The reference is here.\nSELECT COUNT(*) FROM sys.tables\nSection 51.4: Database Files\nDisplay all data ﬁles for all databases with size and growth info\nSELECT  d.name AS 'Database',\n        d.database_id,  \n        SF.fileid,\n        SF.name AS 'LogicalFileName',  \n        CASE SF.status & 0x100000  \n            WHEN 1048576 THEN 'Percentage'\n            WHEN 0 THEN 'MB'\n        END AS 'FileGrowthOption',\n        Growth AS GrowthUnit,\n        ROUND(((CAST(Size AS FLOAT)*8)/1024)/1024,2) [SizeGB], -- Convert 8k pages to GB\n        Maxsize,        \n        filename AS PhysicalFileName\nFROM    Master.SYS.SYSALTFILES SF\nJoin    Master.SYS.Databases d on sf.fileid = d.database_id\nOrder by d.name\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 153\nSection 51.5: See if Enterprise-speciﬁc features are being used\nIt is sometimes useful to verify that your work on Developer edition hasn't introduced a dependency on any\nfeatures restricted to Enterprise edition.\nYou can do this using the sys.dm_db_persisted_sku_features system view, like so:\nSELECT * FROM sys.dm_db_persisted_sku_features\nAgainst the database itself.\nThis will list the features being used, if any.\nSection 51.6: Determine a Windows Login's Permission Path\nThis will show the user type and permission path (which windows group the user is getting its permissions from).\nxp_logininfo 'DOMAIN\\user'\nSection 51.7: Search and Return All Tables and Columns\nContaining a Speciﬁed Column Value\nThis script, from here and here, will return all Tables and Columns where a speciﬁed value exists. This is powerful in\nﬁnding out where a certain value is in a database. It can be taxing, so it is suggested that it be executed in a backup\n/ test enviroment ﬁrst.\nDECLARE @SearchStr nvarchar(100)\nSET @SearchStr = '## YOUR STRING HERE ##'\n \n \n    -- Copyright © 2002 Narayana Vyas Kondreddi. All rights reserved.\n    -- Purpose: To search all columns of all tables for a given search string\n    -- Written by: Narayana Vyas Kondreddi\n    -- Site: http://vyaskn.tripod.com\n    -- Updated and tested by Tim Gaunt\n    -- http://www.thesitedoctor.co.uk\n    --\nhttp://blogs.thesitedoctor.co.uk/tim/2010/02/19/Search+Every+Table+And+Field+In+A+SQL+Server+Databa\nse+Updated.aspx\n    -- Tested on: SQL Server 7.0, SQL Server 2000, SQL Server 2005 and SQL Server 2010\n    -- Date modified: 03rd March 2011 19:00 GMT\n    CREATE TABLE #Results (ColumnName nvarchar(370), ColumnValue nvarchar(3630))\n \n    SET NOCOUNT ON\n \n    DECLARE @TableName nvarchar(256), @ColumnName nvarchar(128), @SearchStr2 nvarchar(110)\n    SET  @TableName = ''\n    SET @SearchStr2 = QUOTENAME('%' + @SearchStr + '%','''')\n \n    WHILE @TableName IS NOT NULL\n     \n    BEGIN\n        SET @ColumnName = ''\n        SET @TableName =\n        (\n            SELECT MIN(QUOTENAME(TABLE_SCHEMA) + '.' + QUOTENAME(TABLE_NAME))\n            FROM     INFORMATION_SCHEMA.TABLES\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 154\n            WHERE         TABLE_TYPE = 'BASE TABLE'\n                AND    QUOTENAME(TABLE_SCHEMA) + '.' + QUOTENAME(TABLE_NAME) > @TableName\n                AND    OBJECTPROPERTY(\n                        OBJECT_ID(\n                            QUOTENAME(TABLE_SCHEMA) + '.' + QUOTENAME(TABLE_NAME)\n                             ), 'IsMSShipped'\n                               ) = 0\n        )\n \n        WHILE (@TableName IS NOT NULL) AND (@ColumnName IS NOT NULL)\n             \n        BEGIN\n            SET @ColumnName =\n            (\n                SELECT MIN(QUOTENAME(COLUMN_NAME))\n                FROM     INFORMATION_SCHEMA.COLUMNS\n                WHERE         TABLE_SCHEMA    = PARSENAME(@TableName, 2)\n                    AND    TABLE_NAME    = PARSENAME(@TableName, 1)\n                    AND    DATA_TYPE IN ('char', 'varchar', 'nchar', 'nvarchar', 'int', 'decimal')\n                    AND    QUOTENAME(COLUMN_NAME) > @ColumnName\n            )\n     \n            IF @ColumnName IS NOT NULL\n             \n            BEGIN\n                INSERT INTO #Results\n                EXEC\n                (\n                    'SELECT ''' + @TableName + '.' + @ColumnName + ''', LEFT(' + @ColumnName + ',\n3630) FROM ' + @TableName + ' (NOLOCK) ' +\n                    ' WHERE ' + @ColumnName + ' LIKE ' + @SearchStr2\n                )\n            END\n        END  \n    END\n \n    SELECT ColumnName, ColumnValue FROM #Results\n \nDROP TABLE #Results\n-- See more at:\nhttp://thesitedoctor.co.uk/blog/search-every-table-and-field-in-a-sql-server-database-updated#sthas\nh.bBEqfJVZ.dpuf\nSection 51.8: Get all schemas, tables, columns and indexes\nSELECT\n    s.name AS [schema],\n    t.object_id AS [table_object_id],\n    t.name AS [table_name],\n    c.column_id,\n    c.name AS [column_name],\n    i.name AS [index_name],\n    i.type_desc AS [index_type]\nFROM sys.schemas AS s\nINNER JOIN sys.tables AS t\n    ON s.schema_id = t.schema_id\nINNER JOIN sys.columns AS c\n    ON t.object_id = c.object_id\nLEFT JOIN sys.index_columns AS ic\n    ON c.object_id = ic.object_id and c.column_id = ic.column_id\nLEFT JOIN sys.indexes AS i\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 155\n    ON ic.object_id = i.object_id and ic.index_id = i.index_id\nORDER BY [schema], [table_name], c.column_id;\nSection 51.9: Return a list of SQL Agent jobs, with schedule\ninformation\nUSE msdb\nGo\nSELECT dbo.sysjobs.Name AS 'Job Name',\n      'Job Enabled' = CASE dbo.sysjobs.Enabled\n            WHEN 1 THEN 'Yes'\n            WHEN 0 THEN 'No'\n      END,\n      'Frequency' = CASE dbo.sysschedules.freq_type\n            WHEN 1 THEN 'Once'\n            WHEN 4 THEN 'Daily'\n            WHEN 8 THEN 'Weekly'\n            WHEN 16 THEN 'Monthly'\n            WHEN 32 THEN 'Monthly relative'\n            WHEN 64 THEN 'When SQLServer Agent starts'\n      END,\n      'Start Date' = CASE active_start_date\n            WHEN 0 THEN null\n            ELSE\n            substring(convert(varchar(15),active_start_date),1,4) + '/' +\n            substring(convert(varchar(15),active_start_date),5,2) + '/' +\n            substring(convert(varchar(15),active_start_date),7,2)\n      END,\n      'Start Time' = CASE len(active_start_time)\n            WHEN 1 THEN cast('00:00:0' + right(active_start_time,2) as char(8))\n            WHEN 2 THEN cast('00:00:' + right(active_start_time,2) as char(8))\n            WHEN 3 THEN cast('00:0'\n                        + Left(right(active_start_time,3),1)  \n                        +':' + right(active_start_time,2) as char (8))\n            WHEN 4 THEN cast('00:'\n                        + Left(right(active_start_time,4),2)  \n                        +':' + right(active_start_time,2) as char (8))\n            WHEN 5 THEN cast('0'\n                        + Left(right(active_start_time,5),1)\n                        +':' + Left(right(active_start_time,4),2)  \n                        +':' + right(active_start_time,2) as char (8))\n            WHEN 6 THEN cast(Left(right(active_start_time,6),2)\n                        +':' + Left(right(active_start_time,4),2)  \n                        +':' + right(active_start_time,2) as char (8))\n      END,\n      CASE len(run_duration)\n            WHEN 1 THEN cast('00:00:0'\n                        + cast(run_duration as char) as char (8))\n            WHEN 2 THEN cast('00:00:'\n                        + cast(run_duration as char) as char (8))\n            WHEN 3 THEN cast('00:0'\n                        + Left(right(run_duration,3),1)  \n                        +':' + right(run_duration,2) as char (8))\n            WHEN 4 THEN cast('00:'\n                        + Left(right(run_duration,4),2)  \n                        +':' + right(run_duration,2) as char (8))\n            WHEN 5 THEN cast('0'\n                        + Left(right(run_duration,5),1)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 156\n                        +':' + Left(right(run_duration,4),2)  \n                        +':' + right(run_duration,2) as char (8))\n            WHEN 6 THEN cast(Left(right(run_duration,6),2)\n                        +':' + Left(right(run_duration,4),2)  \n                        +':' + right(run_duration,2) as char (8))\n      END as 'Max Duration',\n    CASE(dbo.sysschedules.freq_subday_interval)\n            WHEN 0 THEN 'Once'\n            ELSE cast('Every '\n                        + right(dbo.sysschedules.freq_subday_interval,2)\n                        + ' '\n                        +     CASE(dbo.sysschedules.freq_subday_type)\n                                          WHEN 1 THEN 'Once'\n                                          WHEN 4 THEN 'Minutes'\n                                          WHEN 8 THEN 'Hours'\n                                    END as char(16))\n    END as 'Subday Frequency'\nFROM dbo.sysjobs\nLEFT OUTER JOIN dbo.sysjobschedules\nON dbo.sysjobs.job_id = dbo.sysjobschedules.job_id\nINNER JOIN dbo.sysschedules ON dbo.sysjobschedules.schedule_id = dbo.sysschedules.schedule_id\nLEFT OUTER JOIN (SELECT job_id, max(run_duration) AS run_duration\n            FROM dbo.sysjobhistory\n            GROUP BY job_id) Q1\nON dbo.sysjobs.job_id = Q1.job_id\nWHERE Next_run_time = 0\nUNION\nSELECT dbo.sysjobs.Name AS 'Job Name',\n      'Job Enabled' = CASE dbo.sysjobs.Enabled\n            WHEN 1 THEN 'Yes'\n            WHEN 0 THEN 'No'\n      END,\n      'Frequency' = CASE dbo.sysschedules.freq_type\n            WHEN 1 THEN 'Once'\n            WHEN 4 THEN 'Daily'\n            WHEN 8 THEN 'Weekly'\n            WHEN 16 THEN 'Monthly'\n            WHEN 32 THEN 'Monthly relative'\n            WHEN 64 THEN 'When SQLServer Agent starts'\n      END,\n      'Start Date' = CASE next_run_date\n            WHEN 0 THEN null\n            ELSE\n            substring(convert(varchar(15),next_run_date),1,4) + '/' +\n            substring(convert(varchar(15),next_run_date),5,2) + '/' +\n            substring(convert(varchar(15),next_run_date),7,2)\n      END,\n      'Start Time' = CASE len(next_run_time)\n            WHEN 1 THEN cast('00:00:0' + right(next_run_time,2) as char(8))\n            WHEN 2 THEN cast('00:00:' + right(next_run_time,2) as char(8))\n            WHEN 3 THEN cast('00:0'\n                        + Left(right(next_run_time,3),1)  \n                        +':' + right(next_run_time,2) as char (8))\n            WHEN 4 THEN cast('00:'\n                        + Left(right(next_run_time,4),2)  \n                        +':' + right(next_run_time,2) as char (8))\n            WHEN 5 THEN cast('0' + Left(right(next_run_time,5),1)\n                        +':' + Left(right(next_run_time,4),2)  \n                        +':' + right(next_run_time,2) as char (8))\n            WHEN 6 THEN cast(Left(right(next_run_time,6),2)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 157\n                        +':' + Left(right(next_run_time,4),2)  \n                        +':' + right(next_run_time,2) as char (8))\n      END,\n      CASE len(run_duration)\n            WHEN 1 THEN cast('00:00:0'\n                        + cast(run_duration as char) as char (8))\n            WHEN 2 THEN cast('00:00:'\n                        + cast(run_duration as char) as char (8))\n            WHEN 3 THEN cast('00:0'\n                        + Left(right(run_duration,3),1)  \n                        +':' + right(run_duration,2) as char (8))\n            WHEN 4 THEN cast('00:'\n                        + Left(right(run_duration,4),2)  \n                        +':' + right(run_duration,2) as char (8))\n            WHEN 5 THEN cast('0'\n                        + Left(right(run_duration,5),1)\n                        +':' + Left(right(run_duration,4),2)  \n                        +':' + right(run_duration,2) as char (8))\n            WHEN 6 THEN cast(Left(right(run_duration,6),2)\n                        +':' + Left(right(run_duration,4),2)  \n                        +':' + right(run_duration,2) as char (8))\n      END as 'Max Duration',\n    CASE(dbo.sysschedules.freq_subday_interval)\n            WHEN 0 THEN 'Once'\n            ELSE cast('Every '\n                        + right(dbo.sysschedules.freq_subday_interval,2)\n                        + ' '\n                        +     CASE(dbo.sysschedules.freq_subday_type)\n                                          WHEN 1 THEN 'Once'\n                                          WHEN 4 THEN 'Minutes'\n                                          WHEN 8 THEN 'Hours'\n                                    END as char(16))\n    END as 'Subday Frequency'\nFROM dbo.sysjobs\nLEFT OUTER JOIN dbo.sysjobschedules ON dbo.sysjobs.job_id = dbo.sysjobschedules.job_id\nINNER JOIN dbo.sysschedules ON dbo.sysjobschedules.schedule_id = dbo.sysschedules.schedule_id\nLEFT OUTER JOIN (SELECT job_id, max(run_duration) AS run_duration\n            FROM dbo.sysjobhistory\n            GROUP BY job_id) Q1\nON dbo.sysjobs.job_id = Q1.job_id\nWHERE Next_run_time <> 0\nORDER BY [Start Date],[Start Time]\nSection 51.10: Retrieve Tables Containing Known Column\nThis query will return all COLUMNS and their associated TABLES for a given column name. It is designed to show you\nwhat tables (unknown) contain a speciﬁed column (known)\nSELECT\n    c.name AS ColName,\n    t.name AS TableName\nFROM\n    sys.columns c\n    JOIN sys.tables t ON c.object_id = t.object_id\nWHERE\n    c.name LIKE '%MyName%'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 158\nSection 51.11: Show Size of All Tables in Current Database\nSELECT\n    s.name + '.' + t.NAME AS TableName,\n    SUM(a.used_pages)*8 AS 'TableSizeKB'  --a page in SQL Server is 8kb\nFROM sys.tables t\n    JOIN sys.schemas s on t.schema_id = s.schema_id\n    LEFT JOIN sys.indexes i ON t.OBJECT_ID = i.object_id\n    LEFT JOIN sys.partitions p ON i.object_id = p.OBJECT_ID AND i.index_id = p.index_id\n    LEFT JOIN sys.allocation_units a ON p.partition_id = a.container_id\nGROUP BY\n    s.name, t.name\nORDER BY\n    --Either sort by name:\n    s.name + '.' + t.NAME\n    --Or sort largest to smallest:\n    --SUM(a.used_pages) desc\nSection 51.12: Retrieve Database Options\nThe following query returns the database options and metadata:\nselect * from sys.databases WHERE name = 'MyDatabaseName';\nSection 51.13: Find every mention of a ﬁeld in the database\nSELECT DISTINCT\n o.name AS Object_Name,o.type_desc\n FROM sys.sql_modules m\n    INNER JOIN sys.objects  o ON m.object_id=o.object_id\n WHERE m.definition Like '%myField%'\n ORDER BY 2,1\nWill ﬁnd mentions of myField in SProcs, Views, etc.\nSection 51.14: Retrieve information on backup and restore\noperations\nTo get the list of all backup operations performed on the current database instance:\nSELECT sdb.Name AS DatabaseName,\n    COALESCE(CONVERT(VARCHAR(50), bus.backup_finish_date, 120),'-') AS LastBackUpDateTime\nFROM sys.sysdatabases sdb\n    LEFT OUTER JOIN msdb.dbo.backupset bus ON bus.database_name = sdb.name\nORDER BY sdb.name, bus.backup_finish_date DESC\nTo get the list of all restore operations performed on the current database instance:\nSELECT\n    [d].[name] AS database_name,\n    [r].restore_date AS last_restore_date,\n    [r].[user_name],\n    [bs].[backup_finish_date] AS backup_creation_date,\n    [bmf].[physical_device_name] AS [backup_file_used_for_restore]\nFROM master.sys.databases [d]\n    LEFT OUTER JOIN msdb.dbo.[restorehistory] r ON r.[destination_database_name] = d.Name\n    INNER JOIN msdb.dbo.backupset [bs] ON [r].[backup_set_id] = [bs].[backup_set_id]\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 159\n    INNER JOIN msdb.dbo.backupmediafamily bmf ON [bs].[media_set_id] = [bmf].[media_set_id]\nORDER BY [d].[name], [r].restore_date DESC\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 160\nChapter 52: Split String function in SQL\nServer\nSection 52.1: Split string in Sql Server 2008/2012/2014 using\nXML\nSince there is no STRING_SPLIT function we need to use XML hack to split the string into rows:\nExample:\nSELECT split.a.value('.', 'VARCHAR(100)') AS Value\nFROM   (SELECT Cast ('<M>' + Replace('A|B|C', '|', '</M><M>')+ '</M>' AS XML) AS Data) AS A\n       CROSS apply data.nodes ('/M') AS Split(a);\nResult:\n+-----+\n|Value|\n+-----+\n|A    |\n+-----+\n|B    |\n+-----+\n|C    |\n+-----+\nSection 52.2: Split a String in Sql Server 2016\nIn SQL Server 2016 ﬁnally they have introduced Split string function : STRING_SPLIT\nParameters: It accepts two parameters\nString:\nIs an expression of any character type (i.e. nvarchar, varchar, nchar or char).\nseparator :\nIs a single character expression of any character type (e.g. nvarchar(1), varchar(1), nchar(1) or char(1)) that\nis used as separator for concatenated strings.\nNote: You should always check if the expression is a non-empty string.\nExample:\nSelect Value\nFrom STRING_SPLIT('a|b|c','|')\nIn above example\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 161\nString    : 'a|b|c'\nseparator : '|'\nResult :\n+-----+\n|Value|\n+-----+\n|a    |\n+-----+\n|b    |\n+-----+\n|c    |\n+-----+\nIf it's an empty string:\nSELECT value\nFROM STRING_SPLIT('',',')\nResult :\n  +-----+\n|Value|\n+-----+\n1 |     |\n+-----+\nYou can avoid the above situation by adding a WHERE clause\nSELECT value\nFROM STRING_SPLIT('',',')\nWHERE LTRIM(RTRIM(value))<>''\nSection 52.3: T-SQL Table variable and XML\nDeclare @userList Table(UserKey VARCHAR(60))\nInsert into @userList values ('bill'),('jcom'),('others')\n--Declared a table variable and insert 3 records\nDeclare @text XML\nSelect  @text = (\n        select UserKey  from @userList for XML Path('user'), root('group')\n)\n--Set the XML value from Table\nSelect @text\n--View the variable value\nXML:\n\\<group>\\<user>\\<UserKey>bill\\</UserKey>\\</user>\\<user>\\<UserKey>jcom\\</UserKey>\\</user>\\<user>\\<Us\nerKey>others\\</UserKey>\\</user>\\</group>\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 162\nChapter 53: Insert\nSection 53.1: Add a row to a table named Invoices\nINSERT INTO Invoices [ /* column names may go here */ ]\nVALUES (123, '1234abc', '2016-08-05 20:18:25.770', 321, 5, '2016-08-04');\nColumn names are required if the table you are inserting into contains a column with the IDENTITY attribute.\nINSERT INTO Invoices ([ID], [Num], [DateTime], [Total], [Term], [DueDate])\nVALUES (123, '1234abc', '2016-08-05 20:18:25.770', 321, 5, '2016-08-25');\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 163\nChapter 54: Primary Keys\nSection 54.1: Create table w/ identity column as primary key\n -- Identity primary key - unique arbitrary increment number\n create table person (\n id int identity(1,1) primary key not null,\n firstName varchar(100) not null,\n lastName varchar(100) not null,\n dob DateTime not null,\n ssn varchar(9) not null\n )\nSection 54.2: Create table w/ GUID primary key\n -- GUID primary key - arbitrary unique value for table\n create table person (\n id uniqueIdentifier default (newId()) primary key,\n firstName varchar(100) not null,\n lastName varchar(100) not null,\n dob DateTime not null,\n ssn varchar(9) not null\n )\nSection 54.3: Create table w/ natural key\n -- natural primary key - using an existing piece of data within the table that uniquely identifies\nthe record\n create table person (\n firstName varchar(100) not null,\n lastName varchar(100) not null,\n dob DateTime not null,\n ssn varchar(9) primary key not null\n )\nSection 54.4: Create table w/ composite key\n -- composite key - using two or more existing columns within a table to create a primary key\n create table person (\n firstName varchar(100) not null,\n lastName varchar(100) not null,\n dob DateTime not null,\n ssn varchar(9) not null,\n primary key (firstName, lastName, dob)\n )\nSection 54.5: Add primary key to existing table\nALTER TABLE person\n ADD CONSTRAINT pk_PersonSSN PRIMARY KEY (ssn)\nNote, if the primary key column (in this case ssn) has more than one row with the same candidate key, the above\nstatement will fail, as primary key values must be unique.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 164\nSection 54.6: Delete primary key\nALTER TABLE Person\n DROP CONSTRAINT pk_PersonSSN\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 165\nChapter 55: Foreign Keys\nSection 55.1: Foreign key relationship/constraint\nForeign keys enables you to deﬁne relationship between two tables. One (parent) table need to have primary key\nthat uniquely identiﬁes rows in the table. Other (child) table can have value of the primary key from the parent in\none of the columns. FOREIGN KEY REFERENCES constraint ensures that values in child table must exist as a primary\nkey value in the parent table.\nIn this example we have parent Company table with CompanyId primary key, and child Employee table that has id\nof the company where this employee works.\ncreate table Company (\n   CompanyId int primary key,\n   Name nvarchar(200)\n)\ncreate table Employee (\n    EmployeeId int,\n    Name nvarchar(200),\n    CompanyId int\n        foreign key references Company(companyId)\n)\nforeign key references ensures that values inserted in Employee.CompanyId column must also exist in\nCompany.CompanyId column. Also, nobody can delete company in company table if there is ate least one\nemployee with a matching companyId in child table.\nFOREIGN KEY relationship ensures that rows in two tables cannot be \"unlinked\".\nSection 55.2: Maintaining relationship between parent/child\nrows\nLet's assume that we have one row in Company table with companyId 1. We can insert row in employee table that\nhas companyId 1:\ninsert into Employee values (17, 'John', 1)\nHowever, we cannot insert employee that has non-existing CompanyId:\ninsert into Employee values (17, 'John', 111111)\nMsg 547, Level 16, State 0, Line 12 The INSERT statement conﬂicted with the FOREIGN KEY constraint\n\"FK__Employee__Compan__1EE485AA\". The conﬂict occurred in database \"MyDb\", table \"dbo.Company\", column\n'CompanyId'. The statement has been terminated.\nAlso, we cannot delete parent row in company table as long as there is at least one child row in employee table that\nreferences it.\ndelete from company where CompanyId = 1\nMsg 547, Level 16, State 0, Line 14 The DELETE statement conﬂicted with the REFERENCE constraint\n\"FK__Employee__Compan__1EE485AA\". The conﬂict occurred in database \"MyDb\", table \"dbo.Employee\", column\n'CompanyId'. The statement has been terminated.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 166\nForeign key relationship ensures that Company and employee rows will not be \"unlinked\".\nSection 55.3: Adding foreign key relationship on existing table\nFOREIGN KEY constraint can be added on existing tables that are still not in relationship. Imagine that we have\nCompany and Employee tables where Employee table CompanyId column but don't have foreign key relationship.\nALTER TABLE statement enables you to add foreign key constraint on an existing column that references some\nother table and primary key in that table:\nalter table Employee\n    add  foreign key (CompanyId) references Company(CompanyId)\nSection 55.4: Add foreign key on existing table\nFOREIGN KEY columns with constraint can be added on existing tables that are still not in relationship. Imagine\nthat we have Company and Employee tables where Employee table don't have CompanyId column. ALTER TABLE\nstatement enables you to add new column with foreign key constraint that references some other table and\nprimary key in that table:\nalter table Employee\n    add CompanyId int foreign key references Company(CompanyId)\nSection 55.5: Getting information about foreign key\nconstraints\nsys.foreignkeys system view returns information about all foreign key relationships in database:\nselect name,\n OBJECT_NAME(referenced_object_id) as [parent table],\n OBJECT_NAME(parent_object_id) as [child table],\n delete_referential_action_desc,\n update_referential_action_desc\nfrom sys.foreign_keys\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 167\nChapter 56: Last Inserted Identity\nSection 56.1: @@IDENTITY and MAX(ID)\nSELECT MAX(Id) FROM Employees -- Display the value of Id in the last row in Employees table.\nGO\nINSERT INTO Employees (FName, LName, PhoneNumber) -- Insert a new row\nVALUES ('John', 'Smith', '25558696525')\nGO  \nSELECT @@IDENTITY\nGO  \nSELECT MAX(Id) FROM Employees -- Display the value of Id of the newly inserted row.  \nGO\nThe last two SELECT statements values are the same.\nSection 56.2: SCOPE_IDENTITY()\nCREATE TABLE dbo.logging_table(log_id INT IDENTITY(1,1) PRIMARY KEY,\n                               log_message VARCHAR(255))\nCREATE TABLE dbo.person(person_id INT IDENTITY(1,1) PRIMARY KEY,\n                        person_name VARCHAR(100) NOT NULL)\nGO;\nCREATE TRIGGER dbo.InsertToADifferentTable ON dbo.person  \nAFTER INSERT  \nAS\n    INSERT INTO dbo.logging_table(log_message)\n    VALUES('Someone added something to the person table')\nGO;\nINSERT INTO dbo.person(person_name)\nVALUES('John Doe')  \nSELECT SCOPE_IDENTITY();\nThis will return the most recently added identity value produced on the same connection, within the current scope.\nIn this case, 1, for the ﬁrst row in the dbo.person table.\nSection 56.3: @@IDENTITY\nCREATE TABLE dbo.logging_table(log_id INT IDENTITY(1,1) PRIMARY KEY,\n                               log_message VARCHAR(255))\nCREATE TABLE dbo.person(person_id INT IDENTITY(1,1) PRIMARY KEY,\n                        person_name VARCHAR(100) NOT NULL)\nGO;\nCREATE TRIGGER dbo.InsertToADifferentTable ON dbo.person  \nAFTER INSERT  \nAS\n    INSERT INTO dbo.logging_table(log_message)\n    VALUES('Someone added something to the person table')\nGO;\nINSERT INTO dbo.person(person_name)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 168\nVALUES('John Doe')    \nSELECT @@IDENTITY;\nThis will return the most recently-added identity on the same connection, regardless of scope. In this case,\nwhatever the current value of the identity column on logging_table is, assuming no other activity is occurring on the\ninstance of SQL Server and no other triggers ﬁre from this insert.\nSection 56.4: IDENT_CURRENT('tablename')\nSELECT IDENT_CURRENT('dbo.person');\nThis will select the most recently-added identity value on the selected table, regardless of connection or scope.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 169\nChapter 57: SCOPE_IDENTITY()\nSection 57.1: Introduction with Simple Example\nSCOPE_IDENTITY() returns the last identity value inserted into an identity column in the same scope. A scope is a\nmodule: a stored procedure, trigger, function, or batch. Therefore, two statements are in the same scope if they are\nin the same stored procedure, function, or batch.\nINSERT INTO  ([column1],[column2]) VALUES (8,9);\nGO\nSELECT SCOPE_IDENTITY() AS [SCOPE_IDENTITY];\nGO\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 170\nChapter 58: Sequences\nSection 58.1: Create Sequence\nCREATE SEQUENCE [dbo].[CustomersSeq]\nAS INT\nSTART WITH 10001\nINCREMENT BY 1\nMINVALUE -1;\nSection 58.2: Use Sequence in Table\nCREATE TABLE [dbo].[Customers]\n(\n    CustomerID INT DEFAULT (NEXT VALUE FOR [dbo].[CustomersSeq]) NOT NULL,\n    CustomerName VARCHAR(100),\n);\nSection 58.3: Insert Into Table with Sequence\nINSERT INTO [dbo].[Customers]\n       ([CustomerName])\n VALUES\n       ('Jerry'),\n       ('Gorge')\nSELECT * FROM [dbo].[Customers]\nResults\nCustomerID CustomerName\n10001 Jerry\n10002 Gorge\nSection 58.4: Delete From & Insert New\nDELETE FROM [dbo].[Customers]\nWHERE CustomerName = 'Gorge';\nINSERT INTO [dbo].[Customers]\n       ([CustomerName])\n VALUES ('George')\nSELECT * FROM [dbo].[Customers]\nResults\nCustomerID CustomerName\n10001 Jerry\n10003 George\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 171\nChapter 59: Index\nSection 59.1: Create Clustered index\nWith a clustered index the leaf pages contain the actual table rows. Therefore, there can be only one clustered\nindex.\nCREATE TABLE Employees\n(\n    ID CHAR(900),\n    FirstName NVARCHAR(3000),\n    LastName NVARCHAR(3000),\n    StartYear CHAR(900)\n)\nGO\nCREATE CLUSTERED INDEX IX_Clustered\nON Employees(ID)\nGO\nSection 59.2: Drop index\nDROP INDEX IX_NonClustered ON Employees\nSection 59.3: Create Non-Clustered index\nNon-clustered indexes have a structure separate from the data rows. A non-clustered index contains the non-\nclustered index key values and each key value entry has a pointer to the data row that contains the key value. There\ncan be maximum 999 non-clustered index on SQL Server 2008/ 2012.\nLink for reference: https://msdn.microsoft.com/en-us/library/ms143432.aspx\nCREATE TABLE Employees\n(\n    ID CHAR(900),\n    FirstName NVARCHAR(3000),\n    LastName NVARCHAR(3000),\n    StartYear CHAR(900)\n)\nGO\nCREATE NONCLUSTERED INDEX IX_NonClustered\nON Employees(StartYear)\nGO\nSection 59.4: Show index info\nSP_HELPINDEX tableName\nSection 59.5: Returns size and fragmentation indexes\nsys.dm_db_index_physical_stats (  \n    { database_id | NULL | 0 | DEFAULT }  \n  , { object_id | NULL | 0 | DEFAULT }  \n  , { index_id | NULL | 0 | -1 | DEFAULT }  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 172\n  , { partition_number | NULL | 0 | DEFAULT }  \n  , { mode | NULL | DEFAULT }  \n)  \nSample :\nSELECT * FROM sys.dm_db_index_physical_stats  \n    (DB_ID(N'DBName'), OBJECT_ID(N'IX_NonClustered '), NULL, NULL , 'DETAILED');  \nSection 59.6: Reorganize and rebuild index\navg_fragmentation_in_percent value Corrective statement\n>5% and < = 30% REORGANIZE\n>30% REBUILD\nALTER INDEX IX_NonClustered ON tableName REORGANIZE;  \nALTER INDEX ALL ON Production.Product\n REBUILD WITH (FILLFACTOR = 80, SORT_IN_TEMPDB = ON,\n          STATISTICS_NORECOMPUTE = ON);\nSection 59.7: Rebuild or reorganize all indexes on a table\nRebuilding indexes is done using the following statement\nALTER INDEX All ON tableName REBUILD;\nThis drops the index and recreates it, removing fragementation, reclaims disk space and reorders index pages.\nOne can also reorganize an index using\nALTER INDEX All ON tableName REORGANIZE;\nwhich will use minimal system resources and defragments the leaf level of clustered and nonclustered indexes on\ntables and views by physically reordering the leaf-level pages to match the logical, left to right, order of the leaf\nnodes\nSection 59.8: Rebuild all index database\nEXEC sp_MSForEachTable 'ALTER INDEX ALL ON ? REBUILD'\nSection 59.9: Index on view\nCREATE VIEW  View_Index02\nWITH SCHEMABINDING\nAS\nSELECT c.CompanyName, o.OrderDate, o.OrderID, od.ProductID\n     FROM dbo.Customers C\n        INNER JOIN dbo.orders O ON c.CustomerID=o.CustomerID  \n            INNER JOIN dbo.[Order Details] od ON o.OrderID=od.OrderID  \nGO\nCREATE UNIQUE CLUSTERED INDEX IX1 ON\n    View_Index02(OrderID, ProductID)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 173\nSection 59.10: Index investigations\nYou could use \"SP_HELPINDEX Table_Name\", but Kimberly Tripp has a stored procedure (that can be found here),\nwhich is better example, as it shows more about the indexes, including columns and ﬁlter deﬁnition, for example:\nUsage:\nUSE Adventureworks\nEXEC sp_SQLskills_SQL2012_helpindex 'dbo.Product'\nAlternatively, Tibor Karaszi has a stored procedure (found here). The later will show information on index usage too,\nand optionally provide a list of index suggestions. Usage:\nUSE Adventureworks\nEXEC sp_indexinfo 'dbo.Product'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 174\nChapter 60: Full-Text Indexing\nSection 60.1: A. Creating a unique index, a full-text catalog,\nand a full-text index\nThe following example creates a unique index on the JobCandidateID column of the HumanResources.JobCandidate\ntable of the AdventureWorks2012 sample database. The example then creates a default full-text catalog, ft. Finally,\nthe example creates a full-text index on the Resume column, using the ft catalog and the system stoplist.\nUSE AdventureWorks2012;  \nGO  \nCREATE UNIQUE INDEX ui_ukJobCand ON HumanResources.JobCandidate(JobCandidateID);  \nCREATE FULLTEXT CATALOG ft AS DEFAULT;  \nCREATE FULLTEXT INDEX ON HumanResources.JobCandidate(Resume)  \n   KEY INDEX ui_ukJobCand  \n   WITH STOPLIST = SYSTEM;  \nGO\nhttps://www.simple-talk.com/sql/learn-sql-server/understanding-full-text-indexing-in-sql-server/\nhttps://msdn.microsoft.com/en-us/library/cc879306.aspx\nhttps://msdn.microsoft.com/en-us/library/ms142571.aspx\nSection 60.2: Creating a full-text index on several table\ncolumns\nUSE AdventureWorks2012;  \nGO  \nCREATE FULLTEXT CATALOG production_catalog;  \nGO  \nCREATE FULLTEXT INDEX ON Production.ProductReview  \n (  \n  ReviewerName  \n     Language 1033,  \n  EmailAddress  \n     Language 1033,  \n  Comments  \n     Language 1033      \n )  \n  KEY INDEX PK_ProductReview_ProductReviewID  \n      ON production_catalog;  \nGO  \nSection 60.3: Creating a full-text index with a search property\nlist without populating it\nUSE AdventureWorks2012;  \nGO  \nCREATE FULLTEXT INDEX ON Production.Document  \n  (  \n  Title  \n      Language 1033,  \n  DocumentSummary  \n      Language 1033,  \n  Document  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 175\n      TYPE COLUMN FileExtension  \n      Language 1033  \n  )  \n  KEY INDEX PK_Document_DocumentID  \n          WITH STOPLIST = SYSTEM, SEARCH PROPERTY LIST = DocumentPropertyList, CHANGE_TRACKING OFF,\nNO POPULATION;  \n   GO  \nAnd populating it later with\nALTER FULLTEXT INDEX ON Production.Document SET CHANGE_TRACKING AUTO;  \nGO  \nSection 60.4: Full-Text Search\nSELECT product_id  \nFROM products  \nWHERE CONTAINS(product_description, ”Snap Happy 100EZ” OR FORMSOF(THESAURUS,’Snap Happy’) OR ‘100EZ’)\n \nAND product_cost < 200 ;  \nSELECT candidate_name,SSN  \nFROM candidates  \nWHERE CONTAINS(candidate_resume,”SQL Server”) AND candidate_division =DBA;  \nFor more and detailed info https://msdn.microsoft.com/en-us/library/ms142571.aspx\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 176\nChapter 61: Trigger\nA trigger is a special type of stored procedure, which is executed automatically after an event occurs. There are two\ntypes of triggers: Data Deﬁnition Language Triggers and Data Manipulation Language Triggers.\nIt is usually bound to a table and ﬁres automatically. You cannot explicitly call any trigger.\nSection 61.1: DML Triggers\nDML Triggers are ﬁred as a response to dml statements (insert, update or delete).\nA dml trigger can be created to address one or more dml events for a single table or view. This means that a single\ndml trigger can handle inserting, updating and deleting records from a speciﬁc table or view, but in can only handle\ndata being changed on that single table or view.\nDML Triggers provides access to inserted and deleted tables that holds information about the data that was / will\nbe aﬀected by the insert, update or delete statement that ﬁred the trigger.\nNote that DML triggers are statement based, not row based. This means that if the statement eﬀected more then\none row, the inserted or deleted tables will contain more then one row.\nExamples:\nCREATE TRIGGER tblSomething_InsertOrUpdate ON tblSomething  \nFOR INSERT\nAS\n    INSERT INTO tblAudit (TableName, RecordId, Action)\n    SELECT 'tblSomething', Id, 'Inserted'\n    FROM Inserted\nGO\nCREATE TRIGGER tblSomething_InsertOrUpdate ON tblSomething  \nFOR UPDATE\nAS\n    INSERT INTO tblAudit (TableName, RecordId, Action)\n    SELECT 'tblSomething', Id, 'Updated'\n    FROM Inserted\nGO\nCREATE TRIGGER tblSomething_InsertOrUpdate ON tblSomething  \nFOR DELETE\nAS\n    INSERT INTO tblAudit (TableName, RecordId, Action)\n    SELECT 'tblSomething', Id, 'Deleted'\n    FROM Deleted\nGO\nAll the examples above will add records to tblAudit whenever a record is added, deleted or updated in\ntblSomething.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 177\nSection 61.2: Types and classiﬁcations of Trigger\nIn SQL Server, there are two categories of triggers: DDL Triggers and DML Triggers.\nDDL Triggers are ﬁred in response to Data Deﬁnition Language (DDL) events. These events primarily correspond to\nTransact-SQL statements that start with the keywords CREATE, ALTER and DROP.\nDML Triggers are ﬁred in response to Data Manipulation Language (DML) events. These events corresponds to\nTransact-SQL statements that start with the keywords INSERT, UPDATE and DELETE.\nDML triggers are classiﬁed into two main types:\nAfter Triggers (for triggers)1.\nAFTER INSERT Trigger.\nAFTER UPDATE Trigger.\nAFTER DELETE Trigger.\nInstead of triggers2.\nINSTEAD OF INSERT Trigger.\nINSTEAD OF UPDATE Trigger.\nINSTEAD OF DELETE Trigger.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 178\nChapter 62: Cursors\nSection 62.1: Basic Forward Only Cursor\nNormally you would want to avoid using cursors as they can have negative impacts on performance. However in\nsome special cases you may need to loop through your data record by record and perform some action.\nDECLARE @orderId AS INT\n-- here we are creating our cursor, as a local cursor and only allowing\n-- forward operations\nDECLARE rowCursor CURSOR LOCAL FAST_FORWARD FOR\n    -- this is the query that we want to loop through record by record\n    SELECT [OrderId]\n    FROM [dbo].[Orders]\n-- first we need to open the cursor\nOPEN rowCursor\n-- now we will initialize the cursor by pulling the first row of data, in this example the\n[OrderId] column,\n-- and storing the value into a variable called @orderId\nFETCH NEXT FROM rowCursor INTO @orderId\n-- start our loop and keep going until we have no more records to loop through\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    PRINT @orderId\n   \n    -- this is important, as it tells SQL Server to get the next record and store the [OrderId]\ncolumn value into the @orderId variable\n    FETCH NEXT FROM rowCursor INTO @orderId\nEND\n-- this will release any memory used by the cursor\nCLOSE rowCursor\nDEALLOCATE rowCursor\nSection 62.2: Rudimentary cursor syntax\nA simple cursor syntax, operating on a few example test rows:\n/* Prepare test data */\nDECLARE @test_table TABLE\n(\n    Id INT,\n    Val VARCHAR(100)\n);\nINSERT INTO @test_table(Id, Val)\nVALUES\n    (1, 'Foo'),\n    (2, 'Bar'),\n    (3, 'Baz');\n/* Test data prepared */\n/* Iterator variable @myId, for example sake */\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 179\nDECLARE @myId INT;\n/* Cursor to iterate rows and assign values to variables */\nDECLARE myCursor CURSOR FOR\n    SELECT Id\n    FROM @test_table;\n/* Start iterating rows */\nOPEN myCursor;\nFETCH NEXT FROM myCursor INTO @myId;\n/* @@FETCH_STATUS global variable will be 1 / true until there are no more rows to fetch */\nWHILE @@FETCH_STATUS = 0\nBEGIN\n    /* Write operations to perform in a loop here. Simple SELECT used for example */\n    SELECT Id, Val\n    FROM @test_table\n    WHERE Id = @myId;\n    /* Set variable(s) to the next value returned from iterator; this is needed otherwise the\ncursor will loop infinitely. */\n    FETCH NEXT FROM myCursor INTO @myId;\nEND\n/* After all is done, clean up */\nCLOSE myCursor;\nDEALLOCATE myCursor;\nResults from SSMS. Note that these are all separate queries, they are in no way uniﬁed. Notice how the query\nengine processes each iteration one by one instead of as a set.\nId Val\n1 Foo\n(1 row(s) aﬀected)\nId Val\n2 Bar\n(1 row(s) aﬀected)\nId Val\n3 Baz\n(1 row(s) aﬀected)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 180\nChapter 63: Transaction isolation levels\nSection 63.1: Read Committed\nVersion ≥ SQL Server 2008 R2\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED\nThis isolation level is the 2nd most permissive. It prevents dirty reads. The behavior of READ COMMITTED depends on\nthe setting of the READ_COMMITTED_SNAPSHOT:\nIf set to OFF (the default setting) the transaction uses shared locks to prevent other transactions from\nmodifying rows used by the current transaction, as well as block the current transaction from reading rows\nmodiﬁed by other transactions.\nIf set to ON, the READCOMMITTEDLOCK table hint can be used to request shared locking instead of row\nversioning for transactions running in READ COMMITTED mode.\nNote: READ COMMITTED is the default SQL Server behavior.\nSection 63.2: What are \"dirty reads\"?\nDirty reads (or uncommitted reads) are reads of rows which are being modiﬁed by an open transaction.\nThis behavior can be replicated by using 2 separate queries: one to open a transaction and write some data to a\ntable without committing, the other to select the data to be written (but not yet committed) with this isolation level.\nQuery 1 - Prepare a transaction but do not ﬁnish it:\nCREATE TABLE dbo.demo (\n    col1 INT,\n    col2 VARCHAR(255)\n);\nGO\n--This row will get committed normally:\nBEGIN TRANSACTION;\n    INSERT INTO dbo.demo(col1, col2)\n    VALUES (99, 'Normal transaction');\nCOMMIT TRANSACTION;\n--This row will be \"stuck\" in an open transaction, causing a dirty read\nBEGIN TRANSACTION;\n    INSERT INTO dbo.demo(col1, col2)\n    VALUES (42, 'Dirty read');\n--Do not COMMIT TRANSACTION or ROLLBACK TRANSACTION here\nQuery 2 - Read the rows including the open transaction:\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nSELECT * FROM dbo.demo;\nReturns:\ncol1        col2\n----------- ---------------------------------------\n99          Normal transaction\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 181\n42          Dirty read\nP.S.: Don't forget to clean up this demo data:\nCOMMIT TRANSACTION;\nDROP TABLE dbo.demo;\nGO\nSection 63.3: Read Uncommitted\nVersion ≥ SQL Server 2008 R2\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED\nThis is the most permissive isolation level, in that it does not cause any locks at all. It speciﬁes that statements can\nread all rows, including rows that have been written in transactions but not yet committed (i.e., they are still in\ntransaction). This isolation level can be subject to \"dirty reads\".\nSection 63.4: Repeatable Read\nVersion ≥ SQL Server 2008 R2\nSET TRANSACTION ISOLATION LEVEL REPEATABLE READ\nThis transaction isolation level is slightly less permissive than READ COMMITTED, in that shared locks are placed on all\ndata read by each statement in the transaction and are held until the transaction completes, as opposed to\nbeing released after each statement.\nNote: Use this option only when necessary, as it is more likely to cause database performance degradation as well\nas deadlocks than READ COMMITTED.\nSection 63.5: Snapshot\nVersion ≥ SQL Server 2008 R2\nSET TRANSACTION ISOLATION LEVEL SNAPSHOT\nSpeciﬁes that data read by any statement in a transaction will be the transactionally consistent version of the data\nthat existed at the start of the transaction, i.e., it will only read data that has been committed prior to the\ntransaction starting.\nSNAPSHOT transactions do not request or cause any locks on the data that is being read, as it is only reading the\nversion (or snapshot) of the data that existed at the time the transaction began.\nA transaction running in SNAPSHOT isolation level read only its own data changes while it is running. For example, a\ntransaction could update some rows and then read the updated rows, but that change will only be visible to the\ncurrent transaction until it is committed.\nNote: The ALLOW_SNAPSHOT_ISOLATION database option must be set to ON before the SNAPSHOT isolation level can\nbe used.\nSection 63.6: Serializable\nVersion ≥ SQL Server 2008 R2\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 182\nSET TRANSACTION ISOLATION LEVEL SERIALIZEABLE\nThis isolation level is the most restrictive. It requests range locks the range of key values that are read by each\nstatement in the transaction. This also means that INSERT statements from other transactions will be blocked if the\nrows to be inserted are in the range locked by the current transaction.\nThis option has the same eﬀect as setting HOLDLOCK on all tables in all SELECT statements in a transaction.\nNote: This transaction isolation has the lowest concurrency and should only be used when necessary.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 183\nChapter 64: Advanced options\nSection 64.1: Enable and show advanced options\nExec sp_configure 'show advanced options' ,1\nRECONFIGURE\nGO\n-- Show all configure\nsp_configure\nSection 64.2: Enable backup compression default\nExec sp_configure 'backup compression default',1\nGO  \nRECONFIGURE;\nSection 64.3: Enable cmd permission\nEXEC sp_configure 'xp_cmdshell', 1\nGO\nRECONFIGURE\nSection 64.4: Set default ﬁll factor percent\nsp_configure 'fill factor', 100;  \nGO  \nRECONFIGURE;  \nThe server must be restarted before the change can take eﬀect.\nSection 64.5: Set system recovery interval\nUSE master;  \nGO\n-- Set recovery every 3 min\nEXEC sp_configure 'recovery interval', '3';  \nRECONFIGURE WITH OVERRIDE;  \nSection 64.6: Set max server memory size\nUSE master\nEXEC sp_configure 'max server memory (MB)', 64\nRECONFIGURE WITH OVERRIDE\nSection 64.7: Set number of checkpoint tasks\nEXEC sp_configure \"number of checkpoint tasks\", 4\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 184\nChapter 65: Migration\nSection 65.1: How to generate migration scripts\nClick  Right Mouse  on Database you want to migrate then -> Tasks -> Generate Scripts...1.\nWizard will open click Next then chose objects you want to migrate and click Next again, then click Advanced2.\nscroll a bit down and in Types of data to script choose Schema and data (unless you want only\nstructures)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 185\nClick couple more times Next and Finish and you should have your database scripted in .sql ﬁle.3.\nrun .sql ﬁle on your new server, and you should be done.4.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 186\nChapter 66: Table Valued Parameters\nSection 66.1: Using a table valued parameter to insert\nmultiple rows to a table\nFirst, deﬁne a used deﬁned table type to use:\nCREATE TYPE names as TABLE\n(\n    FirstName varchar(10),\n    LastName varchar(10)\n)\nGO\nCreate the stored procedure:\nCREATE PROCEDURE prInsertNames\n(\n    @Names dbo.Names READONLY -- Note: You must specify the READONLY\n)\nAS\nINSERT INTO dbo.TblNames (FirstName, LastName)\nSELECT FirstName, LastName\nFROM @Names\nGO\nExecuting the stored procedure:\nDECLARE @names dbo.Names\nINSERT INTO @Names VALUES\n('Zohar', 'Peled'),\n('First', 'Last')\nEXEC dbo.prInsertNames @Names\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 187\nChapter 67: DBMAIL\nSection 67.1: Send simple email\nThis code sends a simple text-only email to recipient@someaddress.com\nEXEC msdb.dbo.sp_send_dbmail  \n    @profile_name = 'The Profile Name',  \n    @recipients = 'recipient@someaddress.com',  \n    @body = 'This is a simple email sent from SQL Server.',  \n    @subject = 'Simple email'\nSection 67.2: Send results of a query\nThis attaches the results of the query SELECT * FROM Users and sends it to recipient@someaddress.com\nEXEC msdb.dbo.sp_send_dbmail  \n    @profile_name = 'The Profile Name',  \n    @recipients = 'recipient@someaddress.com',  \n    @query = 'SELECT * FROM Users',  \n    @subject = 'List of users',  \n    @attach_query_result_as_file = 1;\nSection 67.3: Send HTML email\nHTML content must be passed to sp_send_dbmail\nVersion ≥ SQL Server 2012\nDECLARE @html VARCHAR(MAX);\nSET @html = CONCAT\n(\n    '<html><body>',\n    '<h1>Some Header Text</h1>',\n    '<p>Some paragraph text</p>',\n    '</body></html>'\n)\nVersion < SQL Server 2012\nDECLARE @html VARCHAR(MAX);\nSET @html =\n    '<html><body>' +\n    '<h1>Some Header Text</h1>' +\n    '<p>Some paragraph text</p>' +\n    '</body></html>';\nThen use the @html variable with the @body argument. The HTML string can also be passed directly to @body,\nalthough it may make the code harder to read.\nEXEC msdb.dbo.sp_send_dbmail\n    @recipients='recipient@someaddress.com',  \n    @subject = 'Some HTML content',  \n    @body = @html,  \n    @body_format = 'HTML';  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 188\nChapter 68: In-Memory OLTP (Hekaton)\nSection 68.1: Declare Memory-Optimized Table Variables\nFor faster performance you can memory-optimize your table variable. Here is the T-SQL for a traditional table\nvariable:\nDECLARE @tvp TABLE  \n(\n    col1   INT NOT NULL ,  \n    Col2   CHAR(10)\n);  \nTo deﬁne memory-optimized variables, you must ﬁrst create a memory-optimized table type and then declare a\nvariable from it:\nCREATE TYPE dbo.memTypeTable\nAS TABLE  \n(  \n    Col1  INT NOT NULL INDEX ix1,  \n    Col2  CHAR(10)  \n)  \nWITH  \n    (MEMORY_OPTIMIZED = ON);  \nThen we can use the table type like this:\nDECLARE @tvp memTypeTable\ninsert INTO @tvp\nvalues (1,'1'),(2,'2'),(3,'3'),(4,'4'),(5,'5'),(6,'6')\nSELECT * FROM @tvp\nResult:\nCol1    Col2\n1       1        \n2       2        \n3       3        \n4       4        \n5       5        \n6       6        \nSection 68.2: Create Memory Optimized Table\n-- Create demo database\nCREATE DATABASE SQL2016_Demo\n ON  PRIMARY\n(\n    NAME = N'SQL2016_Demo',\n    FILENAME = N'C:\\Dump\\SQL2016_Demo.mdf',\n    SIZE = 5120KB,\n    FILEGROWTH = 1024KB\n )\n LOG ON\n (\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 189\n    NAME = N'SQL2016_Demo_log',\n    FILENAME = N'C:\\Dump\\SQL2016_Demo_log.ldf',\n    SIZE = 1024KB,\n    FILEGROWTH = 10%\n )\nGO\nuse SQL2016_Demo\ngo\n-- Add Filegroup by MEMORY_OPTIMIZED_DATA type\nALTER DATABASE SQL2016_Demo\n    ADD FILEGROUP MemFG CONTAINS MEMORY_OPTIMIZED_DATA\nGO\n--Add a file to defined filegroup\nALTER DATABASE SQL2016_Demo ADD FILE\n    (\n        NAME = MemFG_File1,\n        FILENAME = N'C:\\Dump\\MemFG_File1' -- your file path, check directory exist before executing\nthis code\n    )\nTO FILEGROUP MemFG\nGO\n--Object Explorer -- check database created\nGO\n-- create memory optimized table 1\nCREATE TABLE dbo.MemOptTable1  \n(  \n    Column1     INT         NOT NULL,  \n    Column2     NVARCHAR(4000)  NULL,  \n    SpidFilter  SMALLINT    NOT NULL   DEFAULT (@@spid),  \n    INDEX ix_SpidFiler NONCLUSTERED (SpidFilter),  \n    INDEX ix_SpidFilter HASH (SpidFilter) WITH (BUCKET_COUNT = 64),  \n     \n    CONSTRAINT CHK_soSessionC_SpidFilter  \n        CHECK ( SpidFilter = @@spid ),  \n)  \n    WITH  \n        (MEMORY_OPTIMIZED = ON,  \n         DURABILITY = SCHEMA_AND_DATA);  --or DURABILITY = SCHEMA_ONLY\ngo  \n-- create memory optimized table 2\nCREATE TABLE MemOptTable2\n(\n    ID INT NOT NULL PRIMARY KEY NONCLUSTERED HASH WITH (BUCKET_COUNT = 10000),\n    FullName NVARCHAR(200) NOT NULL,\n    DateAdded DATETIME NOT NULL\n) WITH (MEMORY_OPTIMIZED = ON, DURABILITY = SCHEMA_AND_DATA)\nGO\nSection 68.3: Show created .dll ﬁles and tables for Memory\nOptimized Tables\nSELECT\n    OBJECT_ID('MemOptTable1') AS MemOptTable1_ObjectID,\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 190\n    OBJECT_ID('MemOptTable2') AS MemOptTable2_ObjectID\nGO\nSELECT\n    name,description\nFROM sys.dm_os_loaded_modules\nWHERE name LIKE '%XTP%'\nGO\nShow all Memory Optimized Tables:\nSELECT\n    name,type_desc,durability_desc,Is_memory_Optimized\nFROM sys.tables\n    WHERE Is_memory_Optimized = 1\nGO\nSection 68.4: Create Memory Optimized System-Versioned\nTemporal Table\nCREATE TABLE [dbo].[MemOptimizedTemporalTable]\n(\n    [BusinessDocNo] [bigint] NOT NULL,\n    [ProductCode] [int] NOT NULL,\n    [UnitID] [tinyint] NOT NULL,\n    [PriceID] [tinyint] NOT NULL,\n    [SysStartTime] [datetime2](7) GENERATED ALWAYS AS ROW START NOT NULL,\n    [SysEndTime] [datetime2](7) GENERATED ALWAYS AS ROW END NOT NULL,\n    PERIOD FOR SYSTEM_TIME ([SysStartTime], [SysEndTime]),\n    CONSTRAINT [PK_MemOptimizedTemporalTable]  PRIMARY KEY NONCLUSTERED\n    (\n        [BusinessDocNo] ASC,\n        [ProductCode] ASC\n    )\n)\nWITH (\n    MEMORY_OPTIMIZED = ON , DURABILITY = SCHEMA_AND_DATA, -- Memory Optimized Option ON\n    SYSTEM_VERSIONING = ON (HISTORY_TABLE = [dbo].[MemOptimizedTemporalTable_History] ,\nDATA_CONSISTENCY_CHECK = ON )\n)\nmore information\nSection 68.5: Memory-Optimized Table Types and Temp\ntables\nFor example, this is traditional tempdb-based table type:\nCREATE TYPE dbo.testTableType AS TABLE\n(\n   col1 INT NOT NULL,\n   col2 CHAR(10)\n);\nTo memory-optimize this table type simply add the option memory_optimized=on, and add an index if there is none\non the original type:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 191\nCREATE TYPE dbo.testTableType AS TABLE\n(\n   col1 INT NOT NULL,\n   col2 CHAR(10)\n)WITH (MEMORY_OPTIMIZED=ON);\nGlobal temporary table is like this:\nCREATE TABLE ##tempGlobalTabel\n(  \n    Col1   INT   NOT NULL ,  \n    Col2   NVARCHAR(4000)  \n);  \nMemory-optimized global temporary table:\nCREATE TABLE dbo.tempGlobalTabel\n(  \n    Col1   INT   NOT NULL   INDEX ix NONCLUSTERED,  \n    Col2   NVARCHAR(4000)  \n)  \n    WITH  \n        (MEMORY_OPTIMIZED = ON,  \n         DURABILITY = SCHEMA_ONLY);  \nTo memory-optimize global temp tables (##temp):\nCreate a new SCHEMA_ONLY memory-optimized table with the same schema as the global ##temp table1.\nEnsure the new table has at least one index\nChange all references to ##temp in your Transact-SQL statements to the new memory-optimized table temp2.\nReplace the DROP TABLE ##temp statements in your code with DELETE FROM temp, to clean up the contents3.\nRemove the CREATE TABLE ##temp statements from your code – these are now redundant4.\nmore information\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 192\nChapter 69: Temporal Tables\nSection 69.1: CREATE Temporal Tables\nCREATE TABLE dbo.Employee  \n(    \n  [EmployeeID] int NOT NULL PRIMARY KEY CLUSTERED  \n  , [Name] nvarchar(100) NOT NULL  \n  , [Position] varchar(100) NOT NULL  \n  , [Department] varchar(100) NOT NULL  \n  , [Address] nvarchar(1024) NOT NULL  \n  , [AnnualSalary] decimal (10,2) NOT NULL  \n  , [ValidFrom] datetime2 (2) GENERATED ALWAYS AS ROW START  \n  , [ValidTo] datetime2 (2) GENERATED ALWAYS AS ROW END  \n  , PERIOD FOR SYSTEM_TIME (ValidFrom, ValidTo)  \n )    \n WITH (SYSTEM_VERSIONING = ON (HISTORY_TABLE = dbo.EmployeeHistory));  \nINSERTS: On an INSERT, the system sets the value for the ValidFrom column to the begin time of the current\ntransaction (in the UTC time zone) based on the system clock and assigns the value for the ValidTo column to the\nmaximum value of 9999-12-31. This marks the row as open.\nUPDATES: On an UPDATE, the system stores the previous value of the row in the history table and sets the value\nfor the ValidTo column to the begin time of the current transaction (in the UTC time zone) based on the system\nclock. This marks the row as closed, with a period recorded for which the row was valid. In the current table, the\nrow is updated with its new value and the system sets the value for the ValidFrom column to the begin time for the\ntransaction (in the UTC time zone) based on the system clock. The value for the updated row in the current table for\nthe ValidTo column remains the maximum value of 9999-12-31.\nDELETES: On a DELETE, the system stores the previous value of the row in the history table and sets the value for\nthe ValidTo column to the begin time of the current transaction (in the UTC time zone) based on the system clock.\nThis marks the row as closed, with a period recorded for which the previous row was valid. In the current table, the\nrow is removed. Queries of the current table will not return this row. Only queries that deal with history data return\ndata for which a row is closed.\nMERGE: On a MERGE, the operation behaves exactly as if up to three statements (an INSERT, an UPDATE, and/or a\nDELETE) executed, depending on what is speciﬁed as actions in the MERGE statement.\nTip : The times recorded in the system datetime2 columns are based on the begin time of the transaction itself. For\nexample, all rows inserted within a single transaction will have the same UTC time recorded in the column\ncorresponding to the start of the SYSTEM_TIME period.\nSection 69.2: FOR SYSTEM_TIME ALL\nReturns the union of rows that belong to the current and the history table.\nSELECT * FROM Employee\n    FOR SYSTEM_TIME ALL\nSection 69.3: Creating a Memory-Optimized System-Versioned\nTemporal Table and cleaning up the SQL Server history table\nCreating a temporal table with a default history table is a convenient option when you want to control naming and\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 193\nstill rely on system to create history table with default conﬁguration. In the example below, a new system-versioned\nmemory-optimized temporal table linked to a new disk-based history table.\nCREATE SCHEMA History  \nGO  \nCREATE TABLE dbo.Department  \n(  \n    DepartmentNumber char(10) NOT NULL PRIMARY KEY NONCLUSTERED,  \n    DepartmentName varchar(50) NOT NULL,  \n    ManagerID int  NULL,  \n    ParentDepartmentNumber char(10) NULL,  \n    SysStartTime datetime2 GENERATED ALWAYS AS ROW START HIDDEN NOT NULL,  \n    SysEndTime datetime2 GENERATED ALWAYS AS ROW END HIDDEN NOT NULL,    \n    PERIOD FOR SYSTEM_TIME (SysStartTime,SysEndTime)    \n)  \nWITH  \n    (  \n        MEMORY_OPTIMIZED = ON, DURABILITY = SCHEMA_AND_DATA,  \n        SYSTEM_VERSIONING = ON ( HISTORY_TABLE = History.DepartmentHistory )  \n    );  \nCleaning up the SQL Server history table Over time the history table can grow signiﬁcantly. Since inserting,\nupdating or deleting data from the history table are not allowed, the only way to clean up the history table is ﬁrst to\ndisable system versioning:\nALTER TABLE dbo.Employee\nSET (SYSTEM_VERSIONING = OFF);\nGO\nDelete unnecessary data from the history table:\nDELETE FROM dbo.EmployeeHistory\nWHERE EndTime <= '2017-01-26 14:00:29';\nand then re-enable system versioning:\nALTER TABLE dbo.Employee\nSET (SYSTEM_VERSIONING = ON (HISTORY_TABLE = [dbo].[EmployeeHistory], DATA_CONSISTENCY_CHECK =\nON));\nCleaning the history table in Azure SQL Databases is a little diﬀerent, since Azure SQL databases have built-in\nsupport for cleaning of the history table. First, temporal history retention cleanup need to be enable on a database\nlevel:\nALTER DATABASE CURRENT\nSET TEMPORAL_HISTORY_RETENTION ON\nGO\nThen set the retention period per table:\nALTER TABLE dbo.Employee\nSET (SYSTEM_VERSIONING = ON (HISTORY_RETENTION_PERIOD = 90 DAYS));\nThis will delete all data in the history table older than 90 days. SQL Server 2016 on-premise databases do not\nsupport TEMPORAL_HISTORY_RETENTION and HISTORY_RETENTION_PERIOD and either of the above two queries\nare executed on the SQL Server 2016 on-premise databases the following errors will occur.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 194\nFor TEMPORAL_HISTORY_RETENTION error will be:\nMsg 102, Level 15, State 6, Line 34\nIncorrect syntax near ‘TEMPORAL_HISTORY_RETENTION’.\nFor HISTORY_RETENTION_PERIOD error will be:\nMsg 102, Level 15, State 1, Line 39\nIncorrect syntax near ‘HISTORY_RETENTION_PERIOD’.\nSection 69.4: FOR SYSTEM_TIME BETWEEN\n<start_date_time> AND <end_date_time>\nSame as above in the FOR SYSTEM_TIME FROM <start_date_time>TO <end_date_time> description, except the table\nof rows returned includes rows that became active on the upper boundary deﬁned by the <end_date_time>\nendpoint.\nSELECT * FROM Employee  \n    FOR SYSTEM_TIME BETWEEN  '2015-01-01' AND '2015-12-31'\nSection 69.5: FOR SYSTEM_TIME FROM <start_date_time> TO\n<end_date_time>\nReturns a table with the values for all row versions that were active within the speciﬁed time range, regardless of\nwhether they started being active before the <start_date_time> parameter value for the FROM argument or ceased\nbeing active after the <end_date_time> parameter value for the TO argument. Internally, a union is performed\nbetween the temporal table and its history table and the results are ﬁltered to return the values for all row versions\nthat were active at any time during the time range speciﬁed. Rows that became active exactly on the lower\nboundary deﬁned by the FROM endpoint are included and records that became active exactly on the upper\nboundary deﬁned by the TO endpoint are not included.\nSELECT * FROM Employee  \n    FOR SYSTEM_TIME FROM '2015-01-01' TO '2015-12-31'\nSection 69.6: FOR SYSTEM_TIME CONTAINED IN\n(<start_date_time> , <end_date_time>)\nReturns a table with the values for all row versions that were opened and closed within the speciﬁed time range\ndeﬁned by the two datetime values for the CONTAINED IN argument. Rows that became active exactly on the lower\nboundary or ceased being active exactly on the upper boundary are included.\nSELECT * FROM Employee\n    FOR SYSTEM_TIME CONTAINED IN ('2015-04-01', '2015-09-25')  \nSection 69.7: How do I query temporal data?\nSELECT * FROM Employee  \n    FOR SYSTEM_TIME    \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 195\n        BETWEEN '2014-01-01 00:00:00.0000000' AND '2015-01-01 00:00:00.0000000'  \n            WHERE EmployeeID = 1000 ORDER BY ValidFrom;  \nSection 69.8: Return actual value speciﬁed point in time(FOR\nSYSTEM_TIME AS OF <date_time>)\nReturns a table with a rows containing the values that were actual (current) at the speciﬁed point in time in the past.\nSELECT * FROM Employee  \n    FOR SYSTEM_TIME AS  OF '2016-08-06 08:32:37.91'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 196\nChapter 70: Use of TEMP Table\nSection 70.1: Dropping temp tables\nTemp tables must have unique IDs (within the session, for local temp tables, or within the server, for global temp\ntables). Trying to create a table using a name that already exists will return the following error:\nThere is already an object named '#tempTable' in the database.\nIf your query produces temp tables, and you want to run it more than once, you will need to drop the tables before\ntrying to generate them again. The basic syntax for this is:\ndrop table #tempTable\nTrying to execute this syntax before the table exists (e.g. on the ﬁrst run of your syntax) will cause another error:\nCannot drop the table '#tempTable', because it does not exist or you do not have permission.\nTo avoid this, you can check to see if the table already exists before dropping it, like so:\nIF OBJECT_ID ('tempdb..#tempTable', 'U') is not null DROP TABLE #tempTable\nSection 70.2: Local Temp Table\nWill be available till the current connection persists for the user.\nAutomatically deleted when the user disconnects.\nThe name should start with # (#temp)\n CREATE TABLE #LocalTempTable(\n                StudentID      int,\n                StudentName    varchar(50),\n                StudentAddress varchar(150))\ninsert into #LocalTempTable values ( 1, 'Ram','India');\nselect * from #LocalTempTable\nAfter executing all these statements if we close the query window and open it again and try inserting and select it\nwill show an error message\n“Invalid object name #LocalTempTable”\nSection 70.3: Global Temp Table\nWill start with ## (##temp).\nWill be deleted only if user disconnects all connections.\nIt behaves like a permanent table.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 197\nCREATE TABLE ##NewGlobalTempTable(\n               StudentID      int,\n               StudentName    varchar(50),\n               StudentAddress varchar(150))\nInsert Into ##NewGlobalTempTable values ( 1,'Ram','India');\nSelect * from ##NewGlobalTempTable\nNote: These are viewable by all users of the database, irrespective of permissions level.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 198\nChapter 71: Scheduled Task or Job\nSQL Server Agent uses SQL Server to store job information. Jobs contain one or more job steps. Each step contains\nits own task,i.e: backing up a database. SQL Server Agent can run a job on a schedule, in response to a speciﬁc\nevent, or on demand.\nSection 71.1: Create a scheduled Job\nCreate a Job\nTo add a job ﬁrst we have to use a stored procedure named sp_add_job\nUSE msdb ;  \nGO  \nEXEC dbo.sp_add_job  \n@job_name = N'Weekly Job' ;  -- the job name\nThen we have to add a job step using a stored procedure named sp_add_jobStep\nEXEC sp_add_jobstep  \n@job_name = N'Weekly Job',  -- Job name to add a step\n@step_name = N'Set database to read only',  -- step name\n@subsystem = N'TSQL',  -- Step type\n@command = N'ALTER DATABASE SALES SET READ_ONLY',   -- Command\n@retry_attempts = 5,  --Number of attempts\n@retry_interval = 5 ; -- in minutes\nTarget the job to a server\nEXEC dbo.sp_add_jobserver  \n@job_name = N'Weekly Sales Data Backup',\n@server_name = 'MyPC\\data;   -- Default is LOCAL\nGO\nCreate a schedule using SQL\nTo Create a schedule we have to use a system stored procedure called sp_add_schedule\nUSE msdb\nGO  \nEXEC sp_add_schedule  \n    @schedule_name = N'NightlyJobs' ,  -- specify the schedule name\n    @freq_type = 4,   -- A value indicating when a job is to be executed (4) means Daily\n    @freq_interval = 1,  -- The days that a job is executed and depends on the value of\n`freq_type`.\n    @active_start_time = 010000 ;   -- The time on which execution of a job can begin\nGO  \nThere are more parameters that can be used with sp_add_schedule you can read more about in the the link\nprovided above.\nAttaching schedule to a JOB\nTo attach a schedule to an SQL agent job you have to use a stored procedure called sp_attach_schedule\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 199\n-- attaches the schedule to the job BackupDatabase  \nEXEC sp_attach_schedule  \n   @job_name = N'BackupDatabase',  -- The job name to attach with\n   @schedule_name = N'NightlyJobs' ;  -- The schedule name\nGO  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 200\nChapter 72: Isolation levels and locking\nSection 72.1: Examples of setting the isolation level\nExample of setting the isolation level:\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nSELECT * FROM Products WHERE ProductId=1;\nSET TRANSACTION ISOLATION LEVEL REPEATABLE READ; --return to the default one\nREAD UNCOMMITTED - means that a query in the current transaction can't access the modiﬁed data from1.\nanother transaction that is not yet committed - no dirty reads! BUT, nonrepeatable reads and phantom reads\nare possible, because data can still be modiﬁed by other transactions.\nREPEATABLE READ - means that a query in the the current transaction can't access the modiﬁed data from2.\nanother transaction that is not yet committed - no dirty reads! No other transactions can modify data being\nread by the current transaction until it is completed, which eliminates NONREPEATABLE reads. BUT, if\nanother transaction inserts NEW ROWS and the query is executed more then once, phantom rows can\nappear starting the second read (if it matches the where statement of the query).\nSNAPSHOT - only able to return data that exists at the beginning of the query. Ensures consistency of the data.3.\nIt prevents dirty reads, nonrepeatable reads and phantom reads. To use that - DB conﬁgurationis required:\nALTER DATABASE DBTestName SET ALLOW_SNAPSHOT_ISOLATION ON;GO;\nSET TRANSACTION ISOLATION LEVEL SNAPSHOT;\nREAD COMMITTED - default isolation of the SQL server. It prevents reading the data that is changed by another4.\ntransaction until committed. It uses shared locking and row versioning on the tables which prevents dirty\nreads. It depends on DB conﬁguration READ_COMMITTED_SNAPSHOT - if enabled - row versioning is used. to\nenable - use this:\nALTER DATABASE DBTestName SET ALLOW_SNAPSHOT_ISOLATION ON;GO;\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED; --return to the default one\nSERIALIZABLE - uses physical locks that are acquired and held until end of the transaction, which prevents5.\ndirty reads, phantom reads, nonrepeatable reads. BUT, it impacts on the performance of the DataBase,\nbecause the concurrent transactions are serialized and are being executed one by one.\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE ;\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 201\nChapter 73: Sorting/ordering rows\nSection 73.1: Basics\nFirst, let's setup the example table.\n-- Create a table as an example\nCREATE TABLE SortOrder\n(\n    ID INT IDENTITY PRIMARY KEY,\n    [Text] VARCHAR(256)\n)\nGO\n-- Insert rows into the table\nINSERT INTO SortOrder ([Text])\nSELECT ('Lorem ipsum dolor sit amet, consectetur adipiscing elit')\nUNION ALL SELECT ('Pellentesque eu dapibus libero')\nUNION ALL SELECT ('Vestibulum et consequat est, ut hendrerit ligula')\nUNION ALL SELECT ('Suspendisse sodales est congue lorem euismod, vel facilisis libero pulvinar')\nUNION ALL SELECT ('Suspendisse lacus est, aliquam at varius a, fermentum nec mi')\nUNION ALL SELECT ('Praesent tincidunt tortor est, nec consequat dolor malesuada quis')\nUNION ALL SELECT ('Quisque at tempus arcu')\nGO\nRemember that when retrieving data, if you don't specify a row ordering clause (ORDER BY) SQL server does not\nguarantee the sorting (order of the columns) at any time. Really, at any time. And there's no point arguing about\nthat, it has been shown literally thousands of times and all over the internet.\nNo ORDER BY == no sorting. End of story.\n-- It may seem the rows are sorted by identifiers,\n-- but there is really no way of knowing if it will always work.\n-- And if you leave it like this in production, Murphy gives you a 100% that it won't.\nSELECT * FROM SortOrder\nGO\nThere are two directions data can be ordered by:\nascending (moving upwards), using ASC\ndescending (moving downwards), using DESC\n-- Ascending - upwards\nSELECT * FROM SortOrder ORDER BY ID ASC\nGO\n-- Ascending is default\nSELECT * FROM SortOrder ORDER BY ID\nGO\n-- Descending - downwards\nSELECT * FROM SortOrder ORDER BY ID DESC\nGO\nWhen ordering by the textual column ((n)char or (n)varchar), pay attention that the order respects the collation. For\nmore information on collation look up for the topic.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 202\nOrdering and sorting of data can consume resources. This is where properly created indexes come handy. For more\ninformation on indexes look up for the topic.\nThere is a possibility to pseudo-randomize the order of rows in your resultset. Just force the ordering to appear\nnondeterministic.\nSELECT * FROM SortOrder ORDER BY CHECKSUM(NEWID())\nGO\nOrdering can be remembered in a stored procedure, and that's the way you should do it if it is the last step of\nmanipulating the rowset before showing it to the end user.\nCREATE PROCEDURE GetSortOrder\nAS\n    SELECT *\n    FROM SortOrder\n    ORDER BY ID DESC\nGO\nEXEC GetSortOrder\nGO\nThere is a limited (and hacky) support for ordering in the SQL Server views as well, but be encouraged NOT to use it.\n/* This may or may not work, and it depends on the way\n   your SQL Server and updates are installed */\nCREATE VIEW VwSortOrder1\nAS\n    SELECT TOP 100 PERCENT *\n    FROM SortOrder\n    ORDER BY ID DESC\nGO\nSELECT * FROM VwSortOrder1\nGO\n-- This will work, but hey... should you really use it?\nCREATE VIEW VwSortOrder2\nAS\n    SELECT TOP 99999999 *\n    FROM SortOrder\n    ORDER BY ID DESC\nGO\nSELECT * FROM VwSortOrder2\nGO\nFor ordering you can either use column names, aliases or column numbers in your ORDER BY.\nSELECT *\nFROM SortOrder\nORDER BY [Text]\n-- New resultset column aliased as 'Msg', feel free to use it for ordering\nSELECT ID, [Text] + ' (' + CAST(ID AS nvarchar(10)) + ')' AS Msg\nFROM SortOrder\nORDER BY Msg\n-- Can be handy if you know your tables, but really NOT GOOD for production\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 203\nSELECT *\nFROM SortOrder\nORDER BY 2\nI advise against using the numbers in your code, except if you want to forget about it the moment after you execute\nit.\nSection 73.2: Order by Case\nIf you want to sort your data numerically or alphabetically, you can simply use order by [column]. If you want to\nsort using a custom hierarchy, use a case statement.\nGroup\n-----\nTotal\nYoung\nMiddleAge\nOld\nMale\nFemale\nUsing a basic order by:\nSelect * from MyTable\nOrder by Group\nreturns an alphabetical sort, which isn't always desirable:\nGroup\n-----\nFemale\nMale\nMiddleAge\nOld\nTotal\nYoung\nAdding a 'case' statement, assigning ascending numerical values in the order you want your data sorted:\nSelect * from MyTable\nOrder by case Group\n    when 'Total' then 10\n    when 'Male' then 20\n    when 'Female' then 30\n    when 'Young' then 40\n    when 'MiddleAge' then 50\n    when 'Old' then 60\n    end\nreturns data in the order speciﬁed:\nGroup\n-----\nTotal\nMale\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 204\nFemale\nYoung\nMiddleAge\nOld\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 205\nChapter 74: Privileges or Permissions\nSection 74.1: Simple rules\nGranting permission to create tables\nUSE AdventureWorks;  \nGRANT CREATE TABLE TO MelanieK;  \nGO\nGranting SHOWPLAN permission to an application role\nUSE AdventureWorks2012;  \nGRANT SHOWPLAN TO AuditMonitor;  \nGO  \nGranting CREATE VIEW with GRANT OPTION\nUSE AdventureWorks2012;  \nGRANT CREATE VIEW TO CarmineEs WITH GRANT OPTION;  \nGO\nGranting all rights to a user on a speciﬁc database\nuse YourDatabase\ngo\nexec sp_addrolemember 'db_owner', 'UserName'\ngo\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 206\nChapter 75: SQLCMD\nSection 75.1: SQLCMD.exe called from a batch ﬁle or\ncommand line\necho off\ncls\nsqlcmd.exe -S \"your server name\" -U \"sql user name\" -P \"sql password\" -d \"name of databse\" -Q \"here\nyou may write your query/stored procedure\"\nBatch ﬁles like these can be used to automate tasks, for example to make backups of databases at a speciﬁed time\n(can be scheduled with Task Scheduler) for a SQL Server Express version where Agent Jobs can't be used.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 207\nChapter 76: Resource Governor\nSection 76.1: Reading the Statistics\nselect *\nfrom sys.dm_resource_governor_workload_groups\nselect *\nfrom sys.dm_resource_governor_resource_pools\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 208\nChapter 77: File Group\nSection 77.1: Create ﬁlegroup in database\nWe can create it by two way. First from database properties designer mode:\nAnd by sql scripts:\nUSE master;\nGO\n-- Create the database with the default data\n-- filegroup and a log file. Specify the\n-- growth increment and the max size for the\n-- primary data file.\nCREATE DATABASE TestDB ON PRIMARY\n(\n    NAME = 'TestDB_Primary',\n    FILENAME = 'C:\\Program Files\\Microsoft SQL\nServer\\MSSQL12.MSSQLSERVER\\MSSQL\\DATA\\TestDB_Prm.mdf',\n    SIZE = 1 GB,\n    MAXSIZE = 10 GB,\n    FILEGROWTH = 1 GB\n), FILEGROUP TestDB_FG1\n(\n    NAME = 'TestDB_FG1_1',\n    FILENAME = 'C:\\Program Files\\Microsoft SQL\nServer\\MSSQL12.MSSQLSERVER\\MSSQL\\DATA\\TestDB_FG1_1.ndf',\n    SIZE = 10 MB,\n    MAXSIZE = 10 GB,\n    FILEGROWTH = 1 GB\n),\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 209\n(\n    NAME = 'TestDB_FG1_2',\n    FILENAME = 'C:\\Program Files\\Microsoft SQL\nServer\\MSSQL12.MSSQLSERVER\\MSSQL\\DATA\\TestDB_FG1_2.ndf',\n    SIZE = 10 MB,\n    MAXSIZE = 10 GB,\n    FILEGROWTH = 1 GB\n) LOG ON\n(\n    NAME = 'TestDB_log',\n    FILENAME = 'C:\\Program Files\\Microsoft SQL Server\\MSSQL12.MSSQLSERVER\\MSSQL\\DATA\\TestDB.ldf',\n    SIZE = 10 MB,\n    MAXSIZE = 10 GB,\n    FILEGROWTH = 1 GB\n);\ngo\nALTER DATABASE TestDB MODIFY FILEGROUP TestDB_FG1 DEFAULT;\ngo\n-- Create a table in the user-defined filegroup.\nUSE TestDB;\nGo\nCREATE TABLE MyTable\n(\n    col1 INT PRIMARY KEY,\n    col2 CHAR(8)\n)\nON TestDB_FG1;\nGO\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 210\nChapter 78: Basic DDL Operations in MS\nSQL Server\nSection 78.1: Getting started\nThis section describes some basic DDL (=\"Data Deﬁnition Language\") commands to create a database, a table\nwithin a database, a view and ﬁnally a stored procedure.\nCreate Database\nThe following SQL command creates a new database Northwind on the current server, using pathC:\\Program\nFiles\\Microsoft SQL Server\\MSSQL11.INSTSQL2012\\MSSQL\\DATA\\:\nUSE [master]\nGO\nCREATE DATABASE [Northwind]\n CONTAINMENT = NONE\n ON  PRIMARY\n (\n  NAME = N'Northwind',\n  FILENAME = N'C:\\Program Files\\Microsoft SQL Server\\MSSQL11.INSTSQL2012\\MSSQL\\DATA\\Northwind.mdf'\n, SIZE = 5120KB , MAXSIZE = UNLIMITED, FILEGROWTH = 1024KB\n )\n LOG ON\n (\n  NAME = N'Northwind_log',\n  FILENAME = N'C:\\Program Files\\Microsoft SQL\nServer\\MSSQL11.INSTSQL2012\\MSSQL\\DATA\\Northwind_log.ldf' , SIZE = 1536KB , MAXSIZE = 2048GB ,\nFILEGROWTH = 10%\n )\nGO\nALTER DATABASE [Northwind] SET COMPATIBILITY_LEVEL = 110\nGO\nNote: A T-SQL database consists of two ﬁles, the database ﬁle *.mdf, and its transaction log *.ldf. Both need to be\nspeciﬁed when a new database is created.\nCreate Table\nThe following SQL command creates a new table Categories in the current database, using schema dbo (you can\nswitch database context with Use <DatabaseName>):\nCREATE TABLE dbo.Categories(\n    CategoryID int IDENTITY NOT NULL,\n    CategoryName nvarchar(15) NOT NULL,\n    Description ntext NULL,\n    Picture image NULL,\n      CONSTRAINT PK_Categories PRIMARY KEY CLUSTERED\n      (\n         CategoryID ASC\n      )\n      WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF,\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 211\n            ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON PRIMARY\n) ON PRIMARY TEXTIMAGE_ON PRIMARY\nCreate View\nThe following SQL command creates a new view Summary_of_Sales_by_Year in the current database, using schema\ndbo (you can switch database context with Use <DatabaseName>):\nCREATE VIEW dbo.Summary_of_Sales_by_Year AS\n  SELECT ord.ShippedDate, ord.OrderID, ordSub.Subtotal\n  FROM Orders ord\n  INNER JOIN [Order Subtotals] ordSub ON ord.OrderID = ordSub.OrderID\nThis will join tables Orders and [Order Subtotals] to display the columns ShippedDate, OrderID and Subtotal.\nBecause table [Order Subtotals] has a blank in its name in the Northwind database, it needs to be enclosed in\nsquare brackets.\nCreate Procedure\nThe following SQL command creates a new stored procedure CustOrdersDetail in the current database, using\nschema dbo (you can switch database context with Use <DatabaseName>):\nCREATE PROCEDURE dbo.MyCustOrdersDetail @OrderID int, @MinQuantity int=0\nAS BEGIN\n  SELECT ProductName,\n    UnitPrice=ROUND(Od.UnitPrice, 2),\n    Quantity,\n    Discount=CONVERT(int, Discount * 100),\n    ExtendedPrice=ROUND(CONVERT(money, Quantity * (1 - Discount) * Od.UnitPrice), 2)\n  FROM Products P, [Order Details] Od\n  WHERE Od.ProductID = P.ProductID and Od.OrderID = @OrderID\n  and Od.Quantity>=@MinQuantity\nEND\nThis stored procedure, after it has been created, can be invoked as follows:\nexec dbo.MyCustOrdersDetail 10248\nwhich will return all order details with @OrderId=10248 (and quantity >=0 as default). Or you can specify the\noptional parameter\nexec dbo.MyCustOrdersDetail 10248, 10\nwhich will return only orders with a minimum quantity of 10 (or more).\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 212\nChapter 79: Subqueries\nSection 79.1: Subqueries\nA subquery is a query within another SQL query. A subquery is also called inner query or inner select and the\nstatement containing a subquery is called an outer query or outer select.\nNote\nSubqueries must be enclosed within parenthesis,1.\nAn ORDER BY cannot be used in a subquery.2.\nThe image type such as BLOB, array, text datatypes are not allowed in subqueries.3.\nSubqueries can be used with select, insert, update and delete statement within where, from, select clause along\nwith IN, comparison operators, etc.\nWe have a table named ITCompanyInNepal on which we will perform queries to show subqueries examples:\nExamples: SubQueries With Select Statement\nwith In operator and where clause:\nSELECT *\nFROM ITCompanyInNepal\nWHERE Headquarter IN (SELECT Headquarter\n                      FROM ITCompanyInNepal\n                      WHERE Headquarter = 'USA');\nwith comparison operator and where clause\nSELECT *\nFROM ITCompanyInNepal\nWHERE NumberOfEmployee < (SELECT AVG(NumberOfEmployee)\n                          FROM ITCompanyInNepal\n                      )\nwith select clause\nSELECT   CompanyName,\n         CompanyAddress,\n         Headquarter,\n         (Select SUM(NumberOfEmployee)\n         FROM ITCompanyInNepal\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 213\n         Where Headquarter = 'USA') AS TotalEmployeeHiredByUSAInKathmandu\nFROM     ITCompanyInNepal\nWHERE    CompanyAddress = 'Kathmandu' AND Headquarter = 'USA'\nSubqueries with insert statement\nWe have to insert data from IndianCompany table to ITCompanyInNepal. The table for IndianCompany is shown\nbelow:\nINSERT INTO ITCompanyInNepal\nSELECT *\nFROM IndianCompany\nSubqueries with update statement\nSuppose all the companies whose headquarter is USA decided to ﬁre 50 employees from all US based companies of\nNepal due to some change in policy of USA companies.\nUPDATE ITCompanyInNepal\nSET NumberOfEmployee = NumberOfEmployee - 50\nWHERE Headquarter IN (SELECT Headquarter\n                      FROM ITCompanyInNepal\n                      WHERE Headquarter = 'USA')\nSubqueries with Delete Statement\nSuppose all the companies whose headquarter is Denmark decided to shutdown their companies from Nepal.\nDELETE FROM ITCompanyInNepal\nWHERE Headquarter IN (SELECT Headquarter\n                     FROM ITCompanyInNepal\n                     WHERE Headquarter = 'Denmark')\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 214\nChapter 80: Pagination\nRow Oﬀset and Paging in Various Versions of SQL Server\nSection 80.1: Pagination with OFFSET FETCH\nVersion ≥ SQL Server 2012\nThe OFFSET FETCH clause implements pagination in a more concise manner. With it, it's possible to skip N1 rows\n(speciﬁed in OFFSET) and return the next N2 rows (speciﬁed in FETCH):\nSELECT *\nFROM sys.objects\nORDER BY object_id\nOFFSET 40 ROWS FETCH NEXT 10 ROWS ONLY\nThe ORDER BY clause is required in order to provide deterministic results.\nSection 80.2: Paginaton with inner query\nIn earlier versions of SQL Server, developers had to use double sorting combined with the TOP keyword to return\nrows in a page:\n SELECT TOP 10 *\n FROM\n (\n    SELECT\n    TOP 50 object_id,\n        name,\n        type,\n        create_date\n    FROM sys.objects\n    ORDER BY name ASC\n) AS data\nORDER BY name DESC\nThe inner query will return the ﬁrst 50 rows ordered by name. Then the outer query will reverse the order of these\n50 rows and select the top 10 rows (these will be last 10 rows in the group before the reversal).\nSection 80.3: Paging in Various Versions of SQL Server\nSQL Server 2012 / 2014\nDECLARE @RowsPerPage INT = 10, @PageNumber INT = 4\nSELECT OrderId, ProductId\nFROM OrderDetail\nORDER BY OrderId\nOFFSET (@PageNumber - 1) * @RowsPerPage ROWS\nFETCH NEXT @RowsPerPage ROWS ONLY\nSQL Server 2005/2008/R2\nDECLARE @RowsPerPage INT = 10, @PageNumber INT = 4\nSELECT OrderId, ProductId\nFROM (\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 215\n    SELECT OrderId, ProductId, ROW_NUMBER() OVER (ORDER BY OrderId) AS RowNum\n    FROM OrderDetail) AS OD\nWHERE OD.RowNum BETWEEN ((@PageNumber - 1 ) * @RowsPerPage) + 1\nAND @RowsPerPage * @PageNumber\nSQL Server 2000\nDECLARE @RowsPerPage INT = 10, @PageNumber INT = 4\nSELECT OrderId, ProductId\nFROM (SELECT TOP (@RowsPerPage) OrderId, ProductId\n        FROM (SELECT TOP ((@PageNumber)*@RowsPerPage) OrderId, ProductId\n                FROM OrderDetail\n                ORDER BY OrderId) AS OD\n    ORDER BY OrderId DESC) AS OD2\nORDER BY OrderId ASC\nSection 80.4: SQL Server 2012/2014 using ORDER BY OFFSET\nand FETCH NEXT\nFor getting the next 10 rows just run this query:\nSELECT * FROM TableName ORDER BY id OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY;\nKey points to consider when using it:\nORDER BY is mandatory to use OFFSET and FETCH clause.\nOFFSET clause is mandatory with FETCH. You can never use, ORDER BY … FETCH.\nTOP cannot be combined with OFFSET and FETCH in the same query expression.\nSection 80.5: Pagination using ROW_NUMBER with a Common\nTable Expression\nVersion ≥ SQL Server 2008\nThe ROW_NUMBER function can assign an incrementing number to each row in a result set. Combined with a Common\nTable Expression that uses a BETWEEN operator, it is possible to create 'pages' of result sets. For example: page one\ncontaining results 1-10, page two containing results 11-20, page three containing results 21-30, and so on.\nWITH data\nAS\n(\n    SELECT ROW_NUMBER() OVER (ORDER BY name) AS row_id,\n        object_id,\n        name,\n        type,\n        create_date\n    FROM sys.objects\n)\nSELECT *\nFROM data\nWHERE row_id BETWEEN 41 AND 50\nNote: It is not possible to use ROW_NUMBER in a WHERE clause like:\nSELECT object_id,\n    name,\n    type,\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 216\n    create_date\nFROM sys.objects\nWHERE ROW_NUMBER() OVER (ORDER BY name) BETWEEN 41 AND 50\nAlthough this would be more convenient, SQL server will return the following error in this case:\nMsg 4108, Level 15, State 1, Line 6\nWindowed functions can only appear in the SELECT or ORDER BY clauses.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 217\nChapter 81: CLUSTERED COLUMNSTORE\nSection 81.1: Adding clustered columnstore index on existing\ntable\nCREATE CLUSTERED COLUMNSTORE INDEX enables you to organize a table in column format:\nDROP TABLE IF EXISTS Product\nGO\nCREATE TABLE Product (\n    Name nvarchar(50) NOT NULL,\n    Color nvarchar(15),\n    Size nvarchar(5) NULL,\n    Price money NOT NULL,\n    Quantity int\n)\nGO\nCREATE CLUSTERED COLUMNSTORE INDEX cci ON Product\nSection 81.2: Rebuild CLUSTERED COLUMNSTORE index\nClustered column store index can be rebuilt if you have a lot of deleted rows:\nALTER INDEX cci ON Products\nREBUILD PARTITION = ALL\nRebuilding CLUSTERED COLUMNSTORE will \"reload\" data from the current table into new one and apply\ncompression again, remove deleted rows, etc.\nYou can rebuild one or more partitions.\nSection 81.3: Table with CLUSTERED COLUMNSTORE index\nIf you want to have a table organized in column-store format instead of row store, add INDEX cci CLUSTERED\nCOLUMNSTORE in deﬁnition of table:\nDROP TABLE IF EXISTS Product\nGO\nCREATE TABLE Product (\n    ProductID int,\n    Name nvarchar(50) NOT NULL,\n    Color nvarchar(15),\n    Size nvarchar(5) NULL,\n    Price money NOT NULL,\n    Quantity int,\n    INDEX cci CLUSTERED COLUMNSTORE\n)\nCOLUMNSTORE tables are better for tables where you expect full scans and reports, while row store tables are\nbetter for tables where you will read or update smaller sets of rows.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 218\nChapter 82: Parsename\n'object_name' object_piece\nIs the name of the object for which to retrieve the speciﬁed object part.\nobject_name is sysname. This parameter is an optionally-qualiﬁed object\nname. If all parts of the object name are qualiﬁed, this name can have\nfour parts: the server name, the database name, the owner name, and\nthe object name.\nIs the object part to return. object_piece\nis of type int, and can have these\nvalues:1 = Object name 2 = Schema\nname 3 = Database name 4 = Server\nname\nSection 82.1: PARSENAME\nDeclare @ObjectName nVarChar(1000)\nSet @ObjectName = 'HeadOfficeSQL1.Northwind.dbo.Authors'\nSELECT\n PARSENAME(@ObjectName, 4) as Server\n,PARSENAME(@ObjectName, 3) as DB\n,PARSENAME(@ObjectName, 2) as Owner\n,PARSENAME(@ObjectName, 1) as Object\nReturns:\nServer DB\nHeadoﬃceSQL1 Northwind\nOwner Object\ndbo Authors\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 219\nChapter 83: Installing SQL Server on\nWindows\nSection 83.1: Introduction\nThese are the available editions of SQL Server, as told by the Editions Matrix:\nExpress: Entry-level free database. Includes core-RDBMS functionality. Limited to 10G of disk size. Ideal for\ndevelopment and testing.\nStandard Edition: Standard Licensed edition. Includes core functionality and Business Intelligence\ncapabilities.\nEnterprise Edition: Full-featured SQL Server edition. Includes advanced security and data warehousing\ncapabilities.\nDeveloper Edition: Includes all of the features from Enterprise Edition and no limitations, and it is free to\ndownload and use for development purposes only.\nAfter downloading/acquiring SQL Server, the installation gets executed with SQLSetup.exe, which is available as a\nGUI or a command-line program.\nInstalling via either of these will require you to specify a product key and run some initial conﬁguration that\nincludes enabling features, separate services and setting the initial parameters for each of them. Additional services\nand features can be enabled at any time by running the SQLSetup.exe program in either the command-line or the\nGUI version.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 220\nChapter 84: Analyzing a Query\nSection 84.1: Scan vs Seek\nWhen viewing an execution plan, you may see that SQL Server decided to do a Seek or a Scan.\nA Seek occurs when SQL Server knows where it needs to go and only grab speciﬁc items. This typically occurs when\ngood ﬁlters on put in a query, such as where name = 'Foo'.\nA Scan is when SQL Server doesn't know exactly where all of the data it needs is, or decided that the Scan would be\nmore eﬃcient than a Seek if enough of the data is selected.\nSeeks are typically faster since they are only grabbing a sub-section of the data, whereas Scans are selecting a\nmajority of the data.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 221\nChapter 85: Query Hints\nSection 85.1: JOIN Hints\nWhen you join two tables, SQL Server query optimizer (QO) can choose diﬀerent types of joins that will be used in\nquery:\nHASH join\nLOOP join\nMERGE join\nQO will explore plans and choose the optimal operator for joining tables. However, if you are sure that you know\nwhat would be the optimal join operator, you can specify what kind of JOIN should be used. Inner LOOP join will\nforce QO to choose Nested loop join while joining two tables:\nselect top 100 *\nfrom Sales.Orders o\n    inner loop join Sales.OrderLines ol\n    on o.OrderID = ol.OrderID\ninner merge join will force MERGE join operator:\nselect top 100 *\nfrom Sales.Orders o\n    inner merge join Sales.OrderLines ol\n    on o.OrderID = ol.OrderID\ninner hash join will force HASH join operator:\nselect top 100 *\nfrom Sales.Orders o\n    inner hash join Sales.OrderLines ol\n    on o.OrderID = ol.OrderID\nSection 85.2: GROUP BY Hints\nWhen you use GROUP BY clause, SQL Server query optimizer (QO) can choose diﬀerent types of grouping\noperators:\nHASH Aggregate that creates hash-map for grouping entries\nStream Aggregate that works well with pre-ordered inputs\nYou can explicitly require that QO picks one or another aggregate operator if you know what would be the optimal.\nWith OPTION (ORDER GROUP), QO will always choose Stream aggregate and add Sort operator in front of Stream\naggregate if input is not sorted:\nselect OrderID, AVG(Quantity)\nfrom Sales.OrderLines\ngroup by OrderID\nOPTION (ORDER GROUP)\nWith OPTION (HASH GROUP), QO will always choose Hash aggregate :\nselect OrderID, AVG(Quantity)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 222\nfrom Sales.OrderLines\ngroup by OrderID\nOPTION (HASH GROUP)\nSection 85.3: FAST rows hint\nSpeciﬁes that the query is optimized for fast retrieval of the ﬁrst number_rows. This is a nonnegative integer. After\nthe ﬁrst number_rows are returned, the query continues execution and produces its full result set.\nselect OrderID, AVG(Quantity)\nfrom Sales.OrderLines\ngroup by OrderID\nOPTION (FAST 20)\nSection 85.4: UNION hints\nWhen you use UNION operator on two query results, Query optimizer (QO) can use following operators to create a\nunion of two result sets:\nMerge (Union)\nConcat (Union)\nHash Match (Union)\nYou can explicitly specify what operator should be used using OPTION() hint:\nselect OrderID, OrderDate, ExpectedDeliveryDate, Comments\nfrom Sales.Orders\nwhere OrderDate > DATEADD(day, -1, getdate())\nUNION\nselect PurchaseOrderID as OrderID, OrderDate, ExpectedDeliveryDate, Comments\nfrom Purchasing.PurchaseOrders\nwhere OrderDate > DATEADD(day, -1, getdate())\nOPTION(HASH UNION)\n-- or OPTION(CONCAT UNION)\n-- or OPTION(MERGE UNION)\nSection 85.5: MAXDOP Option\nSpeciﬁes the max degree of parallelism for the query specifying this option.\nSELECT OrderID,\n    AVG(Quantity)\nFROM Sales.OrderLines\nGROUP BY OrderID\nOPTION (MAXDOP 2);\nThis option overrides the MAXDOP conﬁguration option of sp_conﬁgure and Resource Governor. If MAXDOP is set\nto zero then the server chooses the max degree of parallelism.\nSection 85.6: INDEX Hints\nIndex hints are used to force a query to use a speciﬁc index, instead of allowing SQL Server's Query Optimizer to\nchoose what it deems the best index. In some cases you may gain beneﬁts by specifying the index a query must\nuse. Usually SQL Server's Query Optimizer chooses the best index suited for the query, but due to missing/outdated\nstatistics or speciﬁc needs you can force it.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 223\nSELECT *\nFROM mytable WITH (INDEX (ix_date))\nWHERE field1 > 0\n    AND CreationDate > '20170101'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 224\nChapter 86: Query Store\nSection 86.1: Enable query store on database\nQuery store can be enabled on database by using the following command:\nALTER DATABASE tpch SET QUERY_STORE = ON\nSQL Server/Azure SQL Database will collect information about executed queries and provide information in\nsys.query_store views:\nsys.query_store_query\nsys.query_store_query_text\nsys.query_store_plan\nsys.query_store_runtime_stats\nsys.query_store_runtime_stats_interval\nsys.database_query_store_options\nsys.query_context_settings\nSection 86.2: Get execution statistics for SQL queries/plans\nThe following query will return informationa about qeries, their plans and average statistics regarding their\nduration, CPU time, physical and logical io reads.\nSELECT Txt.query_text_id, Txt.query_sql_text, Pl.plan_id,\n        avg_duration, avg_cpu_time,\n        avg_physical_io_reads, avg_logical_io_reads\nFROM sys.query_store_plan AS Pl  \nJOIN sys.query_store_query AS Qry  \n    ON Pl.query_id = Qry.query_id  \nJOIN sys.query_store_query_text AS Txt  \n    ON Qry.query_text_id = Txt.query_text_id\nJOIN sys.query_store_runtime_stats Stats\n    ON Pl.plan_id = Stats.plan_id\nSection 86.3: Remove data from query store\nIf you want to remove some query or query plan from query store, you can use the following commands:\nEXEC sp_query_store_remove_query 4;\nEXEC sp_query_store_remove_plan 3;\nParameters for these stored procedures are query/plan id retrieved from system views.\nYou can also just remove execution statistics for particular plan without removing the plan from the store:\nEXEC sp_query_store_reset_exec_stats 3;  \nParameter provided to this procedure plan id.\nSection 86.4: Forcing plan for query\nSQL Query optimizer will choose the baes possible plan that he can ﬁnd for some query. If you can ﬁnd some plan\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 225\nthat works optimally for some query, you can force QO to always use that plan using the following stored\nprocedure:\nEXEC sp_query_store_unforce_plan @query_id, @plan_id\nFrom this point, QO will always use plan provided for the query.\nIf you want to remove this binding, you can use the following stored procedure:\nEXEC sp_query_store_force_plan @query_id, @plan_id\nFrom this point, QO will again try to ﬁnd the best plan.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 226\nChapter 87: Querying results by page\nSection 87.1: Row_Number()\nSELECT Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName\nFROM Users\nFrom which it will yield a result set with a RowID ﬁeld which you can use to page between.\nSELECT *\nFROM\n    ( SELECT Row_Number() OVER(ORDER BY UserName) As RowID, UserFirstName, UserLastName\n      FROM Users\n    ) As RowResults\nWHERE RowID Between 5 AND 10\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 227\nChapter 88: Schemas\nSection 88.1: Purpose\nSchema refers to a speciﬁc database tables and how they are related to each other. It provides an organisational\nblueprint of how the database is constructed. Additional beneﬁts of implementing database schemas is that\nschemas can be used as a method restricting / granting access to speciﬁc tables within a database.\nSection 88.2: Creating a Schema\nCREATE SCHEMA dvr AUTHORIZATION Owner\n    CREATE TABLE sat_Sales (source int, cost int, partid int)\n    GRANT SELECT ON SCHEMA :: dvr TO  User1\n    DENY SELECT ON SCHEMA :: dvr to User 2\nGO\nSection 88.3: Alter Schema\nALTER SCHEMA dvr\n    TRANSFER dbo.tbl_Staging;\nGO\nThis would transfer the tbl_Staging table from the dbo schema to the dvr schema\nSection 88.4: Dropping Schemas\nDROP SCHEMA dvr\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 228\nChapter 89: Backup and Restore Database\nParameter Details\ndatabase The name of the database to backup or restore\nbackup_device The device to backup or restore the database from, Like {DISK or TAPE}. Can be separated by\ncommas ( , )\nwith_options Various options which can be used while performing the operation. Like formatting the disk where\nthe backup is to be placed or restoring the database with replace option.\nSection 89.1: Basic Backup to disk with no options\nThe following command backs up the 'Users' database to 'D:\\DB_Backup' ﬁle. Its better to not give an extension.\nBACKUP DATABASE Users TO DISK = 'D:\\DB_Backup'\nSection 89.2: Basic Restore from disk with no options\nThe following command restores the 'Users' database from 'D:\\DB_Backup' ﬁle.\nRESTORE DATABASE Users FROM DISK = 'D:\\DB_Backup'\nSection 89.3: RESTORE Database with REPLACE\nWhen you try to restore database from another server you might get the following error:\nError 3154: The backup set holds a backup of a database other than the existing database.\nIn that case you should use WITH REPLACE option to replace database with the database from backup:\nRESTORE DATABASE WWIDW\nFROM DISK = 'C:\\Backup\\WideWorldImportersDW-Full.bak'\nWITH REPLACE\nEven in this case you might get the errors saying that ﬁles cannot be located on some path:\nMsg 3156, Level 16, State 3, Line 1 File 'WWI_Primary' cannot be restored to\n'D:\\Data\\WideWorldImportersDW.mdf'. Use WITH MOVE to identify a valid location for the ﬁle.\nThis error happens probably because your ﬁles were not placed on the same folder path that exist on new server.\nIn that case you should move individual database ﬁles to new location:\nRESTORE DATABASE WWIDW\nFROM DISK = 'C:\\Backup\\WideWorldImportersDW-Full.bak'\nWITH REPLACE,\nMOVE 'WWI_Primary' to 'C:\\Data\\WideWorldImportersDW.mdf',\nMOVE 'WWI_UserData' to 'C:\\Data\\WideWorldImportersDW_UserData.ndf',\nMOVE 'WWI_Log' to 'C:\\Data\\WideWorldImportersDW.ldf',\nMOVE 'WWIDW_InMemory_Data_1' to 'C:\\Data\\WideWorldImportersDW_InMemory_Data_1'\nWith this statement you can replace database with all database ﬁles moved to new location.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 229\nChapter 90: Transaction handling\nParameter Details\ntransaction_name for naming your transaction - useful with the parameter [with mark] which will allow a\nmeaningfull logging -- case-sensitive (!)\nwith mark ['description'] can be added to [transaction_name] and will store a mark in the log\nSection 90.1: basic transaction skeleton with error handling\nBEGIN TRY -- start error handling\n    BEGIN TRANSACTION; -- from here on transactions (modifictions) are not final\n        -- start your statement(s)\n        select 42/0 as ANSWER  -- simple SQL Query with an error\n        -- end your statement(s)\n    COMMIT TRANSACTION; -- finalize all transactions (modifications)\nEND TRY   -- end error handling -- jump to end\nBEGIN CATCH -- execute this IF an error occurred\n        ROLLBACK TRANSACTION; -- undo any transactions (modifications)\n-- put together some information as a query\n    SELECT\n        ERROR_NUMBER() AS ErrorNumber\n        ,ERROR_SEVERITY() AS ErrorSeverity\n        ,ERROR_STATE() AS ErrorState\n        ,ERROR_PROCEDURE() AS ErrorProcedure\n        ,ERROR_LINE() AS ErrorLine\n        ,ERROR_MESSAGE() AS ErrorMessage;\nEND CATCH;  -- final line of error handling\nGO -- execute previous code\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 230\nChapter 91: Natively compiled modules\n(Hekaton)\nSection 91.1: Natively compiled stored procedure\nIn a procedure with native compilation, T-SQL code is compiled to dll and executed as native C code. To create a\nNative Compiled stored Procedure you need to:\nUse standard CREATE PROCEDURE syntax\nSet NATIVE_COMPILATION option in stored procedure deﬁnition\nUse SCHEMABINDING option in stored procedure deﬁnition\nDeﬁne EXECUTE AS OWNER option in stored procedure deﬁnition\nInstead of standard BEGIN END block, you need to use BEGIN ATOMIC block:\nBEGIN ATOMIC\n   WITH (TRANSACTION ISOLATION LEVEL=SNAPSHOT, LANGUAGE='us_english')\n   -- T-Sql code goes here\nEND\nExample:\nCREATE PROCEDURE usp_LoadMemOptTable (@maxRows INT, @FullName NVARCHAR(200))\nWITH\n    NATIVE_COMPILATION,\n    SCHEMABINDING,\n    EXECUTE AS OWNER\nAS\nBEGIN ATOMIC\nWITH (TRANSACTION ISOLATION LEVEL=SNAPSHOT, LANGUAGE='us_english')\n    DECLARE @i INT = 1\n    WHILE @i <= @maxRows\n    BEGIN\n        INSERT INTO dbo.MemOptTable3 VALUES(@i, @FullName, GETDATE())\n        SET @i = @i+1\n    END\nEND\nGO\nSection 91.2: Natively compiled scalar function\nCode in natively compiled function will be transformed into C code and compiled as dll. To create a Native Compiled\nscalar function you need to:\nUse standard CREATE FUNCTION syntax\nSet NATIVE_COMPILATION option in function deﬁnition\nUse SCHEMABINDING option in function deﬁnition\nInstead of standard BEGIN END block, you need to use BEGIN ATOMIC block:\nBEGIN ATOMIC\n   WITH (TRANSACTION ISOLATION LEVEL=SNAPSHOT, LANGUAGE='us_english')\n   -- T-Sql code goes here\nEND\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 231\nExample:\nCREATE FUNCTION [dbo].[udfMultiply]( @v1 int, @v2 int )  \nRETURNS bigint\nWITH NATIVE_COMPILATION, SCHEMABINDING  \nAS  \nBEGIN ATOMIC WITH (TRANSACTION ISOLATION LEVEL = SNAPSHOT, LANGUAGE = N'English')  \n \n    DECLARE @ReturnValue bigint;  \n    SET @ReturnValue = @v1 * @v2;        \n \n    RETURN (@ReturnValue);    \nEND\n-- usage sample:\nSELECT dbo.udfMultiply(10, 12)\nSection 91.3: Native inline table value function\nNative compiled table value function returns table as result. Code in natively compiled function will be transformed\ninto C code and compiled as dll. Only inline table valued functions are supported in version 2016. To create a native\ntable value function you need to:\nUse standard CREATE FUNCTION syntax\nSet NATIVE_COMPILATION option in function deﬁnition\nUse SCHEMABINDING option in function deﬁnition\nInstead of standard BEGIN END block, you need to use BEGIN ATOMIC block:\nBEGIN ATOMIC\n   WITH (TRANSACTION ISOLATION LEVEL=SNAPSHOT, LANGUAGE='us_english')\n   -- T-Sql code goes here\nEND\nExample:\nCREATE FUNCTION [dbo].[udft_NativeGetBusinessDoc]\n(\n   @RunDate VARCHAR(25)\n)\nRETURNS TABLE\nWITH SCHEMABINDING,\n     NATIVE_COMPILATION\nAS\n     RETURN\n(\n    SELECT BusinessDocNo,\n           ProductCode,\n           UnitID,              \n           ReasonID,\n           PriceID,\n           RunDate,\n           ReturnPercent,\n           Qty,\n           RewardAmount,\n           ModifyDate,\n           UserID\n    FROM dbo.[BusinessDocDetail_11]\n    WHERE RunDate >= @RunDate\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 232\n);\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 233\nChapter 92: Spatial Data\nThere are 2 spatial data types\nGeometry X/Y coordinate system for a ﬂat surface\nGeography Latitude/Longitude coordinate system for a curved surface (the earth). There are multiple projections\nof curved surfaces so each geography spatial must let SQL Server know which projection to use. The usual Spatial\nReference ID (SRID) is 4326, which is measuring distances in Kilometers. This is the default SRID used in most web\nmaps\nSection 92.1: POINT\nCreates a single Point. This will be a geometry or geography point depending on the class used.\nParameter Detail\nLat or X Is a ﬂoat expression representing the x-coordinate of the Point being generated\nLong or Y Is a ﬂoat expression representing the y-coordinate of the Point being generated\nString Well Known Text (WKB) of a geometry/geography shape\nBinary Well Known Binary (WKB) of a geometry/geography shape\nSRID Is an int expression representing the spatial reference ID (SRID) of the geometry/geography instance\nyou wish to return\n--Explicit constructor\nDECLARE @gm1 GEOMETRY = GEOMETRY::Point(10,5,0)\nDECLARE @gg1 GEOGRAPHY = GEOGRAPHY::Point(51.511601,-0.096600,4326)\n--Implicit constructor (using WKT - Well Known Text)\nDECLARE @gm1 GEOMETRY = GEOMETRY::STGeomFromText('POINT(5 10)', 0)\nDECLARE @gg1 GEOGRAPHY= GEOGRAPHY::STGeomFromText('POINT(-0.096600 51.511601)', 4326)\n--Implicit constructor (using WKB - Well Known Binary)\nDECLARE @gm1 GEOMETRY = GEOMETRY::STGeomFromWKB(0x010100000000000000000014400000000000002440, 0)\nDECLARE @gg1 GEOGRAPHY= GEOGRAPHY::STGeomFromWKB(0x01010000005F29CB10C7BAB8BFEACC3D247CC14940,\n4326)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 234\nChapter 93: Dynamic SQL\nSection 93.1: Execute SQL statement provided as string\nIn some cases, you would need to execute SQL query placed in string. EXEC, EXECUTE, or system procedure\nsp_executesql can execute any SQL query provided as string:\nsp_executesql N'SELECT * FROM sys.objects'\n-- or\nsp_executesql @stmt = N'SELECT * FROM sys.objects'\n-- or\nEXEC sp_executesql N'SELECT * FROM sys.objects'\n-- or\nEXEC('SELECT * FROM sys.columns')\n-- or\nEXECUTE('SELECT * FROM sys.tables')\nThis procedure will return the same result-set as SQL query provided as statement text. sp_executesql can execute\nSQL query provided as string literal, variable/parameter, or even expression:\ndeclare @table nvarchar(40) = N'product items'\nEXEC(N'SELECT * FROM ' + @table)\ndeclare @sql nvarchar(40) = N'SELECT * FROM ' + QUOTENAME(@table);\nEXEC sp_executesql @sql\nYou need QUOTENAME function to escape special characters in @table variable. Without this function you would\nget syntax error if @table variable contains something like spaces, brackets, or any other special character.\nSection 93.2: Dynamic SQL executed as dierent user\nYou can execute SQL query as diﬀerent user using AS USER = 'name of database user'\nEXEC(N'SELECT * FROM product') AS USER = 'dbo'\nSQL query will be executed under dbo database user. All permission checks applicable to dbo user will be checked\non SQL query.\nSection 93.3: SQL Injection with dynamic SQL\nDynamic queries are\nSET @sql = N'SELECT COUNT(*) FROM AppUsers WHERE Username = ''' + @user + ''' AND Password = ''' +\n@pass + ''''\nEXEC(@sql)\nIf value of user variable is myusername'' OR 1=1 -- the following query will be executed:\nSELECT COUNT(*)\nFROM AppUsers\nWHERE Username = 'myusername' OR 1=1 --' AND Password = ''\nComment at the end of value of variable @username will comment-out trailing part of the query and condition 1=1\nwill be evaluated. Application that checks it there at least one user returned by this query will return count greater\nthan 0 and login will succeed.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 235\nUsing this approach attacker can login into application even if he don't know valid username and password.\nSection 93.4: Dynamic SQL with parameters\nIn order to avoid injection and escaping problems, dynamic SQL queries should be executed with parameters, e.g.:\nSET @sql = N'SELECT COUNT(*) FROM AppUsers WHERE Username = @user AND Password = @pass\nEXEC sp_executesql @sql, '@user nvarchar(50), @pass nvarchar(50)', @username, @password\nSecond parameter is a list of parameters used in query with their types, after this list are provided variables that will\nbe used as parameter values.\nsp_executesql will escape special characters and execute sql query.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 236\nChapter 94: Dynamic data masking\nSection 94.1: Adding default mask on the column\nIf you add default mask on the column, instead of actual value in SELECT statement will be shown mask:\nALTER TABLE  Company\nALTER COLUMN Postcode ADD MASKED WITH (FUNCTION = 'default()')\nSection 94.2: Mask email address using Dynamic data\nmasking\nIf you have email column you can mask it with email() mask:\nALTER TABLE  Company\nALTER COLUMN Email ADD MASKED WITH (FUNCTION = 'email()')\nWhen user tries to select emails from Company table, he will get something like the following values:\nmXXX@XXXX.com\nzXXX@XXXX.com\nrXXX@XXXX.com\nSection 94.3: Add partial mask on column\nYou can add partial mask on the column that will show few characters from te beginning and the end of the string\nand show mask instead of the characters in the middle:\nALTER TABLE  Company\nALTER COLUMN Phone ADD MASKED WITH (FUNCTION = 'partial(5,\"XXXXXXX\",2)')\nIn the parameters of the partial function you can specify how many values from the beginning will be shown, how\nmany values from the end will be shown, and what woudl be the pattern that is shown in the middle.\nWhen user tries to select emails from Company table, he will get something like the following values:\n(381)XXXXXXX39\n(360)XXXXXXX01\n(415)XXXXXXX05\nSection 94.4: Showing random value from the range using\nrandom() mask\nRandom mask will show a rundom number from the speciﬁed range instead of the actual value:\nALTER TABLE  Product\nALTER COLUMN Price ADD MASKED WITH (FUNCTION = 'random(100,200)')\nNote that is some cases displayed value might match actual value in column (if randomly selected number matches\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 237\nvalue in the cell).\nSection 94.5: Controlling who can see unmasked data\nYou can grant in-privileged users right to see unmasked values using the following statement:\nGRANT UNMASK TO MyUser\nIf some user already has unmask permission, you can revoke this permission:\nREVOKE UNMASK TO MyUser\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 238\nChapter 95: Export data in txt ﬁle by using\nSQLCMD\nSection 95.1: By using SQLCMD on Command Prompt\nCommand Structure is\nsqlcmd -S yourservername\\instancename -d database_name -o outputfilename_withpath -Q\n\"your select query\"\nSwitches are as follows\n-S\nfor servername and instance name\n-d\nfor source database\n-o\nfor target outputﬁle (it will create output ﬁle)\n-Q\nfor query to fetch data\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 239\nChapter 96: Common Language Runtime\nIntegration\nSection 96.1: Enable CLR on database\nCLR procedures are not enabled by default. You need to run the following queries to enable CLR:\nsp_configure 'show advanced options', 1;\nGO\nRECONFIGURE;\nGO\nsp_configure 'clr enabled', 1;\nGO\nRECONFIGURE;\nGO\nIn addition, if some CLR module need external access, you should set TRUSTWORTHY property to ON in your\ndatabase:\nALTER DATABASE MyDbWithClr SET TRUSTWORTHY ON\nSection 96.2: Adding .dll that contains Sql CLR modules\nProcedures, functions, triggers, and types written in .Net languages are stored in .dll ﬁles. Once you create .dll ﬁle\ncontaining CLR procedures you should import it into SQL Server:\nCREATE ASSEMBLY MyLibrary\nFROM 'C:\\lib\\MyStoredProcedures.dll'\n    WITH PERMISSION_SET = EXTERNAL_ACCESS\nPERMISSION_SET is Safe by default meaning that code in .dll don't need permission to access external resources\n(e.g. ﬁles, web sites, other servers), and that it will not use native code that can access memory.\nPERMISSION_SET = EXTERNAL_ACCESS is used to mark assemblies that contain code that will access external\nresources.\nyou can ﬁnd information about current CLR assembly ﬁles in sys.assemblies view:\nSELECT *\nFROM sys.assemblies asms\nWHERE is_user_defined = 1\nSection 96.3: Create CLR Function in SQL Server\nIf you have created .Net function, compiled it into .dll, and imported it into SQL server as an assembly, you can\ncreate user-deﬁned function that references function in that assembly:\nCREATE FUNCTION dbo.TextCompress(@input nvarchar(max))\nRETURNS varbinary(max)\nAS EXTERNAL NAME MyLibrary.[Name.Space.ClassName].TextCompress\nYou need to specify name of the function and signature with input parameters and return values that match .Net\nfunction. In AS EXTERNAL NAME clause you need to specify assembly name, namespace/class name where this\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 240\nfunction is placed and name of the method in the class that contains the code that will be exposed as function.\nYou can ﬁnd information about the CLR functions using the following query:\nSELECT * FROM dbo.sysobjects WHERE TYPE ='FS'\nSection 96.4: Create CLR User-deﬁned type in SQL Server\nIf you have create .Net class that represents some user-deﬁned type, compiled it into .dll, and imported it into SQL\nserver as an assembly, you can create user-deﬁned function that references this class:\nCREATE TYPE dbo.Point\nEXTERNAL NAME MyLibrary.[Name.Space.Point]\nYou need to specify name of the type that will be used in T-SQL queries. In EXTERNAL NAME clause you need to\nspecify assembly name, namespace, and class name.\nSection 96.5: Create CLR procedure in SQL Server\nIf you have created .Net method in some class, compiled it into .dll, and imported it into SQL server as an assembly,\nyou can create user-deﬁned stored procedure that references method in that assembly:\nCREATE PROCEDURE dbo.DoSomethng(@input nvarchar(max))\nAS EXTERNAL NAME MyLibrary.[Name.Space.ClassName].DoSomething\nYou need to specify name of the procedure and signature with input parameters that match .Net method. In AS\nEXTERNAL NAME clause you need to specify assembly name, namespace/class name where this procedure is\nplaced and name of the method in the class that contains the code that will be exposed as procedure.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 241\nChapter 97: Delimiting special characters\nand reserved words\nSection 97.1: Basic Method\nThe basic method to escape reserved words for SQL Server is the use of the square brackets ([ and ]). For example,\nDescription and Name are reserved words; however, if there is an object using both as names, the syntax used is:\nSELECT [Description]\nFROM   dbo.TableName\nWHERE  [Name] = 'foo'\nThe only special character for SQL Server is the single quote ' and it is escaped by doubling its usage. For example,\nto ﬁnd the name O'Shea in the same table, the following syntax would be used:\nSELECT [Description]\nFROM   dbo.TableName\nWHERE  [Name] = 'O''Shea'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 242\nChapter 98: DBCC\nSection 98.1: DBCC statement\nDBCC statements act as Database Console Commands for SQL Server. To get the syntax information for the\nspeciﬁed DBCC command use DBCC HELP (...) statement.\nThe following example returns all DBCC statements for which Help is available:\nDBCC HELP ('?');  \nThe following example returns options for DBCC CHECKDB statement:\nDBCC HELP ('CHECKDB');\nSection 98.2: DBCC maintenance commands\nDBCC commands enable user to maintain space in database, clean caches, shrink databases and tables.\nExamples are:\nDBCC DROPCLEANBUFFERS\nRemoves all clean buﬀers from the buﬀer pool, and columnstore objects from the columnstore object pool.\nDBCC FREEPROCCACHE\n-- or\nDBCC FREEPROCCACHE (0x060006001ECA270EC0215D05000000000000000000000000);\nRemoves all SQL query in plan cache. Every new plan will be recompiled: You can specify plan handle, query handle\nto clean plans for the speciﬁc query plan or SQL statement.\nDBCC FREESYSTEMCACHE ('ALL', myresourcepool);\n-- or\nDBCC FREESYSTEMCACHE;\nCleans all cached entries created by system. It can clean entries o=in all or some speciﬁed resource pool\n(myresourcepool in the example above)\nDBCC FLUSHAUTHCACHE\nEmpties the database authentication cache containing information about logins and ﬁrewall rules.\nDBCC SHRINKDATABASE (MyDB [, 10]);\nShrinks database MyDB to 10%. Second parameter is optional. You can use database id instead of name.\nDBCC SHRINKFILE (DataFile1, 7);\nShrinks data ﬁle named DataFile1 in the current database. Target size is 7 MB (tis parameter is optional).\nDBCC CLEANTABLE (AdventureWorks2012,'Production.Document', 0)\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 243\nReclaims a space from speciﬁed table\nSection 98.3: DBCC validation statements\nDBCC commands enable user to validate state of database.\nALTER TABLE Table1 WITH NOCHECK ADD CONSTRAINT chkTab1 CHECK (Col1 > 100);  \nGO  \nDBCC CHECKCONSTRAINTS(Table1);  \n--OR\nDBCC CHECKCONSTRAINTS ('Table1.chkTable1');  \nCheck constraint is added with nocheck options, so it will not be checked on existing data. DBCC will trigger\nconstraint check.\nFollowing DBCC commands check integrity of database, table or catalog:\nDBCC CHECKTABLE tablename1 | tableid\nDBCC CHECKDB databasename1 | dbid\nDBCC CHECKFILEGROUP filegroup_name | filegroup_id | 0\nDBCC CHECKCATALOG databasename1 | database_id1 | 0\nSection 98.4: DBCC informational statements\nDBCC commands can show information about database objects.\nDBCC PROCCACHE\nDisplays information in a table format about the procedure cache.\nDBCC OUTPUTBUFFER ( session_id [ , request_id ])  \nReturns the current output buﬀer in hexadecimal and ASCII format for the speciﬁed session_id (and optional\nrequest_id).\nDBCC INPUTBUFFER ( session_id [ , request_id ])  \nDisplays the last statement sent from a client to an instance of Microsoft SQL Server.\nDBCC SHOW_STATISTICS ( table_or_indexed_view_name , column_statistic_or_index_name)\nSection 98.5: DBCC Trace commands\nTrace ﬂags in SQL Server are used to modify behavior of SQL server, turn on/oﬀ some features. DBCC commands\ncan control trace ﬂags:\nThe following example switches on trace ﬂag 3205 globally and 3206 for the current session:\nDBCC TRACEON (3205, -1);\nDBCC TRACEON (3206);\nThe following example switches oﬀ trace ﬂag 3205 globally and 3206 for the current session:\nDBCC TRACEON (3205, -1);\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 244\nDBCC TRACEON (3206);\nThe following example displays the status of trace ﬂags 2528 and 3205:\nDBCC TRACESTATUS (2528, 3205);  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 245\nChapter 99: BULK Import\nSection 99.1: BULK INSERT\nBULK INSERT command can be used to import ﬁle into SQL Server:\nBULK INSERT People\nFROM 'f:\\orders\\people.csv'  \nBULK INSERT command will map columns in ﬁles with columns in target table.\nSection 99.2: BULK INSERT with options\nYou can customize parsing rules using diﬀerent options in WITH clause:\nBULK INSERT People\nFROM 'f:\\orders\\people.csv'  \nWITH  (  CODEPAGE = '65001',  \n         FIELDTERMINATOR =',',  \n         ROWTERMINATOR ='\\n'  \n      );\nIn this example, CODEPAGE speciﬁes that a source ﬁle in UTF-8 ﬁle, and TERMINATORS are coma and new line.\nSection 99.3: Reading entire content of ﬁle using\nOPENROWSET(BULK)\nYou can read content of ﬁle using OPENROWSET(BULK) function and store content in some table:\nINSERT INTO myTable(content)  \n   SELECT BulkColumn\n          FROM OPENROWSET(BULK N'C:\\Text1.txt', SINGLE_BLOB) AS Document;\nSINGLE_BLOB option will read entire content from a ﬁle as single cell.\nSection 99.4: Read ﬁle using OPENROWSET(BULK) and format\nﬁle\nYu can deﬁne format of the ﬁle that will be imported using FORMATFILE option:\nINSERT INTO mytable\nSELECT a.*\nFROM OPENROWSET(BULK 'c:\\test\\values.txt',  \n   FORMATFILE = 'c:\\test\\values.fmt') AS a;  \nThe format ﬁle, format_ﬁle.fmt, describes the columns in values.txt:\n9.0  \n2  \n1  SQLCHAR  0  10 \"\\t\"        1  ID                SQL_Latin1_General_Cp437_BIN  \n2  SQLCHAR  0  40 \"\\r\\n\"      2  Description       SQL_Latin1_General_Cp437_BIN  \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 246\nSection 99.5: Read json ﬁle using OPENROWSET(BULK)\nYou can use OPENROWSET to read content of ﬁle and pass it to some other function that will parse results.\nThe following example shows hot to read entire content of JSON ﬁle using OPENROWSET(BULK) and then provide\nBulkColumn to OPENJSON function that will parse JSON and return columns:\nSELECT book.*\n FROM OPENROWSET (BULK 'C:\\JSON\\Books\\books.json', SINGLE_CLOB) as j\n CROSS APPLY OPENJSON(BulkColumn)\n       WITH( id nvarchar(100), name nvarchar(100), price float,\n             pages int, author nvarchar(100)) AS book\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 247\nChapter 100: Service broker\nSection 100.1: Basics\nService broker is technology based on asyncronous communication between two(or more) entities. Service broker\nconsists of: message types, contracts, queues, services, routes, and at least instance endpoints\nMore: https://msdn.microsoft.com/en-us/library/bb522893.aspx\nSection 100.2: Enable service broker on database\nALTER DATABASE [MyDatabase] SET ENABLE_BROKER WITH ROLLBACK IMMEDIATE;\nSection 100.3: Create basic service broker construction on\ndatabase (single database communication)\nUSE [MyDatabase]  \nCREATE MESSAGE TYPE [//initiator] VALIDATION = WELL_FORMED_XML;\n    GO\nCREATE CONTRACT [//call/contract]\n(\n    [//initiator] SENT BY INITIATOR\n)\nGO\nCREATE QUEUE  InitiatorQueue;\nGO\nCREATE QUEUE  TargetQueue;\nGO\nCREATE SERVICE InitiatorService\n      ON QUEUE InitiatorQueue\n(\n     [//call/contract]    \n     \n)\nCREATE SERVICE TargetService\nON QUEUE TargetQueue\n(\n     [//call/contract]    \n     \n)\nGRANT SEND ON SERVICE::[InitiatorService] TO PUBLIC\nGO\nGRANT SEND ON SERVICE::[TargetService] TO PUBLIC\nGO\nWe don't need route for one database communication.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 248\nSection 100.4: How to send basic communication through\nservice broker\nFor this demonstration we will use service broker construction created in another part of this documentation.\nMentioned part is named 3. Create basic service broker construction on database (single database\ncommunication).\nUSE [MyDatabase]\nDECLARE @ch uniqueidentifier = NEWID()\nDECLARE @msg XML\nBEGIN DIALOG CONVERSATION @ch\n    FROM SERVICE [InitiatorService]\n    TO SERVICE 'TargetService'\n    ON CONTRACT [//call/contract]\n    WITH ENCRYPTION = OFF; -- more possible options\n        SET @msg = (\n                    SELECT 'HelloThere' \"elementNum1\"\n                    FOR XML PATH(''), ROOT('ExampleRoot'), ELEMENTS XSINIL, TYPE\n                    );        \n     \nSEND ON CONVERSATION @ch MESSAGE TYPE [//initiator] (@msg);\nEND CONVERSATION @ch;\nAfter this conversation will be your msg in TargetQueue\nSection 100.5: How to receive conversation from TargetQueue\nautomatically\nFor this demonstration we will use service broker construction created in another part of this documentation.\nMentioned part is called 3. Create basic service broker construction on database (single database\ncommunication).\nFirst we need to create a procedure that is able to read and process data from the Queue\nUSE [MyDatabase]\nGO\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\nCREATE PROCEDURE [dbo].[p_RecieveMessageFromTargetQueue]\n   \n    AS\n    BEGIN\n   \n    declare\n    @message_body xml,\n    @message_type_name nvarchar(256),\n    @conversation_handle uniqueidentifier,\n    @messagetypename nvarchar(256);\n   \n             \nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 249\n   WHILE 1=1\n   BEGIN\n   \n    BEGIN TRANSACTION\n        WAITFOR(\n        RECEIVE TOP(1)\n        @message_body = CAST(message_body as xml),\n        @message_type_name = message_type_name,\n        @conversation_handle = conversation_handle,\n        @messagetypename = message_type_name\n        FROM DwhInsertSmsQueue\n        ), TIMEOUT 1000;\n   \n         IF (@@ROWCOUNT = 0)\n            BEGIN\n                ROLLBACK TRANSACTION\n                BREAK\n            END\n         IF (@messagetypename = '//initiator')\n             BEGIN\n               \n                IF OBJECT_ID('MyDatabase..MyExampleTableHelloThere') IS NOT NULL\n                    DROP TABLE dbo.MyExampleTableHelloThere\n               \n                SELECT @message_body.value('(/ExampleRoot/\"elementNum1\")[1]', 'VARCHAR(50)') AS\nMyExampleMessage\n                INTO dbo.MyExampleTableHelloThere\n             \n             END\n   \n           \n \n         IF (@messagetypename = 'http://schemas.microsoft.com/SQL/ServiceBroker/EndDialog')\n            BEGIN\n                END CONVERSATION @conversation_handle;\n            END\n   \n    COMMIT TRANSACTION\n   END\n   \nEND\nSecond step: Allow your TargetQueue to automatically run your procedure:\nUSE [MyDatabase]\nALTER QUEUE [dbo].[TargetQueue] WITH STATUS = ON , RETENTION = OFF ,\nACTIVATION\n (  STATUS = ON , --activation status\n    PROCEDURE_NAME = dbo.p_RecieveMessageFromTargetQueue , --procedure name\n    MAX_QUEUE_READERS = 1 , --number of readers\n    EXECUTE AS SELF  )\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 250\nChapter 101: Permissions and Security\nSection 101.1: Assign Object Permissions to a user\nIn Production its good practice to secure your data and only allow operations on it to be undertaken via Stored\nProcedures. This means your application can't directly run CRUD operations on your data and potentially cause\nproblems. Assigning permissions is a time-consuming, ﬁddly and generally onerous task. For this reason its often\neasier to harness some of the (considerable) power contained in the INFORMATION_SCHEMA er schema which is\ncontained in every SQL Server database.\nInstead individually assigning permissions to a user on a piece-meal basis, just run the script below, copy the output\nand then run it in a Query window.\nSELECT 'GRANT EXEC ON core.' + r.ROUTINE_NAME + ' TO ' + <MyDatabaseUsername>\nFROM INFORMATION_SCHEMA.ROUTINES r\nWHERE r.ROUTINE_CATALOG = '<MyDataBaseName>'\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 251\nChapter 102: Database permissions\nSection 102.1: Changing permissions\nGRANT SELECT ON [dbo].[someTable] TO [aUser];\nREVOKE SELECT ON [dbo].[someTable] TO [aUser];\n--REVOKE SELECT [dbo].[someTable] FROM [aUser]; is equivalent\nDENY SELECT ON [dbo].[someTable] TO [aUser];\nSection 102.2: CREATE USER\n--implicitly map this user to a login of the same name as the user\nCREATE USER [aUser];\n--explicitly mapping what login the user should be associated with\nCREATE USER [aUser] FOR LOGIN [aUser];\nSection 102.3: CREATE ROLE\nCREATE ROLE [myRole];\nSection 102.4: Changing role membership\n-- SQL 2005+\nexec sp_addrolemember @rolename = 'myRole', @membername = 'aUser';\nexec sp_droprolemember @rolename = 'myRole', @membername = 'aUser';\n-- SQL 2008+\nALTER ROLE [myRole] ADD MEMBER [aUser];\nALTER ROLE [myRole] DROP MEMBER [aUser];\nNote: role members can be any database-level principal. That is, you can add a role as a member in another role.\nAlso, adding/dropping role members is idempotent. That is, attempting to add/drop will result in their\npresence/absence (respectively) in the role regardless of the current state of their role membership.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 252\nChapter 103: Row-level security\nSection 103.1: RLS ﬁlter predicate\nSql Server 2016+ and Azure Sql database enables you to automatically ﬁlter rows that are returned in select\nstatement using some predicate. This feature is called Row-level security.\nFirst, you need a table-valued function that contains some predicate that describes what it the condition that will\nallow users to read data from some table:\nDROP FUNCTION IF EXISTS dbo.pUserCanAccessCompany\nGO\nCREATE FUNCTION\ndbo.pUserCanAccessCompany(@CompanyID int)\n    RETURNS TABLE\n    WITH SCHEMABINDING\nAS RETURN (\n    SELECT 1 as canAccess WHERE\n    CAST(SESSION_CONTEXT(N'CompanyID') as int) = @CompanyID\n)\nIn this example, the predicate says that only users that have a value in SESSION_CONTEXT that is matching input\nargument can access the company. You can put any other condition e.g. that checks database role or database_id\nof the current user, etc.\nMost of the code above is a template that you will copy-paste. The only thing that will change here is the\nname and arguments of predicate and condition in WHERE clause. Now you create security policy that will\napply this predicate on some table.\nNow you can create security policy that will apply predicate on some table:\nCREATE SECURITY POLICY dbo.CompanyAccessPolicy\n    ADD FILTER PREDICATE dbo.pUserCanAccessCompany(CompanyID) ON dbo.Company\n    WITH (State=ON)\nThis security policy assigns predicate to company table. Whenever someone tries to read data from Company table\n, security policy will apply predicate on each row, pass CompanyID column as a parameter of the predicate, and\npredicate will evaluate should this row be returned in the result of SELECT query.\nSection 103.2: Altering RLS security policy\nSecurity policy is a group of predicates associated to tables that can be managed together. You can add, or remove\npredicates or turn on/oﬀ entire policy.\nYou can add more predicates on tables in the existing security policy.\nALTER SECURITY POLICY dbo.CompanyAccessPolicy\n    ADD FILTER PREDICATE dbo.pUserCanAccessCompany(CompanyID) ON dbo.Company\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 253\nYou can drop some predicates from security policy:\nALTER SECURITY POLICY dbo.CompanyAccessPolicy\n    DROP FILTER PREDICATE ON dbo.Company\nYou can disable security policy\nALTER SECURITY POLICY dbo.CompanyAccessPolicy WITH ( STATE = OFF );  \nYou can enable security policy that was disabled:\nALTER SECURITY POLICY dbo.CompanyAccessPolicy WITH ( STATE = ON );  \nSection 103.3: Preventing updated using RLS block predicate\nRow-level security enables you to deﬁne some predicates that will control who could update rows in the table. First\nyou need to deﬁne some table-value function that represents predicate that wll control access policy.\nCREATE FUNCTION\ndbo.pUserCanAccessProduct(@CompanyID int)\nRETURNS TABLE\nWITH SCHEMABINDING\nAS RETURN (\n   SELECT 1 as canAccess WHERE\n   CAST(SESSION_CONTEXT(N'CompanyID') as int) = @CompanyID\n)\nIn this example, the predicate says that only users that have a value in SESSION_CONTEXT that is matching input\nargument can access the company. You can put any other condition e.g. that checks database role or database_id\nof the current user, etc.\nMost of the code above is a template that you will copy-paste. The only thing that will change here is the\nname and arguments of predicate and condition in WHERE clause. Now you create security policy that will\napply this predicate on some table.\nNow we can create security policy with the predicate that will block updates on product table if CompanyID column\nin table do not satisﬁes predicate.\nCREATE SECURITY POLICY dbo.ProductAccessPolicy\nADD BLOCK PREDICATE dbo.pUserCanAccessProduct(CompanyID) ON dbo.Product\nThis predicate will be applied on all operations. If you want to apply predicate on some operation you can write\nsomething like:\nCREATE SECURITY POLICY dbo.ProductAccessPolicy\nADD BLOCK PREDICATE dbo.pUserCanAccessProduct(CompanyID) ON dbo.Product AFTER INSERT\nPossible options that you can add after block predicate deﬁnition are:\n[ { AFTER { INSERT | UPDATE } }\n| { BEFORE { UPDATE | DELETE } } ]\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 254\nChapter 104: Encryption\nOptional Parameters Details\nWITH PRIVATE KEY For CREATE CERTIFICATE, a private key can be speciﬁed:\n(FILE='D:\\Temp\\CertTest\\private.pvk', DECRYPTION BY PASSWORD = 'password');\nSection 104.1: Encryption by certiﬁcate\nCREATE CERTIFICATE My_New_Cert\nFROM FILE = 'D:\\Temp\\CertTest\\certificateDER.cer'\nGO\nCreate the certiﬁcate\nSELECT EncryptByCert(Cert_ID('My_New_Cert'),\n'This text will get encrypted') encryption_test\nUsually, you would encrypt with a symmetric key, that key would get encrypted by the asymmetric key (public key)\nfrom your certiﬁcate.\nAlso, note that encryption is limited to certain lengths depending on key length and returns NULL otherwise.\nMicrosoft writes: \"The limits are: a 512 bit RSA key can encrypt up to 53 bytes, a 1024 bit key can encrypt up to 117\nbytes, and a 2048 bit key can encrypt up to 245 bytes.\"\nEncryptByAsymKey has the same limits. For UNICODE this would be divided by 2 (16 bits per character), so 58\ncharacters for a 1024 bit key.\nSection 104.2: Encryption of database\nUSE TDE\nCREATE DATABASE ENCRYPTION KEY\nWITH ALGORITHM = AES_256\nENCRYPTION BY SERVER CERTIFICATE My_New_Cert\nGO\nALTER DATABASE TDE\nSET ENCRYPTION ON\nGO\nThis uses 'Transparent Data Encryption' (TDE)\nSection 104.3: Encryption by symmetric key\n-- Create the key and protect it with the cert\nCREATE SYMMETRIC KEY My_Sym_Key\nWITH ALGORITHM = AES_256  \nENCRYPTION BY CERTIFICATE My_New_Cert;\nGO\n-- open the key\nOPEN SYMMETRIC KEY My_Sym_Key\nDECRYPTION BY CERTIFICATE My_New_Cert;\n-- Encrypt\nSELECT EncryptByKey(Key_GUID('SSN_Key_01'), 'This text will get encrypted');\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 255\nSection 104.4: Encryption by passphrase\nSELECT EncryptByPassphrase('MyPassPhrase', 'This text will get encrypted')\nThis will also encrypt but then by passphrase instead of asymmetric(certiﬁcate) key or by an explicit symmetric key.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 256\nChapter 105: PHANTOM read\nIn database systems, isolation determines how transaction integrity is visible to other users and systems, so it\ndeﬁnes how/when the changes made by one operation become visible to other. The phantom read may occurs\nwhen you getting data not yet commited to database.\nSection 105.1: Isolation level READ UNCOMMITTED\nCreate a sample table on a sample database\nCREATE TABLE [dbo].[Table_1](\n    [Id] [int] IDENTITY(1,1) NOT NULL,\n    [title] [varchar](50) NULL,\n CONSTRAINT [PK_Table_1] PRIMARY KEY CLUSTERED\n(\n    [Id] ASC\n)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON,\nALLOW_PAGE_LOCKS = ON) ON [PRIMARY]\n) ON [PRIMARY]\nNow open a First query editor (on the database) insert the code below, and execute (do not touch the --rollback)\nin this case you insert a row on DB but do not commit changes.\nbegin tran\nINSERT INTO Table_1 values('Title 1')\nSELECT * FROM [Test].[dbo].[Table_1]\n--rollback\nNow open a Second Query Editor (on the database), insert the code below and execute\nbegin tran\nset transaction isolation level READ UNCOMMITTED\nSELECT * FROM [Test].[dbo].[Table_1]\nYou may notice that on second editor you can see the newly created row (but not committed) from ﬁrst transaction.\nOn ﬁrst editor execute the rollback (select the rollback word and execute).\n-- Rollback the first transaction\nrollback\nExecute the query on second editor and you see that the record disappear (phantom read), this occurs because you\ntell, to the 2nd transaction to get all rows, also the uncommitteds.\nThis occurs when you change the isolation level with\nset transaction isolation level READ UNCOMMITTED\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 257\nChapter 106: Filestream\nFILESTREAM integrates the SQL Server Database Engine with an NTFS ﬁle system by storing varbinary(max) binary\nlarge object (BLOB) data as ﬁles on the ﬁle system. Transact-SQL statements can insert, update, query, search, and\nback up FILESTREAM data. Win32 ﬁle system interfaces provide streaming access to the data.\nSection 106.1: Example\nSource : MSDN https://technet.microsoft.com/en-us/library/bb933993(v=sql.105).aspx\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 258\nChapter 107: bcp (bulk copy program)\nUtility\nThe bulk copy program utility (bcp) bulk copies data between an instance of Microsoft SQL Server and a data ﬁle in\na user-speciﬁed format. The bcp utility can be used to import large numbers of new rows into SQL Server tables or\nto export data out of tables into data ﬁles.\nSection 107.1: Example to Import Data without a Format\nFile(using Native Format )\nREM Truncate table (for testing)\nSQLCMD -Q \"TRUNCATE TABLE TestDatabase.dbo.myNative;\"\nREM Import data\nbcp TestDatabase.dbo.myNative IN D:\\BCP\\myNative.bcp -T -n\nREM Review results\nSQLCMD -Q \"SELECT * FROM TestDatabase.dbo.myNative;\"\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 259\nChapter 108: SQL Server Evolution through\ndierent versions (2000 - 2016)\nI am using SQL Server since 2004. I started with 2000 and now I am going to use SQL Server 2016. I created tables,\nviews, functions, triggers, stored procedures and wrote many SQL queries but I did not use many new features\nfrom subsequent versions. I googled it but unfortunately, I did not ﬁnd all the features in one place. So I gathered\nand validated these information from diﬀerent sources and put here. I am just adding the high level information for\nall the versions starting from 2000 to 20\nSection 108.1: SQL Server Version 2000 - 2016\nThe following features added in SQL Server 2000 from its previous version:\nNew data types were added (BIGINT, SQL_VARIANT, TABLE)1.\nInstead of and for Triggers were introduced as advancement to the DDL.2.\nCascading referential integrity.3.\nXML support4.\nUser deﬁned functions and partition views.5.\nIndexed Views (Allowing index on views with computed columns).6.\nThe following features added in version 2005 from its previous version:\nEnhancement in TOP clause with “WITH TIES” option.1.\nData Manipulation Commands (DML) and OUTPUT clause to get INSERTED and DELETED values2.\nThe PIVOT and UNPIVOT operators.3.\nException Handling with TRY/CATCH block4.\nRanking functions5.\nCommon Table Expressions (CTE)6.\nCommon Language Runtime (Integration of .NET languages to build objects like stored procedures, triggers,7.\nfunctions etc.)\nService Broker (Handling message between a sender and receiver in a loosely coupled manner)8.\nData Encryption (Native capabilities to support encryption of data stored in user deﬁned databases)9.\nSMTP mail10.\nHTTP endpoints (Creation of endpoints using simple T-SQL statement exposing an object to be accessed over11.\nthe internet)\nMultiple Active Result Sets (MARS).This allows a persistent database connection from a single client to have12.\nmore than one active request per connection.\nSQL Server Integration Services (Will be used as a primary ETL (Extraction, Transformation and Loading) Tool13.\nEnhancements in Analysis Services and Reporting Services.14.\nTable and index partitioning. Allows partitioning of tables and indexes based on partition boundaries as15.\nspeciﬁed by a PARTITION FUNCTION with individual partitions mapped to ﬁle groups via a PARTITION\nSCHEME.\nThe following features added in version 2008 from its previous version:\nEnhancement in existing DATE and TIME Data Types1.\nNew functions like – SYSUTCDATETIME() and SYSDATETIMEOFFSET()2.\nSpare Columns – To save a signiﬁcant amount of disk space.3.\nLarge User Deﬁned Types (up to 2 GB in size)4.\nIntroduced a new feature to pass a table datatype into stored procedures and functions5.\nNew MERGE command for INSERT, UPDATE and DELETE operations6.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 260\nNew HierarchyID datatype7.\nSpatial datatypes - To represent the physical location and shape of any geometric object.8.\nFaster queries and reporting with GROUPING SETS - An extension to the GROUP BY clause.9.\nEnhancement to FILESTREAM storage option10.\nThe following features added in version 2008 R2 from its previous version:\nPowerPivot – For processing large data sets.1.\nReport Builder 3.02.\nCloud ready3.\nStreamInsight4.\nMaster Data Services5.\nSharePoint Integration6.\nDACPAC (Data-tier Application Component Packages)7.\nEnhancement in other features of SQL Server 20088.\nThe following features added in version 2012 from its previous version:\nColumn store indexes - reduces I/O and memory utilization on large queries.1.\nPagination - pagination can be done by using “OFFSET” and “FETCH’ commands.2.\nContained database – Great feature for periodic data migrations.3.\nAlwaysOn Availability Groups4.\nWindows Server Core Support5.\nUser-Deﬁned Server Roles6.\nBig Data Support7.\nPowerView8.\nSQL Azure Enhancements9.\nTabular Model (SSAS)10.\nDQS Data quality services11.\nFile Table - an enhancement to the FILESTREAM feature which was introduced in 2008.12.\nEnhancement in Error Handling including THROW statement13.\nImprovement to SQL Server Management Studio Debugging a. SQL Server 2012 introduces more options to14.\ncontrol breakpoints. b. Improvements to debug-mode windows\nc. Enhancement in IntelliSense - like Inserting Code Snippets.\nThe following features added in version 2014 from its previous version:\nIn-Memory OLTP Engine – Improves performance up to 20 times.1.\nAlwaysOn Enhancements2.\nBuﬀer Pool Extension3.\nHybrid Cloud Features4.\nEnhancement in Column store Indexes (like Updatable Column store Indexes)5.\nQuery Handling Enhancements (like parallel SELECT INTO)6.\nPower BI for Oﬃce 365 Integration7.\nDelayed durability8.\nEnhancements for Database Backups9.\nThe following features added in version 2016 from its previous version:\nAlways Encrypted - Always Encrypted is designed to protect data at rest or in motion.1.\nReal-time Operational Analytics2.\nPolyBase into SQL Server3.\nNative JSON Support4.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 261\nQuery Store5.\nEnhancements to AlwaysOn6.\nEnhanced In-Memory OLTP7.\nMultiple TempDB Database Files8.\nStretch Database9.\nRow Level Security10.\nIn-Memory Enhancements11.\nT-SQL Enhancements or new additions in SQL Server 2016\nTRUNCATE TABLE with PARTITION1.\nDROP IF EXISTS2.\nSTRING_SPLIT and STRING_ESCAPE Functions3.\nALTER TABLE can now alter many columns while the table remains online, using WITH (ONLINE = ON | OFF).4.\nMAXDOP for DBCC CHECKDB, DBCC CHECKTABLE and DBCC CHECKFILEGROUP5.\nALTER DATABASE SET AUTOGROW_SINGLE_FILE6.\nALTER DATABASE SET AUTOGROW_ALL_FILES7.\nCOMPRESS and DECOMPRESS Functions8.\nFORMATMESSAGE Statement9.\n2016 introduces 8 more properties with SERVERPROPERTY10.\na. InstanceDefaultDataPath\nb. InstanceDefaultLogPath\nc. ProductBuild\nd. ProductBuildType\ne. ProductMajorVersion\nf. ProductMinorVersion\ng. ProductUpdateLevel\nh. ProductUpdateReference\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 262\nChapter 109: SQL Server Management\nStudio (SSMS)\nSQL Server Management Studio (SSMS) is a tool to manage and administer SQL Server and SQL Database.\nSSMS is oﬀered free of charge by Microsoft.\nSSMS Documentation is available.\nSection 109.1: Refreshing the IntelliSense cache\nWhen objects are created or modiﬁed they are not automatically available for IntelliSense. To make them available\nto IntelliSense the local cache has to be refreshed.\nWithin an query editor window either press Ctrl + Shift + R or select Edit | IntelliSense | Refresh Local\nCache from the menu.\nAfter this all changes since the last refresh will be available to IntelliSense.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 263\nChapter 110: Managing Azure SQL\nDatabase\nSection 110.1: Find service tier information for Azure SQL\nDatabase\nAzure SQL Database has diﬀerent editions and performance tiers.\nYou can ﬁnd version, edition (basic, standard, or premium), and service objective (S0,S1,P4,P11, etc.) of SQL\nDatabase that is running as a service in Azure using the following statements:\nselect @@version\nSELECT DATABASEPROPERTYEX('Wwi', 'EDITION')\nSELECT DATABASEPROPERTYEX('Wwi', 'ServiceObjective')\nSection 110.2: Change service tier of Azure SQL Database\nYou can scale-up or scale-down Azure SQL database using ALTER DATABASE statement:\nALTER DATABASE WWI\nMODIFY (SERVICE_OBJECTIVE = 'P6')\n-- or\nALTER DATABASE CURRENT\nMODIFY (SERVICE_OBJECTIVE = 'P2')\nIf you try to change service level while changing service level of the current database is still in progress you wil get\nthe following error:\nMsg 40802, Level 16, State 1, Line 1 A service objective assignment on server '......' and database '.......' is\nalready in progress. Please wait until the service objective assignment state for the database is marked as\n'Completed'.\nRe-run your ALTER DATABASE statement when transition period ﬁnishes.\nSection 110.3: Replication of Azure SQL Database\nYou can create a secondary replica of database with the same name on another Azure SQL Server, making the local\ndatabase primary, and begins asynchronously replicating data from the primary to the new secondary.\nALTER DATABASE <<mydb>>\nADD SECONDARY ON SERVER <<secondaryserver>>\nWITH ( ALLOW_CONNECTIONS = ALL )\nTarget server may be in another data center (usable for geo-replication). If a database with the same name already\nexists on the target server, the command will fail. The command is executed on the master database on the server\nhosting the local database that will become the primary. When ALLOW_CONNECTIONS is set to ALL (it is set to NO\nby default), secondary replica will be a read-only database that will allow all logins with the appropriate permissions\nto connect.\nSecondary database replica might be promoted to primary using the following command:\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 264\nALTER DATABASE mydb FAILOVER\nYou can remove the secondary database on secondary server:\nALTER DATABASE <<mydb>>\nREMOVE SECONDARY ON SERVER <<testsecondaryserver>>\nSection 110.4: Create Azure SQL Database in Elastic pool\nYou can put your azure SQL Database in SQL elastic pool:\nCREATE DATABASE wwi\n( SERVICE_OBJECTIVE = ELASTIC_POOL ( name = mypool1 ) )\nYou can create copy of an existing database and place it in some elastic pool:\nCREATE DATABASE wwi\nAS COPY OF myserver.WideWorldImporters  \n( SERVICE_OBJECTIVE = ELASTIC_POOL ( name = mypool1 ) )\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 265\nChapter 111: System database - TempDb\nSection 111.1: Identify TempDb usage\nFollowing query will provide information about TempDb usage. Analyzing the counts you can identify which thing is\nimpacting TempDb\nSELECT\n SUM (user_object_reserved_page_count)*8 as usr_obj_kb,\n SUM (internal_object_reserved_page_count)*8 as internal_obj_kb,\n SUM (version_store_reserved_page_count)*8  as version_store_kb,\n SUM (unallocated_extent_page_count)*8 as freespace_kb,\n SUM (mixed_extent_page_count)*8 as mixedextent_kb\nFROM sys.dm_db_file_space_usage\nSection 111.2: TempDB database details\nBelow query can be used to get TempDB database details:\nUSE [MASTER]\nSELECT * FROM sys.databases WHERE database_id = 2\nOR\nUSE [MASTER]\nSELECT * FROM sys.master_files WHERE database_id = 2\nWith the help of below DMV, you can check how much TempDb space does your session is using. This query is quite\nhelpful while debugging TempDb issues\nSELECT * FROM sys.dm_db_session_space_usage WHERE session_id = @@SPID\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 266\nAppendix A: Microsoft SQL Server\nManagement Studio Shortcut Keys\nSection A.1: Shortcut Examples\nOpen a new Query Window with current connection ( Ctrl  +  N )1.\nToggle between opened tabs ( Ctrl  +  Tab )2.\nShow/Hide Results pane ( Ctrl  +  R )3.\nExecute highlighted query ( Ctrl  +  E )4.\nMake selected text uppercase or lowercase ( Ctrl  +  Shift  +  U ,  Ctrl  +  Shift  +  L )5.\nIntellisense list member and complete word ( Ctrl  +  Space ,  Tab )6.\nGo to line ( Ctrl  +  G )7.\nclose a tab in SQL Server Managament Studio ( Ctrl  +  F4 )8.\nSection A.2: Menu Activation Keyboard Shortcuts\nMove to the SQL Server Management Studio menu bar ( ALT )1.\nActivate the menu for a tool component ( ALT +  HYPHEN )2.\nDisplay the context menu ( SHIFT + F )3.\nDisplay the New File dialog box to create a ﬁle ( CTRL + N )4.\nDisplay the Open Project dialog box to open an existing project ( CTRL + SHIFT + 0 )5.\nDisplay the Add New Item dialog box to add a new ﬁle to the current project ( CTRL + SHIFT + A )6.\nDisplay the Add Existing Item dialog box to add an existing ﬁle to the current project7.\n( CTRL + SHIFT + A )\nDisplay the Query Designer ( CTRL + SHIFT + Q )8.\nClose a menu or dialog box, canceling the action ( ESC )9.\nSection A.3: Custom keyboard shortcuts\nGo to Tools -> Options. Go to Environment -> Keyboard -> Query Shortcuts\nOn the right side you can see some shortcuts which are by default in SSMS. Now if you need to add a new one, just\nclick on any column under Stored Procedure column.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 267\nClick OK. Now please go to a query window and select the stored procedure then press CTRL+3, it will show the\nstored procedure result.\nNow if you need to select all the records from a table when you select the table and press CTRL+5(You can select\nany key). You can make the shortcut as follows.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 268\nNow go ahead and select the table name from the query window and press CTRL+4(The key we selected), it will give\nyou the result.\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 269\nCredits\nThank you greatly to all the people from Stack Overﬂow Documentation who helped provide this content,\nmore changes can be sent to web@petercv.com for new content to be published or updated\n5arx Chapter 101\nAbhilash R Vankayala Chapters 1 and 16\nAbhishek Jain Chapter 1\nAdam Porad Chapter 9\nAhmad Aghazadeh Chapters 59, 64 and 69\nAkash Chapter 11\nAko Chapter 76\nAkshay Anand Chapters 43 and 59\nalalp Chapters 1 and 38\nAlex Chapter 16\nAlmir Vuk Chapters 1 and 17\nAmir Pourmand ریما\nدنمروپChapter 7\nAndrea Chapter 51\nAndy Chapters 23 and 33\nandyabel Chapter 46\nAnuj Tripathi Chapters 51 and 111\nAPH Chapters 8, 24, 33, 70 and 73\nArif Chapter 36\nArthur D Chapter 1\nArun Prasad E S Chapters 3, 9, 12, 17, 23, 60 and 71\nAthafoud Chapters 22 and 47\nA_Arnold Chapters 9 and 41\nBaodad Chapter 51\nbarcanoj Chapter 15\nbassrek Chapter 97\nbbrown Chapter 36\nBeaglesEnd Chapter 1\nbeercohol Chapter 24\nBehzad Chapters 68 and 77\nBellash Chapter 9\nBen Thul Chapter 102\nBharat Prasad Satyal Chapter 12\nBiju jose Chapter 1\nBino Mathew Varghese Chapters 33, 39, 50 and 112\nBrandon Chapter 22\nChetan Sanghani Chapter 25\nchrisb Chapter 38\ncnayak Chapters 19, 35, 43, 50 and 79\ncteski Chapters 9, 17, 33, 36, 43, 50, 80 and 85\nD M Chapter 1\ndacohenii Chapter 25\nDan Guzman Chapters 49 and 108\nDaniel Lemke Chapter 18\nDavid Kaminski Chapter 16\ndd4711 Chapters 42 and 109\nDean Ward Chapter 33\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 270\nDForck42 Chapters 36 and 84\nDheeraj Kumar Chapter 57\nDhruvJoshi Chapters 33 and 47\nDileep Chapter 33\nDVJex Chapter 15\nErikE Chapter 38\nEugene Niemand Chapters 50 and 75\nfeetwet Chapters 46 and 51\nGajendra Chapter 33\nGidil Chapters 1 and 24\ngofr1 Chapter 22\nGordon Bell Chapter 1\ngotqn Chapter 32\nHadi Chapters 7, 17, 41 and 71\nHamza Rabah Chapter 34\nHari K M Chapter 50\nhatchet Chapter 41\nHenrik Staun Poulsen Chapter 59\nHK1 Chapter 33\nIgor Micev Chapter 41\nintox Chapter 15\nIztoksson Chapters 1 and 33\nJames Chapter 10\nJames Anderson Chapters 39, 49 and 51\nJamieA Chapters 9 and 51\nJared Hooper Chapters 1 and 9\nJayasurya Satheesh Chapter 50\nJeﬀrey L Whitledge Chapter 43\nJeﬀrey Van Laethem Chapters 36, 51 and 56\nJenism Chapter 23\nJesse Chapter 48\nJibin Balachandran Chapters 41 and 52\nJivan Chapter 4\nJoe Taras Chapters 1 and 43\nJohn Odom Chapters 1 and 49\nJones Joseph Chapter 89\nJosh B Chapter 17\nJosh Morel Chapter 58\nJovan MSFT Chapters 5, 11, 20, 26, 27, 28, 29, 30, 31, 34, 41, 52, 55, 80, 81, 85, 86, 89, 91,\n93, 94, 96, 98, 99, 103 and 110\njuergen d Chapter 23\njyao Chapter 51\nK48 Chapter 1\nKane Chapter 62\nKannan Kandasamy Chapters 7, 35 and 44\nKarthikeyan Chapter 12\nKeith Hall Chapters 8, 32 and 36\nKiran Ukande Chapters 23 and 25\nkolunar Chapters 45 and 47\nKritner Chapters 7, 9, 39 and 54\nLaughing Vergil Chapters 1, 2, 7, 14 and 51\nlord5et Chapter 21\nLowlyDBA Chapters 33, 39 and 51\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 271\nLuis Bosquez Chapter 83\nM.Ali Chapters 13 and 108\nMahesh Dahal Chapter 1\nMalt Chapter 1\nMani Chapter 82\nMarmiK Chapters 46 and 107\nmartinshort Chapter 15\nMasterBob Chapter 52\nMatas Vaitkevicius Chapters 2, 15, 21, 23, 50 and 65\nMatej Chapters 12 and 100\nMatt Chapters 1 and 78\nMax Chapters 1, 15, 18 and 105\nMerenix Chapter 88\nMetanormal Chapter 90\nMichael Stum Chapter 15\nMihai Chapter 1\nMono Chapter 26\nMonty Wild Chapter 36\nMoshiour Chapter 16\nMrE Chapter 25\nMspaja Chapter 69\nMudassir Hasan Chapter 1\nn00b Chapters 1 and 15\nNathan Skerl Chapter 50\nNeil Kennedy Chapter 92\nNew Chapters 45 and 70\nNick Chapters 1 and 9\nNick.McDermaid Chapter 37\nOluwafemi Chapters 61 and 74\nOzrenTkalcecKrznaric Chapters 1, 33 and 73\nPat Chapter 87\nPaul Bambury Chapter 22\nPeter Tirrell Chapter 1\nPhrancis Chapters 1, 8, 9, 33, 41, 51, 62, 63 and 67\nPirate X Chapter 50\npodiluska Chapters 7 and 21\nPrateek Chapter 1\nPரத◌ீப◌் Chapters 6 and 52\nRaghu Ariga Chapter 106\nRaidri Chapter 41\nRam Grandhi Chapter 33\nRandall Chapter 53\nravindra Chapter 20\nRhumborl Chapter 51\nRobert Columbia Chapters 15 and 17\nRoss Presser Chapter 41\nRubenisme Chapter 104\nS.Karras Chapter 39\nSam Chapters 1 and 22\nscsimon Chapters 12, 39, 50 and 51\nSender Chapter 80\nShaneis Chapter 1\nsheraz mirza Chapter 95\nGoalKicker.com – Microsoft® SQL Server® Notes for Professionals 272\nSibeesh Venu Chapter 112\nSiyual Chapters 9 and 10\nSoukai Chapter 9\nspaghettidba Chapter 51\nsqlandmore.com Chapter 72\nSQLMason Chapter 36\nsqluser Chapter 56\nSusang Chapter 49\nTab Alleman Chapter 12\ntakrl Chapter 41\nTaryn Chapter 25\nTechie Chapter 75\nTheGameiswar Chapter 40\nThuta Aung Chapter 1\nTom V Chapter 59\nTot Zam Chapters 1, 15, 17 and 51\nTZHX Chapter 51\nUberzen1 Chapters 1 and 20\nUnhandledExcepSean Chapter 9\nuser1690166 Chapter 25\nuser_0 Chapters 1 and 85\nVexator Chapter 43\nVikas Vaidya Chapter 14\nVladimir Oselsky Chapter 92\nWolfgang Chapters 11 and 32\nZohar Peled Chapters 4, 7, 9, 41, 61 and 66\nYou may also like"
  }
]